{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import xml.etree.ElementTree as ET\n","import random\n","from collections import defaultdict\n","import json"],"metadata":{"id":"YHUKbBU8zifT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ti·ªÅn x·ª≠ l√Ω v√† chia t·∫≠p d·ªØ li·ªáu t·ª´ file XML - GramVar"],"metadata":{"id":"UpWG67bQywJg"}},{"cell_type":"markdown","source":["- **M·ª•c ƒë√≠ch**:  \n","  - ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c file XML g·ªëc (theo c·∫•u tr√∫c `*_full.xml`).  \n","  - Gom nh√≥m c√°c v√≠ d·ª• d·ª±a tr√™n to√†n b·ªô argument c√≥ d·∫°ng `ARG-n`.  \n","  - Chia d·ªØ li·ªáu th√†nh 3 t·∫≠p: Train, Validation, Test.  \n","  - L∆∞u k·∫øt qu·∫£ th√†nh c√°c file `.json` ƒë·ªÉ ph·ª•c v·ª• hu·∫•n luy·ªán m√¥ h√¨nh sau n√†y.  "],"metadata":{"id":"glILNB25xcvR"}},{"cell_type":"markdown","source":["- **Quy tr√¨nh**"],"metadata":{"id":"e26kDC2NDDYm"}},{"cell_type":"markdown","source":["  1. **H√†m `parse_and_group_examples`**  \n","     - ƒê·ªçc n·ªôi dung XML v√† l·∫•y c√°c ph·∫ßn t·ª≠ `<example>`.  \n","     - Chu·∫©n h√≥a nh√£n argument th√†nh d·∫°ng `ARG-n`.  \n","     - Gom nh√≥m v√≠ d·ª• theo to√†n b·ªô b·ªô `ARG-n` (s·∫Øp x·∫øp theo s·ªë th·ª© t·ª± ƒë·ªÉ tr√°nh tr√πng l·∫∑p kh√¥ng c·∫ßn thi·∫øt).  \n","     - Tr·∫£ v·ªÅ danh s√°ch nh√≥m v√† t·ªïng s·ªë v√≠ d·ª•.  "],"metadata":{"id":"6G8c40Iexdgg"}},{"cell_type":"code","source":["def parse_and_group_examples(xml_content, predicate_name):\n","    \"\"\"\n","    Ph√¢n t√≠ch n·ªôi dung XML, chu·∫©n h√≥a nh√£n argument th√†nh ARG-n,\n","    r·ªìi gom nh√≥m c√°c v√≠ d·ª• theo to√†n b·ªô t·∫≠p c√°c ARG-n.\n","    \"\"\"\n","    groups = defaultdict(list)  # D√πng dict ƒë·ªÉ gom nh√≥m: key = b·ªô ARG, value = list v√≠ d·ª•\n","\n","    try:\n","        # Parse n·ªôi dung XML (lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu/cu·ªëi)\n","        root = ET.fromstring(xml_content.strip())\n","    except ET.ParseError as e:\n","        # N·∫øu l·ªói XML, in ra v√† b·ªè qua\n","        print(f\"L·ªói khi ph√¢n t√≠ch XML: {e}\")\n","        return [], 0\n","\n","    # T√¨m t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ <example> trong file\n","    examples_found = root.findall('.//example')\n","\n","    for ex in examples_found:\n","        # L·∫•y ph·∫ßn t·ª≠ <text> (n·ªôi dung c√¢u)\n","        text_element = ex.find('text')\n","        if text_element is None or text_element.text is None:\n","            continue\n","        text = text_element.text\n","\n","        # L∆∞u c√°c argument d∆∞·ªõi d·∫°ng dict: {ARG-n: \"n·ªôi dung\"}\n","        args = {}\n","        for arg in ex.findall('arg'):\n","            key = arg.get('n')  # L·∫•y thu·ªôc t√≠nh 'n' c·ªßa <arg> (s·ªë th·ª© t·ª±)\n","            if key and key.isdigit():\n","                new_key = f\"ARG-{key}\"   # Chu·∫©n h√≥a: n·∫øu l√† s·ªë th√¨ th√†nh \"ARG-<s·ªë>\"\n","            else:\n","                new_key = key            # N·∫øu kh√¥ng ph·∫£i s·ªë, gi·ªØ nguy√™n\n","            args[new_key] = arg.text\n","\n","        # 1. L·∫•y t·∫•t c·∫£ c√°c key c√≥ d·∫°ng 'ARG-...'\n","        arg_keys = [k for k in args if k.startswith('ARG-')]\n","\n","        # 2. S·∫Øp x·∫øp key theo s·ªë th·ª© t·ª± ƒë·ªÉ th·ªëng nh·∫•t th·ª© t·ª±\n","        #    => tr√°nh tr∆∞·ªùng h·ª£p ('a','b') kh√°c ('b','a')\n","        sorted_arg_keys = sorted(arg_keys, key=lambda k: int(k.split('-')[1]))\n","\n","        # 3. T·∫°o kh√≥a gom nh√≥m t·ª´ *gi√° tr·ªã* c·ªßa c√°c ARG (theo th·ª© t·ª± ƒë√£ s·∫Øp x·∫øp)\n","        group_key = tuple(args[k] for k in sorted_arg_keys)\n","\n","        # ƒê√≥ng g√≥i d·ªØ li·ªáu c·ªßa 1 v√≠ d·ª•\n","        sample_data = {\n","            \"text\": text,\n","            \"predicate\": predicate_name,\n","            \"arguments\": args\n","        }\n","\n","        # Th√™m v√≠ d·ª• n√†y v√†o nh√≥m t∆∞∆°ng ·ª©ng (theo group_key)\n","        groups[group_key].append(sample_data)\n","\n","    # Tr·∫£ v·ªÅ: danh s√°ch c√°c nh√≥m (list of list) v√† t·ªïng s·ªë v√≠ d·ª• t√¨m th·∫•y\n","    return list(groups.values()), len(examples_found)"],"metadata":{"id":"d9KFraL_xpxL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["  2. **H√†m `split_groups_train_val_test`**  \n","     - Nh·∫≠n danh s√°ch c√°c nh√≥m v√† chia th√†nh 3 t·∫≠p theo t·ª∑ l·ªá: Train (80%), Validation (10%), Test (10%).  \n","     - Vi·ªác chia theo nh√≥m ƒë·∫£m b·∫£o c√°c v√≠ d·ª• c√πng pattern argument kh√¥ng b·ªã r∆°i v√†o t·∫≠p kh√°c nhau (tr√°nh r√≤ r·ªâ d·ªØ li·ªáu).  \n","     - Flatten danh s√°ch nh√≥m th√†nh danh s√°ch m·∫´u ri√™ng l·∫ª."],"metadata":{"id":"uoprCWt0xqGD"}},{"cell_type":"code","source":["def split_groups_train_val_test(groups, val_size=0.1, test_size=0.1):\n","    \"\"\"\n","    Chia danh s√°ch nh√≥m d·ªØ li·ªáu th√†nh 3 ph·∫ßn: Train, Validation, Test.\n","    - M·ªói \"group\" l√† t·∫≠p c√°c v√≠ d·ª• c√≥ c√πng pattern argument (gom t·ª´ b∆∞·ªõc tr∆∞·ªõc).\n","    - Vi·ªác chia theo nh√≥m gi√∫p tr√°nh r√≤ r·ªâ d·ªØ li·ªáu gi·ªØa c√°c t·∫≠p.\n","\n","    Tham s·ªë:\n","      - groups: list c√°c nh√≥m (m·ªói nh√≥m ch·ª©a nhi·ªÅu sample_data)\n","      - val_size: t·ª∑ l·ªá validation (m·∫∑c ƒë·ªãnh = 10%)\n","      - test_size: t·ª∑ l·ªá test (m·∫∑c ƒë·ªãnh = 10%)\n","\n","    Tr·∫£ v·ªÅ:\n","      - train_set, val_set, test_set: list c√°c sample (ƒë√£ flatten t·ª´ nh√≥m)\n","    \"\"\"\n","\n","    # 1. Ki·ªÉm tra t·ªïng val_size + test_size ph·∫£i < 1\n","    if val_size + test_size >= 1.0:\n","        raise ValueError(\"T·ªïng c·ªßa val_size v√† test_size ph·∫£i nh·ªè h∆°n 1.\")\n","\n","    # 2. X√°o tr·ªôn ng·∫´u nhi√™n th·ª© t·ª± c√°c nh√≥m (ƒë·ªÉ chia d·ªØ li·ªáu c√¥ng b·∫±ng h∆°n)\n","    random.shuffle(groups)\n","\n","    n_groups = len(groups)  # T·ªïng s·ªë nh√≥m\n","\n","    # 3. T√≠nh ch·ªâ s·ªë c·∫Øt ƒë·ªÉ l·∫•y ph·∫ßn test (ph√≠a cu·ªëi)\n","    test_split_index = int(n_groups * (1 - test_size))\n","\n","    # 4. T√≠nh t·ª∑ l·ªá validation trong ph·∫ßn c√≤n l·∫°i sau khi t√°ch test\n","    #    (v√≠ d·ª•: val=0.1, test=0.1 => val chi·∫øm 0.1/0.9 ‚âà 11% ph·∫ßn c√≤n l·∫°i)\n","    val_proportion_in_remainder = val_size / (1 - test_size)\n","    val_split_index = int(test_split_index * (1 - val_proportion_in_remainder))\n","\n","    # 5. C·∫Øt d·ªØ li·ªáu th√†nh c√°c nh√≥m train/val/test\n","    train_groups = groups[:val_split_index]\n","    val_groups   = groups[val_split_index:test_split_index]\n","    test_groups  = groups[test_split_index:]\n","\n","    # 6. Flatten (t·ª´ list nh√≥m -> list sample) ƒë·ªÉ hu·∫•n luy·ªán d·ªÖ h∆°n\n","    train_set = [sample for group in train_groups for sample in group]\n","    val_set   = [sample for group in val_groups   for sample in group]\n","    test_set  = [sample for group in test_groups  for sample in group]\n","\n","    # 7. Tr·∫£ v·ªÅ k·∫øt qu·∫£\n","    return train_set, val_set, test_set"],"metadata":{"id":"7KnX6-fbxtEH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" 3. **Ch∆∞∆°ng tr√¨nh ch√≠nh**  \n","     - Khai b√°o ƒë∆∞·ªùng d·∫´n dataset g·ªëc v√† ƒë∆∞·ªùng d·∫´n l∆∞u k·∫øt qu·∫£.  \n","     - T·∫°o th∆∞ m·ª•c `Train`, `Validation`, `Test` trong th∆∞ m·ª•c ƒë√≠ch.  \n","     - Duy·ªát qua t·∫•t c·∫£ file `*_full.xml`.  \n","     - V·ªõi m·ªói file:  \n","       - Sinh ra `predicate_id` t·ª´ t√™n file (vd: `lemma_3_full.xml ‚Üí lemma.03`).  \n","       - Parse n·ªôi dung v√† gom nh√≥m d·ªØ li·ªáu.  \n","       - Chia d·ªØ li·ªáu th√†nh Train/Val/Test.  \n","       - L∆∞u ra 3 file `.json` t∆∞∆°ng ·ª©ng.  \n"],"metadata":{"id":"MxCVZYd4xtYn"}},{"cell_type":"code","source":["# 1. Khai b√°o c√°c ƒë∆∞·ªùng d·∫´n g·ªëc\n","# dataset_path: ch·ª©a c√°c file XML ƒë·∫ßu v√†o\n","# output_path : n∆°i l∆∞u d·ªØ li·ªáu sau khi chia Train/Validation/Test\n","dataset_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/GramVar/'\n","output_path  = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_GramVar/'\n","\n","# 2. T·ª± ƒë·ªông t·∫°o c√°c th∆∞ m·ª•c con (n·∫øu ch∆∞a c√≥)\n","# M·ªói t·∫≠p d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c l∆∞u ri√™ng v√†o th∆∞ m·ª•c t∆∞∆°ng ·ª©ng\n","os.makedirs(os.path.join(output_path, 'Train'),      exist_ok=True)\n","os.makedirs(os.path.join(output_path, 'Validation'), exist_ok=True)\n","os.makedirs(os.path.join(output_path, 'Test'),       exist_ok=True)\n","\n","# 3. L·∫∑p qua t·∫•t c·∫£ c√°c file trong th∆∞ m·ª•c ngu·ªìn\n","print(f\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c√°c file trong: {dataset_path}\\n\")\n","for filename in os.listdir(dataset_path):\n","    # Ch·ªâ x·ª≠ l√Ω c√°c file c√≥ ƒëu√¥i \"_full.xml\"\n","    if filename.endswith('_full.xml'):\n","        print(f\"ƒêang x·ª≠ l√Ω file: {filename}\")\n","\n","        # 3.1. L·∫•y t√™n file g·ªëc (b·ªè \"_full.xml\")\n","        base_name = filename.replace('_full.xml', '')\n","\n","        # 3.2. Sinh ra \"predicate_id\" t·ª´ t√™n file\n","        #  - N·∫øu t√™n file c√≥ d·∫°ng lemma_3_full.xml -> lemma.03\n","        #  - N·∫øu ch·ªâ c√≥ lemma_full.xml -> lemma.01\n","        predicate_id = ''\n","        if '_' in base_name:\n","            parts = base_name.split('_')\n","            lemma = parts[0]\n","            version_num = int(parts[1])\n","            predicate_id = f\"{lemma}.{version_num:02d}\"\n","        else:\n","            lemma = base_name\n","            predicate_id = f\"{lemma}.01\"\n","\n","        print(f\"T√™n file g·ªëc: {base_name}  -->  Predicate ID: {predicate_id}\")\n","\n","        # 3.3. ƒê·ªçc n·ªôi dung XML t·ª´ file\n","        full_input_path = os.path.join(dataset_path, filename)\n","        try:\n","            with open(full_input_path, 'r', encoding='utf-8') as f:\n","                xml_content = f.read()\n","        except FileNotFoundError:\n","            print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y file '{full_input_path}'. B·ªè qua.\")\n","            continue\n","\n","        # 3.4. Parse n·ªôi dung XML v√† gom nh√≥m theo to√†n b·ªô ARG-n\n","        all_groups, total_examples = parse_and_group_examples(xml_content, predicate_id)\n","        if not all_groups:\n","             print(\"Kh√¥ng t√¨m th·∫•y m·∫´u n√†o h·ª£p l·ªá. B·ªè qua.\")\n","             continue\n","\n","        print(f\"T√¨m th·∫•y {total_examples} m·∫´u, gom th√†nh {len(all_groups)} nh√≥m.\")\n","\n","        # 3.5. Chia d·ªØ li·ªáu th√†nh Train/Val/Test (theo t·ª∑ l·ªá m·∫∑c ƒë·ªãnh: 80/10/10)\n","        train_data, val_data, test_data = split_groups_train_val_test(\n","            all_groups, val_size=0.1, test_size=0.1\n","        )\n","        print(f\"Chia th√†nh c√¥ng: {len(train_data)} Train, {len(val_data)} Val, {len(test_data)} Test.\")\n","\n","        # 3.6. X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n l∆∞u d·ªØ li·ªáu sau khi chia\n","        train_output_path      = os.path.join(output_path, 'Train',      f'{base_name}_train_set.json')\n","        validation_output_path = os.path.join(output_path, 'Validation', f'{base_name}_validation_set.json')\n","        test_output_path       = os.path.join(output_path, 'Test',       f'{base_name}_test_set.json')\n","\n","        # 3.7. Ghi d·ªØ li·ªáu ra file JSON v·ªõi format d·ªÖ ƒë·ªçc (indent=2)\n","        with open(train_output_path, 'w', encoding='utf-8') as f:\n","            json.dump(train_data, f, indent=2, ensure_ascii=False)\n","\n","        with open(validation_output_path, 'w', encoding='utf-8') as f:\n","            json.dump(val_data, f, indent=2, ensure_ascii=False)\n","\n","        with open(test_output_path, 'w', encoding='utf-8') as f:\n","            json.dump(test_data, f, indent=2, ensure_ascii=False)\n","\n","        print(f\"L∆∞u th√†nh c√¥ng 3 file cho '{base_name}'.\\n\")\n","\n","# 4. Ho√†n t·∫•t to√†n b·ªô qu√° tr√¨nh\n","print(\"Ho√†n t·∫•t! T·∫•t c·∫£ c√°c file ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω.\")\n"],"metadata":{"id":"1HLwkuZEx4ZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **K·∫øt qu·∫£**:  \n","  - V·ªõi m·ªói file XML g·ªëc, sinh ra 3 file JSON:  \n","    - `<t√™n_file>_train_set.json`  \n","    - `<t√™n_file>_validation_set.json`  \n","    - `<t√™n_file>_test_set.json`  \n","  - M·ªói file ch·ª©a d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω, s·∫µn s√†ng cho b∆∞·ªõc hu·∫•n luy·ªán m√¥ h√¨nh."],"metadata":{"id":"YL3qyCvmx67D"}},{"cell_type":"markdown","source":["# Ti·ªÅn x·ª≠ l√Ω v√† chia t·∫≠p d·ªØ li·ªáu t·ª´ file XML - ParaVE"],"metadata":{"id":"cwx3H3tLzME3"}},{"cell_type":"code","source":["# 1. Khai b√°o c√°c ƒë∆∞·ªùng d·∫´n g·ªëc\n","# dataset_path: th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu XML ƒë·∫ßu v√†o (ParaVE corpus)\n","# split_parave_path: th∆∞ m·ª•c ƒë√≠ch ƒë·ªÉ l∆∞u d·ªØ li·ªáu ƒë√£ chia th√†nh Train/Val/Test\n","dataset_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/ParaVE/'\n","split_parave_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_ParaVE/'\n","\n","# 2. T·ª± ƒë·ªông t·∫°o c√°c th∆∞ m·ª•c con n·∫øu ch√∫ng ch∆∞a t·ªìn t·∫°i\n","# M·ªói t·∫≠p d·ªØ li·ªáu (Train, Validation, Test) s·∫Ω ƒë∆∞·ª£c l∆∞u ·ªü m·ªôt th∆∞ m·ª•c ri√™ng\n","os.makedirs(os.path.join(split_parave_path, 'Train'), exist_ok=True)\n","os.makedirs(os.path.join(split_parave_path, 'Validation'), exist_ok=True)\n","os.makedirs(os.path.join(split_parave_path, 'Test'), exist_ok=True)\n","\n","# 3. L·∫∑p qua t·∫•t c·∫£ c√°c file trong th∆∞ m·ª•c ngu·ªìn\n","print(f\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c√°c file trong: {dataset_path}\\n\")\n","for filename in os.listdir(dataset_path):\n","    # Ch·ªâ x·ª≠ l√Ω c√°c file XML c√≥ h·∫≠u t·ªë \"_full.xml\"\n","    if filename.endswith('_full.xml'):\n","        print(f\"--- ƒêang x·ª≠ l√Ω file: {filename} ---\")\n","\n","        # L·∫•y t√™n file g·ªëc (vd: \"begin_1\" t·ª´ \"begin_1_full.xml\")\n","        base_name = filename.replace('_full.xml', '')\n","\n","        # Sinh predicate_id theo ƒë·ªãnh d·∫°ng chu·∫©n (vd: \"begin.01\")\n","        predicate_id = ''\n","        if '_' in base_name:\n","            # N·∫øu t√™n file c√≥ s·ªë version -> gh√©p l·∫°i th√†nh lemma.xx\n","            parts = base_name.split('_')\n","            lemma = parts[0]\n","            version_num = int(parts[1])\n","            predicate_id = f\"{lemma}.{version_num:02d}\"\n","        else:\n","            # N·∫øu kh√¥ng c√≥ version -> m·∫∑c ƒë·ªãnh l√† .01\n","            lemma = base_name\n","            predicate_id = f\"{lemma}.01\"\n","\n","        print(f\"T√™n file g·ªëc: {base_name}  -->  Predicate ID: {predicate_id}\")\n","\n","        # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß cho file XML\n","        full_input_path = os.path.join(dataset_path, filename)\n","\n","        # ƒê·ªçc n·ªôi dung file XML\n","        try:\n","            with open(full_input_path, 'r', encoding='utf-8') as f:\n","                xml_content = f.read()\n","        except FileNotFoundError:\n","            # N·∫øu kh√¥ng t√¨m th·∫•y file -> b·ªè qua\n","            print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y file '{full_input_path}'. B·ªè qua.\")\n","            continue\n","\n","        # G·ªçi h√†m ph√¢n t√≠ch v√† gom nh√≥m d·ªØ li·ªáu theo ARG-n\n","        all_groups, total_examples = parse_and_group_examples(xml_content, predicate_id)\n","\n","        # N·∫øu kh√¥ng c√≥ nh√≥m n√†o h·ª£p l·ªá th√¨ b·ªè qua\n","        if not all_groups:\n","             print(\"Kh√¥ng t√¨m th·∫•y m·∫´u n√†o h·ª£p l·ªá. B·ªè qua.\")\n","             continue\n","\n","        print(f\"T√¨m th·∫•y {total_examples} m·∫´u, gom th√†nh {len(all_groups)} nh√≥m.\")\n","\n","        # Chia d·ªØ li·ªáu ƒë√£ gom nh√≥m th√†nh Train/Val/Test (m·∫∑c ƒë·ªãnh 80/10/10)\n","        train_data, val_data, test_data = split_groups_train_val_test(all_groups, val_size=0.1, test_size=0.1)\n","        print(f\"Chia th√†nh c√¥ng: {len(train_data)} Train, {len(val_data)} Val, {len(test_data)} Test.\")\n","\n","        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n ƒë·ªÉ l∆∞u file JSON k·∫øt qu·∫£\n","        train_output_path = os.path.join(split_parave_path, 'Train', f'{base_name}_train_set.json')\n","        validation_output_path = os.path.join(split_parave_path, 'Validation', f'{base_name}_validation_set.json')\n","        test_output_path = os.path.join(split_parave_path, 'Test', f'{base_name}_test_set.json')\n","\n","        # L∆∞u d·ªØ li·ªáu ƒë√£ chia ra 3 file JSON\n","        with open(train_output_path, 'w', encoding='utf-8') as f:\n","            json.dump(train_data, f, indent=2, ensure_ascii=False)\n","        with open(validation_output_path, 'w', encoding='utf-8') as f:\n","            json.dump(val_data, f, indent=2, ensure_ascii=False)\n","        with open(test_output_path, 'w', encoding='utf-8') as f:\n","            json.dump(test_data, f, indent=2, ensure_ascii=False)\n","\n","        print(f\"L∆∞u th√†nh c√¥ng 3 file cho '{base_name}'.\\n\")\n","\n","# 4. Ho√†n t·∫•t qu√° tr√¨nh x·ª≠ l√Ω t·∫•t c·∫£ file trong ParaVE\n","print(\"Ho√†n t·∫•t! T·∫•t c·∫£ c√°c file trong ParaVE ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω.\")"],"metadata":{"id":"fiw6nahBA6kX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759109326153,"user_tz":-420,"elapsed":35677,"user":{"displayName":"Khi Kaka","userId":"13789543103991274797"}},"outputId":"ed09b123-3111-4739-fa9c-506dd409e074","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c√°c file trong: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/ParaVE/\n","\n","--- ƒêang x·ª≠ l√Ω file: catalyse_full.xml ---\n","    T√™n file g·ªëc: catalyse  -->  Predicate ID: catalyse.01\n","    T√¨m th·∫•y 55 m·∫´u, gom th√†nh 38 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 43 Train, 5 Val, 7 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'catalyse'.\n","\n","--- ƒêang x·ª≠ l√Ω file: result_full.xml ---\n","    T√™n file g·ªëc: result  -->  Predicate ID: result.01\n","    T√¨m th·∫•y 275 m·∫´u, gom th√†nh 145 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 218 Train, 26 Val, 31 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'result'.\n","\n","--- ƒêang x·ª≠ l√Ω file: develop_full.xml ---\n","    T√™n file g·ªëc: develop  -->  Predicate ID: develop.01\n","    T√¨m th·∫•y 12 m·∫´u, gom th√†nh 10 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 10 Train, 1 Val, 1 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'develop'.\n","\n","--- ƒêang x·ª≠ l√Ω file: eliminate_full.xml ---\n","    T√™n file g·ªëc: eliminate  -->  Predicate ID: eliminate.01\n","    T√¨m th·∫•y 42 m·∫´u, gom th√†nh 33 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 30 Train, 7 Val, 5 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'eliminate'.\n","\n","--- ƒêang x·ª≠ l√Ω file: translate_3_full.xml ---\n","    T√™n file g·ªëc: translate_3  -->  Predicate ID: translate.03\n","    T√¨m th·∫•y 238 m·∫´u, gom th√†nh 78 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 186 Train, 27 Val, 25 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'translate_3'.\n","\n","--- ƒêang x·ª≠ l√Ω file: transform_1_full.xml ---\n","    T√™n file g·ªëc: transform_1  -->  Predicate ID: transform.01\n","    T√¨m th·∫•y 37 m·∫´u, gom th√†nh 29 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 30 Train, 4 Val, 3 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'transform_1'.\n","\n","--- ƒêang x·ª≠ l√Ω file: begin_2_full.xml ---\n","    T√™n file g·ªëc: begin_2  -->  Predicate ID: begin.02\n","    T√¨m th·∫•y 232 m·∫´u, gom th√†nh 100 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 186 Train, 27 Val, 19 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'begin_2'.\n","\n","--- ƒêang x·ª≠ l√Ω file: block_full.xml ---\n","    T√™n file g·ªëc: block  -->  Predicate ID: block.01\n","    T√¨m th·∫•y 145 m·∫´u, gom th√†nh 72 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 115 Train, 17 Val, 13 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'block'.\n","\n","--- ƒêang x·ª≠ l√Ω file: abolish_full.xml ---\n","    T√™n file g·ªëc: abolish  -->  Predicate ID: abolish.01\n","    T√¨m th·∫•y 82 m·∫´u, gom th√†nh 59 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 64 Train, 7 Val, 11 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'abolish'.\n","\n","--- ƒêang x·ª≠ l√Ω file: translate_2_full.xml ---\n","    T√™n file g·ªëc: translate_2  -->  Predicate ID: translate.02\n","    T√¨m th·∫•y 177 m·∫´u, gom th√†nh 72 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 138 Train, 23 Val, 16 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'translate_2'.\n","\n","--- ƒêang x·ª≠ l√Ω file: inhibit_full.xml ---\n","    T√™n file g·ªëc: inhibit  -->  Predicate ID: inhibit.01\n","    T√¨m th·∫•y 81 m·∫´u, gom th√†nh 61 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 63 Train, 7 Val, 11 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'inhibit'.\n","\n","--- ƒêang x·ª≠ l√Ω file: begin_1_full.xml ---\n","    T√™n file g·ªëc: begin_1  -->  Predicate ID: begin.01\n","    T√¨m th·∫•y 34 m·∫´u, gom th√†nh 18 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 25 Train, 3 Val, 6 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'begin_1'.\n","\n","--- ƒêang x·ª≠ l√Ω file: splice_2_full.xml ---\n","    T√™n file g·ªëc: splice_2  -->  Predicate ID: splice.02\n","    T√¨m th·∫•y 48 m·∫´u, gom th√†nh 36 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 39 Train, 4 Val, 5 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'splice_2'.\n","\n","--- ƒêang x·ª≠ l√Ω file: modify_full.xml ---\n","    T√™n file g·ªëc: modify  -->  Predicate ID: modify.01\n","    T√¨m th·∫•y 9 m·∫´u, gom th√†nh 7 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 7 Train, 1 Val, 1 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'modify'.\n","\n","--- ƒêang x·ª≠ l√Ω file: lose_full.xml ---\n","    T√™n file g·ªëc: lose  -->  Predicate ID: lose.01\n","    T√¨m th·∫•y 45 m·∫´u, gom th√†nh 40 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 36 Train, 5 Val, 4 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'lose'.\n","\n","--- ƒêang x·ª≠ l√Ω file: lead_full.xml ---\n","    T√™n file g·ªëc: lead  -->  Predicate ID: lead.01\n","    T√¨m th·∫•y 97 m·∫´u, gom th√†nh 67 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 80 Train, 8 Val, 9 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'lead'.\n","\n","--- ƒêang x·ª≠ l√Ω file: encode_full.xml ---\n","    T√™n file g·ªëc: encode  -->  Predicate ID: encode.01\n","    T√¨m th·∫•y 37 m·∫´u, gom th√†nh 35 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 29 Train, 4 Val, 4 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'encode'.\n","\n","--- ƒêang x·ª≠ l√Ω file: proliferate_full.xml ---\n","    T√™n file g·ªëc: proliferate  -->  Predicate ID: proliferate.01\n","    T√¨m th·∫•y 43 m·∫´u, gom th√†nh 42 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 33 Train, 5 Val, 5 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'proliferate'.\n","\n","--- ƒêang x·ª≠ l√Ω file: express_full.xml ---\n","    T√™n file g·ªëc: express  -->  Predicate ID: express.01\n","    T√¨m th·∫•y 110 m·∫´u, gom th√†nh 63 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 86 Train, 8 Val, 16 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'express'.\n","\n","--- ƒêang x·ª≠ l√Ω file: alter_full.xml ---\n","    T√™n file g·ªëc: alter  -->  Predicate ID: alter.01\n","    T√¨m th·∫•y 89 m·∫´u, gom th√†nh 57 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 72 Train, 8 Val, 9 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'alter'.\n","\n","--- ƒêang x·ª≠ l√Ω file: decrease_1_full.xml ---\n","    T√™n file g·ªëc: decrease_1  -->  Predicate ID: decrease.01\n","    T√¨m th·∫•y 170 m·∫´u, gom th√†nh 93 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 136 Train, 17 Val, 17 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'decrease_1'.\n","\n","--- ƒêang x·ª≠ l√Ω file: recognize_full.xml ---\n","    T√™n file g·ªëc: recognize  -->  Predicate ID: recognize.01\n","    T√¨m th·∫•y 39 m·∫´u, gom th√†nh 32 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 29 Train, 5 Val, 5 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'recognize'.\n","\n","--- ƒêang x·ª≠ l√Ω file: translate_1_full.xml ---\n","    T√™n file g·ªëc: translate_1  -->  Predicate ID: translate.01\n","    T√¨m th·∫•y 55 m·∫´u, gom th√†nh 37 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 42 Train, 7 Val, 6 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'translate_1'.\n","\n","--- ƒêang x·ª≠ l√Ω file: confer_full.xml ---\n","    T√™n file g·ªëc: confer  -->  Predicate ID: confer.01\n","    T√¨m th·∫•y 241 m·∫´u, gom th√†nh 119 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 193 Train, 21 Val, 27 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'confer'.\n","\n","--- ƒêang x·ª≠ l√Ω file: delete_full.xml ---\n","    T√™n file g·ªëc: delete  -->  Predicate ID: delete.01\n","    T√¨m th·∫•y 34 m·∫´u, gom th√†nh 32 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 26 Train, 4 Val, 4 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'delete'.\n","\n","--- ƒêang x·ª≠ l√Ω file: splice_1_full.xml ---\n","    T√™n file g·ªëc: splice_1  -->  Predicate ID: splice.01\n","    T√¨m th·∫•y 80 m·∫´u, gom th√†nh 58 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 60 Train, 8 Val, 12 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'splice_1'.\n","\n","--- ƒêang x·ª≠ l√Ω file: transform_2_full.xml ---\n","    T√™n file g·ªëc: transform_2  -->  Predicate ID: transform.02\n","    T√¨m th·∫•y 114 m·∫´u, gom th√†nh 63 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 90 Train, 16 Val, 8 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'transform_2'.\n","\n","--- ƒêang x·ª≠ l√Ω file: generate_full.xml ---\n","    T√™n file g·ªëc: generate  -->  Predicate ID: generate.01\n","    T√¨m th·∫•y 99 m·∫´u, gom th√†nh 69 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 77 Train, 12 Val, 10 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'generate'.\n","\n","--- ƒêang x·ª≠ l√Ω file: decrease_2_full.xml ---\n","    T√™n file g·ªëc: decrease_2  -->  Predicate ID: decrease.02\n","    T√¨m th·∫•y 65 m·∫´u, gom th√†nh 15 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 41 Train, 13 Val, 11 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'decrease_2'.\n","\n","--- ƒêang x·ª≠ l√Ω file: initiate_full.xml ---\n","    T√™n file g·ªëc: initiate  -->  Predicate ID: initiate.01\n","    T√¨m th·∫•y 45 m·∫´u, gom th√†nh 41 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 35 Train, 4 Val, 6 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'initiate'.\n","\n","--- ƒêang x·ª≠ l√Ω file: truncate_full.xml ---\n","    T√™n file g·ªëc: truncate  -->  Predicate ID: truncate.01\n","    T√¨m th·∫•y 24 m·∫´u, gom th√†nh 19 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 20 Train, 2 Val, 2 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'truncate'.\n","\n","--- ƒêang x·ª≠ l√Ω file: transcribe_full.xml ---\n","    T√™n file g·ªëc: transcribe  -->  Predicate ID: transcribe.01\n","    T√¨m th·∫•y 77 m·∫´u, gom th√†nh 61 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 62 Train, 7 Val, 8 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'transcribe'.\n","\n","--- ƒêang x·ª≠ l√Ω file: disrupt_full.xml ---\n","    T√™n file g·ªëc: disrupt  -->  Predicate ID: disrupt.01\n","    T√¨m th·∫•y 30 m·∫´u, gom th√†nh 27 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 23 Train, 3 Val, 4 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'disrupt'.\n","\n","--- ƒêang x·ª≠ l√Ω file: skip_full.xml ---\n","    T√™n file g·ªëc: skip  -->  Predicate ID: skip.01\n","    T√¨m th·∫•y 74 m·∫´u, gom th√†nh 49 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 61 Train, 8 Val, 5 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'skip'.\n","\n","--- ƒêang x·ª≠ l√Ω file: mutate_full.xml ---\n","    T√™n file g·ªëc: mutate  -->  Predicate ID: mutate.01\n","    T√¨m th·∫•y 56 m·∫´u, gom th√†nh 49 nh√≥m.\n","    --> Chia th√†nh c√¥ng: 45 Train, 6 Val, 5 Test.\n","    L∆∞u th√†nh c√¥ng 3 file cho 'mutate'.\n","\n","üéâ Ho√†n t·∫•t! T·∫•t c·∫£ c√°c file trong ParaVE ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω.\n"]}]},{"cell_type":"markdown","source":["# G·ªôp d·ªØ li·ªáu t·ª´ hai corpus (GramVar & ParaVE)"],"metadata":{"id":"lkC8Yyc23LWm"}},{"cell_type":"markdown","source":["- **M·ª•c ƒë√≠ch**:  \n","  - H·ª£p nh·∫•t d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia s·∫µn (Train, Validation, Test) t·ª´ hai corpus kh√°c nhau.  \n","  - T·∫°o ra m·ªôt b·ªô d·ªØ li·ªáu k·∫øt h·ª£p ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh t·ªïng qu√°t h∆°n.  \n","  - L∆∞u d·ªØ li·ªáu ƒë√£ g·ªôp th√†nh c√°c file `.json` ri√™ng bi·ªát cho t·ª´ng t·∫≠p.  \n"],"metadata":{"id":"wCQJS2gB3Yb4"}},{"cell_type":"markdown","source":["- **Quy tr√¨nh**"],"metadata":{"id":"nD5GxfexDNG0"}},{"cell_type":"markdown","source":[" 1. **Khai b√°o ƒë∆∞·ªùng d·∫´n**  \n","     - ƒê·ªãnh nghƒ©a `gramvar_path` v√† `parave_path` tr·ªè t·ªõi hai th∆∞ m·ª•c ngu·ªìn.  \n","     - T·∫°o th∆∞ m·ª•c m·ªõi `Combined_Dataset` ƒë·ªÉ ch·ª©a k·∫øt qu·∫£.  "],"metadata":{"id":"MQij0LR23bFB"}},{"cell_type":"code","source":["# KHAI B√ÅO C√ÅC ƒê∆Ø·ªúNG D·∫™N\n","# base_path: th∆∞ m·ª•c g·ªëc ch·ª©a to√†n b·ªô corpus\n","# gramvar_path: d·ªØ li·ªáu ƒë√£ chia Train/Val/Test c·ªßa GramVar\n","# parave_path : d·ªØ li·ªáu ƒë√£ chia Train/Val/Test c·ªßa ParaVE\n","# combined_output_path: th∆∞ m·ª•c m·ªõi ƒë·ªÉ l∆∞u d·ªØ li·ªáu sau khi g·ªôp\n","base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/'\n","gramvar_path = os.path.join(base_path, 'Split_GramVar')\n","parave_path = os.path.join(base_path, 'Split_ParaVE')\n","combined_output_path = os.path.join(base_path, 'Combined_Dataset')\n","\n","# T·ª∞ ƒê·ªòNG T·∫†O TH∆Ø M·ª§C OUTPUT\n","os.makedirs(combined_output_path, exist_ok=True)\n","print(f\"Th∆∞ m·ª•c output s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {combined_output_path}\")"],"metadata":{"id":"pfyH9KYR3nWs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. **X·ª≠ l√Ω theo t·ª´ng t·∫≠p con (Train, Validation, Test)**  \n","     - L·∫ßn l∆∞·ª£t duy·ªát qua 3 th∆∞ m·ª•c con: `Train`, `Validation`, `Test`.  \n","     - V·ªõi m·ªói t·∫≠p con:  \n","       - ƒê·ªçc to√†n b·ªô file `.json` t·ª´ GramVar v√† ParaVE.  \n","       - Load d·ªØ li·ªáu v√† n·ªëi v√†o m·ªôt list chung `combined_data`."],"metadata":{"id":"ixBIQh973dtZ"}},{"cell_type":"code","source":["# TI·∫æN H√ÄNH G·ªòP D·ªÆ LI·ªÜU\n","# subfolders: danh s√°ch c√°c t·∫≠p con c·∫ßn g·ªôp (Train, Test, Validation)\n","subfolders = ['Train', 'Test', 'Validation']\n","\n","for folder_name in subfolders:\n","    print(f\"\\n B·∫Øt ƒë·∫ßu g·ªôp th∆∞ m·ª•c: '{folder_name}'\")\n","\n","    # List ƒë·ªÉ ch·ª©a d·ªØ li·ªáu g·ªôp t·ª´ c·∫£ hai ngu·ªìn (GramVar + ParaVE)\n","    combined_data = []\n","\n","    # X√°c ƒë·ªãnh c√°c th∆∞ m·ª•c ngu·ªìn c·∫ßn ƒë·ªçc (vd: GramVar/Train, ParaVE/Train)\n","    source_folders_to_process = [\n","        os.path.join(gramvar_path, folder_name),\n","        os.path.join(parave_path, folder_name)\n","    ]\n","\n","    # L·∫∑p qua t·ª´ng th∆∞ m·ª•c ngu·ªìn\n","    for source_path in source_folders_to_process:\n","        print(f\"ƒêang ƒë·ªçc t·ª´: {source_path}\")\n","\n","        # Ki·ªÉm tra xem th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng\n","        if not os.path.isdir(source_path):\n","            print(f\"C·∫¢NH B√ÅO: Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i. B·ªè qua.\")\n","            continue\n","\n","        # L·∫∑p qua t·∫•t c·∫£ c√°c file JSON trong th∆∞ m·ª•c ngu·ªìn\n","        for filename in os.listdir(source_path):\n","            if filename.endswith('.json'):\n","                file_path = os.path.join(source_path, filename)\n","                try:\n","                    # ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON\n","                    with open(file_path, 'r', encoding='utf-8') as f:\n","                        data = json.load(f)\n","                        # Th√™m d·ªØ li·ªáu v√†o list t·ªïng\n","                        combined_data.extend(data)\n","                except Exception as e:\n","                    print(f\"L·ªói khi ƒë·ªçc file {filename}: {e}\")"],"metadata":{"id":"K6CvM8hV3yh4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" 3. **L∆∞u d·ªØ li·ªáu ƒë√£ g·ªôp**  \n","     - N·∫øu c√≥ d·ªØ li·ªáu, ghi ra file `.json` m·ªõi (vd: `combined_train_set.json`).  \n","     - L∆∞u trong th∆∞ m·ª•c `Combined_Dataset`.  "],"metadata":{"id":"pO-VLsFq3hy8"}},{"cell_type":"code","source":[" # L∆ØU FILE K·∫æT QU·∫¢ ƒê√É G·ªòP\n","    if combined_data:\n","        # ƒê·∫∑t t√™n file output theo ƒë·ªãnh d·∫°ng (vd: combined_train_set.json)\n","        output_filename = f\"combined_{folder_name.lower()}_set.json\"\n","        full_output_path = os.path.join(combined_output_path, output_filename)\n","\n","        # Ghi d·ªØ li·ªáu ƒë√£ g·ªôp ra file JSON m·ªõi\n","        with open(full_output_path, 'w', encoding='utf-8') as f:\n","            json.dump(combined_data, f, indent=2, ensure_ascii=False)\n","\n","        print(f\"G·ªôp th√†nh c√¥ng {len(combined_data)} m·∫´u.\")\n","        print(f\"ƒê√£ l∆∞u v√†o file: {output_filename}\")\n","    else:\n","        print(\"Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ g·ªôp.\")\n","\n","print(\"\\n Ho√†n t·∫•t! T·∫•t c·∫£ c√°c file ƒë√£ ƒë∆∞·ª£c g·ªôp v√† l∆∞u.\")"],"metadata":{"id":"YoEBKc_g358I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **K·∫øt qu·∫£**:  \n","  - Nh·∫≠n ƒë∆∞·ª£c 3 file JSON:  \n","    - `combined_train_set.json`  \n","    - `combined_validation_set.json`  \n","    - `combined_test_set.json`  \n","  - M·ªói file ch·ª©a d·ªØ li·ªáu ƒë√£ h·ª£p nh·∫•t t·ª´ c·∫£ hai corpus, s·∫µn s√†ng cho b∆∞·ªõc hu·∫•n luy·ªán ti·∫øp theo.  "],"metadata":{"id":"fPm5JFbB3kP9"}}]}