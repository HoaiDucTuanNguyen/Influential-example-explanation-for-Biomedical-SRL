{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Tính điểm Help/Harm thông qua Neutralization cho Top-k neighbors**"],"metadata":{"id":"aTFaVyaRVU_B"}},{"cell_type":"markdown","source":["### **Mục tiêu**\n","\n","Thực hiện tính toán điểm Help_Harm cho top-k (cụ thể là 5 mẫu) của mỗi mẫu test."],"metadata":{"id":"rrjEVCDvVx1E"}},{"cell_type":"markdown","source":["**Về phương pháp**, ta thực hiện như sau:\n","- Sau khi đã tính Mahalanobis distance giữa 2 Adaptation Vector của mẫu train và mẫu test, và chọn ra top-k (5 mẫu) có Mahalanobis distance thấp nhất (chứng tỏ các mẫu train này gần với mẫu test nhất), ta sẽ thực hiện phép Counter Factual (CF):\n","- Trừ phần ảnh hưởng của training span đó (loại bỏ A-vector training span đó ra khỏi representation của test span).\n","- Cho model predict lại để xem xác suất đầu ra đúng thay đổi như thế nào."],"metadata":{"id":"V0EJzCPKWU9N"}},{"cell_type":"markdown","source":["**Cụ thể như sau:**\n","\n","\n","- Ta tính trọng số ảnh hưởng $w$ giữa một training span ($s_{tr}$) và test span ($s_{ts}$) bằng cách dựa trên khoảng cách Mahalanobis ($d_M$) trong không gian biểu diễn.\n","\n","  $$\n","  w(s_{tr}, s_{ts}) = \\frac{1}{1 + d_M(s_{tr}, s_{ts})}\n","  $$\n","\n","- Tuy nhiên, nếu chỉ sử dụng mỗi Mahalanobis Distance để tìm những mẫu test gần nhất thôi là chưa đủ vì gần nhau chưa chắc đã cùng hướng.\n","  - Vậy nên nếu neutralize (trừ đi) những vector không liên quan thì $P_{cf}$ gần như không thay đổi gì với $P_{origin}$. Dẫn đến help_harm_score = 0 => FI không được chính xác.\n","  - Biện pháp: Sử dụng thêm **Cosine Similarity**. Mục tiêu là chỉ giữ lại thành phần của $s_{tr}$ có hướng đóng góp tích cực với $s_{ts}$.\n","  \n","Công thức Cosine Similarity giữa vector đặc trưng của mẫu train ($v_{tr}$) và mẫu test ($v_{ts}$):\n","\n","$$\n","\\text{Cosine}(v_{tr}, v_{ts}) = \\frac{v_{tr} \\cdot v_{ts}}{\\|v_{tr}\\| \\|v_{ts}\\|}\n","$$\n","\n","Với **Cosine Similarity**, ta biết được:\n","\n","    - **Cosine ≈ 1:** Hai vector **cùng chiều**. (Mẫu train có đặc điểm giống hệt mẫu test). -> Mẫu train này quan trọng\n","    - **Cosine ≈ 0:** Hai vector **vuông góc**. (Mẫu train không liên quan đến đặc điểm của mẫu test, dù có thể đứng gần). -> Không quan trọng (Nhiễu).\n","    - **Cosine < 0:** Hai vector ngược chiều. -> Bỏ qua.\n","\n","Vậy nên, ta nhân giá trị cosine vào weight ban đầu, tạo $$w_{final} = w(s_{tr}, s_{ts}) * \\text{cosine}(v_{tr}, v_{ts}) $$ để đảm bảo quá trình can thiệp (neutralization) chỉ tập trung vào các mẫu có vector đặc trưng cùng chiều (tương đồng về bản chất), đồng thời triệt tiêu các tín hiệu nhiễu (vuông góc) hoặc đối nghịch, giúp giá trị Influence Score phản ánh chính xác mức độ đóng góp thực tế của mẫu train.\n","\n","\n","- Tiếp theo, ta can thiệp vào hidden_states của model tại layer *$l_{i}$* và mô phỏng rằng \"Nếu model chưa học mẫu train này\".\n","\n","$$h_{CF}^{(l)} = h_{ft}^{(l)} - \\lambda \\cdot w_{final} \\cdot h_{A}^{(l)}$$\n","\n","  - Trong đó, ta sẽ neutralize xuống từng token:\n","    - Token đầu (begin) và cuối (end) của test span dùng chính các thành phần biên trong A-Vector của $s_{tr}$.\n","    - Token nội dung (Content): Sử dụng phần trung bình nội dung (content-pooled), nhân thêm trọng số chiếu ($w_k$) của từng token. Đây chính là trọng số projection đã lưu trong phần content pooling.\n","      - Trong cấu trúc file lưu trữ Span Adaptation Vector, các trọng số $w_k$ này tương ứng với **13 phần tử cuối cùng** của tensor (được lưu tách biệt để phục vụ bước tính toán này).\n","\n","  "],"metadata":{"id":"ygIZKWn3XM7_"}},{"cell_type":"markdown","source":["**Quy trình xử lý**:\n","- Index toàn bộ file vector và load Dataset để tham chiếu text gốc.\n","\n","- Chỉ xử lý Top-5 láng giềng gần nhất để tối ưu hiệu năng.\n","\n","- Thực hiện vòng lặp:\n","  - Tính $P_{orig}$ (xác suất ban đầu).\n","  - Tính trọng số $w_{final}$.\n","  - Thực hiện phép trừ vector (Neutralize) để tính $P_{CF}$ (xác suất sau can thiệp).\n","  \n","- Lưu kết quả help_harm_score lại để cho việc thực hiện tính FI sau này."],"metadata":{"id":"xdKHqvwea_4V"}},{"cell_type":"code","source":["# Import thư viện\n","import torch\n","import os\n","import json\n","import glob\n","import re\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","import torch.nn.functional as F\n","from functools import lru_cache"],"metadata":{"id":"n8RaC_PucwXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEK6sSJEhUeE"},"outputs":[],"source":["# CẤU HÌNH\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","model_path = os.path.join(drive_base_path, 'Finetuned_Models/biobert-srl-best-model')\n","\n","input_results_dir = os.path.join(drive_base_path, 'search_results/layer_wise_results_baseline_A')\n","output_results_dir = os.path.join(drive_base_path, 'search_results/v2_neutralize_with_cosine_similarity/layer_wise_results_with_help_harm_scores')\n","os.makedirs(output_results_dir, exist_ok=True)\n","\n","# Thư mục chứa Vector .pt\n","VECTOR_DIRS = [\n","    os.path.join(drive_base_path, 'Span Adaptation Vector/With Weight/Train/span_adaptation_vectors_train_gramvar_inner_content'),\n","    os.path.join(drive_base_path, 'Span Adaptation Vector/With Weight/Train/span_adaptation_vectors_train_parave_inner_content'),\n","    os.path.join(drive_base_path, 'Span Adaptation Vector/With Weight/Test/span_adaptation_vectors_test_gramvar_inner_content'),\n","    os.path.join(drive_base_path, 'Span Adaptation Vector/With Weight/Test/span_adaptation_vectors_test_parave_inner_content'),\n","]\n","\n","# Thư mục Dataset gốc\n","DATASET_ROOT = os.path.join(drive_base_path, 'Clean_Dataset/Corpus')\n","DATASET_DIRS = [\n","    os.path.join(DATASET_ROOT, 'Split_GramVar/Train'),\n","    os.path.join(DATASET_ROOT, 'Split_GramVar/Test'),\n","    os.path.join(DATASET_ROOT, 'Split_ParaVE/Train'),\n","    os.path.join(DATASET_ROOT, 'Split_ParaVE/Test'),\n","]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")\n","\n","# Lưu cache và lookup\n","\n","file_path_map = {}\n","dataset_json_cache = {}\n","\n","def build_file_map():\n","    print(\"[INIT] Đang quét file vector...\")\n","    count = 0\n","    for root_dir in VECTOR_DIRS:\n","        if not os.path.exists(root_dir): continue\n","        for root, dirs, files in os.walk(root_dir):\n","            corpus_id = os.path.basename(root)\n","            for f in files:\n","                if f.endswith('.pt'):\n","                    file_path_map[(corpus_id, f)] = os.path.join(root, f)\n","                    count += 1\n","    print(f\"[INIT] Đã index {count} vector files.\")\n","\n","@lru_cache(maxsize=5000)\n","def load_torch_file_cached(path):\n","    try: return torch.load(path, map_location='cpu')\n","    except: return None\n","\n","def get_file_path_robust(corpus_id, sent_idx, span_id, absolute_path_hint=None):\n","    fname1 = f\"sentence_{sent_idx}_{span_id}.pt\"\n","    if (corpus_id, fname1) in file_path_map: return file_path_map[(corpus_id, fname1)]\n","\n","    fname2 = f\"sentence_{sent_idx}_{str(span_id).replace('-', '_')}.pt\"\n","    if (corpus_id, fname2) in file_path_map: return file_path_map[(corpus_id, fname2)]\n","\n","    # Dùng path có sẵn trong json nếu file tồn tại\n","    if absolute_path_hint and os.path.exists(absolute_path_hint):\n","        return absolute_path_hint\n","    return None\n","\n","def load_pt_data(corpus_id, sent_idx, span_id, layer_idx, absolute_path=None):\n","    path = get_file_path_robust(corpus_id, sent_idx, span_id, absolute_path)\n","    if not path: return None, None\n","    data = load_torch_file_cached(path)\n","    if data is None: return None, None\n","\n","    vec, weights = None, None\n","    layer_key = f\"layer_{layer_idx}\"\n","    weight_key = f\"layer_{layer_idx}_token_weights\"\n","\n","    if isinstance(data, dict):\n","        if layer_key in data: vec = data[layer_key]\n","        if weight_key in data: weights = data[weight_key]\n","        if vec is None:\n","            for k, v in data.items():\n","                if torch.is_tensor(v) and \"token_weights\" not in k:\n","                    vec = v; break\n","    elif torch.is_tensor(data):\n","        vec = data[:-13]; weights = data[-13:]\n","    return vec, weights\n","\n","class TextLookup:\n","    def __init__(self):\n","        self.path_cache = {}\n","        # Kiểm tra nhanh xem thư mục dataset có tồn tại không\n","        print(\"\\n[DEBUG] Kiểm tra thư mục Dataset:\")\n","        found_any = False\n","        for d in DATASET_DIRS:\n","            exists = os.path.exists(d)\n","            status = \"OK\" if exists else \"Missing\"\n","            print(f\"  - {d}: {status}\")\n","            if exists: found_any = True\n","        if not found_any:\n","            print(\"CẢNH BÁO: Không tìm thấy thư mục dataset nào. Việc tra cứu text sẽ thất bại!\")\n","\n","    def get_text(self, corpus_id, sent_idx, span_id):\n","        # Tìm file json dataset\n","        json_path = self.path_cache.get(corpus_id)\n","        if not json_path:\n","            possible_names = [f\"{corpus_id}_test_set.json\", f\"{corpus_id}_train_set.json\"]\n","            for d in DATASET_DIRS:\n","                for name in possible_names:\n","                    p = os.path.join(d, name)\n","                    if os.path.exists(p):\n","                        json_path = p; break\n","                if json_path: break\n","\n","            if json_path: self.path_cache[corpus_id] = json_path\n","            else: return None, None\n","\n","        # Load nội dung\n","        if json_path not in dataset_json_cache:\n","            try:\n","                with open(json_path, 'r', encoding='utf-8') as f:\n","                    dataset_json_cache[json_path] = json.load(f)\n","            except: return None, None\n","\n","        data = dataset_json_cache[json_path]\n","        if sent_idx >= len(data): return None, None\n","\n","        entry = data[sent_idx]\n","        full_text = entry.get('text', '')\n","        clean_span = str(span_id).replace('_', '-')\n","        arg_text = entry.get('arguments', {}).get(clean_span, '')\n","\n","        return full_text, arg_text\n","\n","# Thực hiện Neutralizer\n","class Neutralizer:\n","    def __init__(self, model):\n","        self.model = model\n","        self.device = next(model.parameters()).device\n","\n","    def get_counterfactual_prob(self, inputs, layer_idx, train_vec_full, test_vec_full, dist, span_mask, test_token_weights, target_label_idx):\n","        \"\"\"\n","        Thực hiện tính với Cosine Similarity\n","        \"\"\"\n","        #Trọng số Mahalanobis cũ\n","        w_mahalanobis = 1.0 / (1.0 + dist)\n","\n","        #  Tính Cosine Similarity\n","        w_cosine = 0.0\n","        if test_vec_full is not None and train_vec_full is not None:\n","            tv = test_vec_full.to(self.device)\n","            trv = train_vec_full.to(self.device)\n","\n","            # Tính cosine (dim=0 vì vector là 1D [d])\n","            cos_sim = F.cosine_similarity(tv, trv, dim=0).item()\n","\n","            # Lọc nhiễu: Chỉ lấy phần tương đồng dương (max(0, cos))\n","            w_cosine = max(0.0, cos_sim)\n","        else:\n","            w_cosine = 1.0\n","\n","        # Tổng hợp trọng số\n","        final_weight = w_mahalanobis * w_cosine\n","\n","        # Neutralize xuống phần token\n","        hidden_size = 768\n","        vec_start = train_vec_full[0:hidden_size].to(self.device)\n","        vec_end   = train_vec_full[hidden_size : 2*hidden_size].to(self.device)\n","        vec_content = train_vec_full[2*hidden_size:].to(self.device)\n","\n","        sub_start = (final_weight * vec_start).view(1, 1, -1)\n","        sub_end   = (final_weight * vec_end).view(1, 1, -1)\n","        sub_content_base = (final_weight * vec_content).view(1, 1, -1)\n","\n","        # Thực hiện can thiệp lại mô hình để tính P_cf\n","        def neutralization_hook(module, input, output):\n","            is_tuple = isinstance(output, tuple)\n","            hidden_states = output[0] if is_tuple else output\n","\n","            indices = torch.where(span_mask[0] == 1)[0]\n","            if len(indices) == 0: return output\n","\n","            # Trừ Start Token\n","            hidden_states[:, indices[0], :] -= sub_start.squeeze(0)\n","\n","            # Trừ End Token (nếu span > 1 token)\n","            if len(indices) > 1:\n","                hidden_states[:, indices[-1], :] -= sub_end.squeeze(0)\n","\n","            # Trừ Content Tokens (nếu span > 2 token)\n","            if len(indices) > 2:\n","                inner_indices = indices[1:-1]\n","                if test_token_weights is not None and len(test_token_weights) == len(indices):\n","                    inner_weights = test_token_weights[1:-1].to(self.device)\n","                    to_subtract = inner_weights.unsqueeze(1) * sub_content_base.squeeze(0)\n","                    hidden_states[0, inner_indices, :] -= to_subtract\n","\n","            if is_tuple: return (hidden_states,) + output[1:]\n","            return hidden_states\n","\n","        layer_module = self.model.bert.encoder.layer[layer_idx]\n","        hook_handle = layer_module.register_forward_hook(neutralization_hook)\n","        try:\n","            out_cf = self.model(**inputs)\n","            probs_cf = F.softmax(out_cf.logits, dim=-1)[0]\n","            span_idxs = torch.where(span_mask[0] == 1)[0]\n","            p_cf = probs_cf[span_idxs, target_label_idx].mean().item()\n","        finally:\n","            hook_handle.remove()\n","\n","        return p_cf, w_cosine\n","\n","# Các hàm hỗ trợ\n","def get_query_text_info(query):\n","    sent = query.get('sentence_text')\n","    arg = query.get('argument_text')\n","    if not sent or not arg:\n","        meta = query.get('metadata', {})\n","        if not sent: sent = meta.get('sentence_text')\n","        if not arg: arg = meta.get('argument_text')\n","    return sent, arg\n","\n","def get_query_ids(query):\n","    cid = query.get('corpus_id')\n","    sid = query.get('span_id')\n","    sidx = query.get('sentence_idx')\n","    if not cid or not sid or sidx is None:\n","        meta = query.get('metadata', {})\n","        if not cid: cid = meta.get('corpus_id')\n","        if not sid: sid = meta.get('span_id')\n","        if sidx is None: sidx = meta.get('sentence_idx')\n","    return cid, sidx, sid\n","\n","def load_input_text_data(sentence_text, argument_text, corpus_id, span_id, tokenizer, label2id):\n","    if not sentence_text or not argument_text: return None, None, None\n","\n","    inputs = tokenizer(sentence_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n","    arg_ids = tokenizer.encode(argument_text.strip(), add_special_tokens=False)\n","    input_ids = inputs.input_ids[0].cpu().numpy()\n","\n","    n, m = len(input_ids), len(arg_ids)\n","    start_idx = -1\n","    for i in range(n - m + 1):\n","        if np.array_equal(input_ids[i:i+m], arg_ids):\n","            start_idx = i; break\n","\n","    if start_idx == -1: return None, None, None\n","\n","    span_mask = torch.zeros_like(inputs.input_ids)\n","    span_mask[0, start_idx : start_idx + m] = 1\n","\n","    parts = str(corpus_id).split('_')\n","    verb = parts[0]\n","    sense = f\"{int(parts[1]):02d}\" if len(parts)>1 and parts[1].isdigit() else \"01\"\n","    clean_span = str(span_id).replace('_', '-')\n","\n","    candidates = [f\"B-{verb}.{sense}-{clean_span}\", f\"B-{clean_span}\", f\"B-{span_id}\"]\n","    target_id = None\n","    for cand in candidates:\n","        if cand in label2id: target_id = label2id[cand]; break\n","\n","    if target_id is None:\n","        for k, v in label2id.items():\n","            if verb in k and clean_span in k and k.startswith(\"B-\"): target_id = v; break\n","\n","    return inputs, span_mask, target_id\n","\n","# Hàm main\n","def main():\n","    build_file_map()\n","    print(\"[1] Loading Model...\")\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    model = AutoModelForTokenClassification.from_pretrained(model_path).to(device)\n","    model.eval()\n","    neutralizer = Neutralizer(model)\n","    label2id = model.config.label2id\n","\n","    text_lookup = TextLookup()\n","\n","    json_files = sorted(glob.glob(os.path.join(input_results_dir, \"search_results_layer_*.json\")))\n","    def get_layer_num(fname):\n","        m = re.search(r'layer_(\\d+)', fname)\n","        return int(m.group(1)) if m else 999\n","    json_files.sort(key=get_layer_num)\n","\n","    print(f\"[2] Tìm thấy {len(json_files)} file kết quả. Bắt đầu xử lý...\")\n","\n","    for json_file in tqdm(json_files, desc=\"Processing Layers\"):\n","        layer_num = get_layer_num(os.path.basename(json_file))\n","        layer_idx = layer_num - 1\n","\n","        with open(json_file, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        updated_data = []\n","        count_calc = 0\n","\n","        for entry in tqdm(data, desc=f\"L{layer_num}\", leave=False):\n","            # lấy 5 neighbors gần nhất\n","            if 'neighbors' in entry:\n","                entry['neighbors'] = entry['neighbors'][:5]\n","\n","            query = entry.get('query_info', {})\n","\n","            # Lấy Text (Lookup)\n","            sent_text, arg_text = get_query_text_info(query)\n","            cid, sidx, sid = get_query_ids(query)\n","\n","            if not sent_text or not arg_text:\n","                if cid and sidx is not None and sid:\n","                    found_s, found_a = text_lookup.get_text(cid, sidx, sid)\n","                    if found_s:\n","                        sent_text, arg_text = found_s, found_a\n","                        # Cập nhật ngược vào entry\n","                        query['sentence_text'] = sent_text\n","                        query['argument_text'] = arg_text\n","\n","            if not sent_text or not cid:\n","                updated_data.append(entry); continue\n","\n","            # Load Input\n","            inputs, span_mask, target_id = load_input_text_data(\n","                sent_text, arg_text, cid, sid, tokenizer, label2id\n","            )\n","\n","            if inputs is None or target_id is None:\n","                updated_data.append(entry); continue\n","\n","            # Tính P_orig\n","            with torch.no_grad():\n","                out_orig = model(**inputs)\n","                probs_orig = F.softmax(out_orig.logits, dim=-1)[0]\n","                span_idxs = torch.where(span_mask[0] == 1)[0]\n","                if len(span_idxs) == 0:\n","                    updated_data.append(entry); continue\n","                p_orig = probs_orig[span_idxs, target_id].mean().item()\n","\n","            # Load Test Weights\n","            test_vec, test_weights = load_pt_data(cid, sidx, sid, layer_idx)\n","            if test_vec is None:\n","                print(f\"Warning: Missing Test Vector for {cid} | {sidx} | {sid}\")\n","            if test_weights is not None:\n","                test_weights = test_weights.float()\n","                if test_weights.sum() > 0: test_weights /= test_weights.sum()\n","\n","            for nb in entry.get('neighbors', []):\n","                train_meta = nb.get('match_metadata', {})\n","\n","                # Load Train Vec\n","                train_vec, _ = load_pt_data(\n","                    train_meta.get('corpus_id'),\n","                    train_meta.get('sentence_idx'),\n","                    train_meta.get('span_id'),\n","                    layer_idx,\n","                    absolute_path=train_meta.get('vector_file_path')\n","                )\n","\n","                if train_vec is not None:\n","                    p_cf, w_cos = neutralizer.get_counterfactual_prob(\n","                        inputs, layer_idx,\n","                        train_vec_full=train_vec,\n","                        test_vec_full=test_vec,\n","                        dist=nb.get('l2_distance', 0.0),\n","                        span_mask=span_mask,\n","                        test_token_weights=test_weights,\n","                        target_label_idx=target_id\n","                    )\n","                    nb['help_harm_score'] = p_orig - p_cf\n","                    nb['p_orig'] = p_orig\n","                    nb['p_cf'] = p_cf\n","                    nb['cosine_weight'] = w_cos\n","                    count_calc += 1\n","\n","            updated_data.append(entry)\n","\n","        print(f\"Layer {layer_num}: Đã tính điểm cho {count_calc} neighbors.\")\n","        out_name = os.path.basename(json_file)\n","        with open(os.path.join(output_results_dir, out_name), 'w', encoding='utf-8') as f:\n","            json.dump(updated_data, f, indent=2)\n","\n","    print(\"\\n Đã lưu kết quả (Top-5 neighbors) vào thư mục output.\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}