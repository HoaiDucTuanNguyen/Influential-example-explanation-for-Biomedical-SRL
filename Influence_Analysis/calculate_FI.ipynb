{"cells":[{"cell_type":"markdown","source":["# **Tính chỉ số Faithfulness Index (FI)**"],"metadata":{"id":"sRvJii1gRYHE"}},{"cell_type":"markdown","source":["### **Mục tiêu:**\n","\n","Thực hiện đo lường chỉ số Faithfulness Index.\n","\n","Mục tiêu là kiểm chứng xem các mẫu train top-k có thực sự ảnh hưởng mạnh hơn các mẫu ngẫu nhiên (random) hay không?\n","\n"],"metadata":{"id":"aQsQPc4sR1gX"}},{"cell_type":"markdown","source":["### **Mã nguồn**\n","\n","Thực hiện 3 bước chính sau:\n","\n","**1. Đảm bảo rằng Help/Harm score sẽ được so sánh nếu cả Top-k và Random của 1 mẫu test đều tồn tại.**\n","\n","- Hàm `generate_unique_key` tạo ID cho mỗi mẫu test: `corpus_id + sentence index + span_id`\n","\n","- Nếu ở tập top-k và tập random đều tìm được ID này thì mới tiến hành tính toán.\n","\n","- Nếu không thì mẫu đó sẽ bị bỏ qua.\n","\n","**2. Trích xuất điểm Help/Harm.**\n","\n","- Hàm `extract_valid_scores` lấy danh sách điểm `help_harm_score` từ file json (top-k và random).\n","\n","**3. Tính FI.**\n","\n","Với mỗi mẫu test $x_{ts}$, chỉ số FI được tính như sau:\n","\n","$$\n","FI(x_{ts}) = \\frac{1}{k} \\sum_{i=1}^{k} \\frac{\\text{count}(|S_{random}| < |S_{top\\_k\\_i}|)}{M}\n","$$\n","\n","**Trong đó:**\n","\n","* $|S_{top\\_k\\_i}|$: Độ lớn tuyệt đối của điểm Help/Harm của láng giềng thứ $i$ trong Top-k.\n","* $|S_{random}|$: Độ lớn tuyệt đối của điểm Help/Harm của các mẫu trong tập Random.\n","* $M$: Tổng số mẫu Random (Baseline).\n","\n","Code sẽ chạy qua 12 layer (lược bỏ layer 0 vì nó chỉ liên quan đến embedding).\n","- Tính trung bình cộng FI của tất cả các mẫu test hợp lệ trong layer đó.\n","- Tính trung bình cộng FI của toàn bộ 12 layer.\n","\n","\n","Nếu trung bình FI của 12 layer:\n","- **FI > 0.5:** Các mẫu train tìm được bằng phương pháp đề xuất có ảnh hưởng mạnh hơn so với các mẫu ngẫu nhiên.\n","- **FI <= 0.5**: Các mẫu train không thật sự ảnh hưởng mạnh so với các mẫu ngẫu nhiên."],"metadata":{"id":"wQyxPj-cSUwu"}},{"cell_type":"code","source":["# Import thư viện:\n","import json\n","import os\n","import numpy as np\n","import glob\n","from tqdm import tqdm"],"metadata":{"id":"IzrtYaPiRiEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Thư mục chứa kết quả Top-5 (Top-k)\n","TOP_K_DIR = os.path.join(drive_base_path, 'search_results/v2_neutralize_with_cosine_similarity/layer_wise_results_with_help_harm_scores')\n","\n","# Thư mục chứa kết quả Random 100 (Baseline)\n","RANDOM_BASE_DIR = os.path.join(drive_base_path, 'search_results/v2_neutralize_with_cosine_similarity/layer_results_with_help_harm_scores_stratified_from_top500')\n","\n","NUM_LAYERS = 12\n","\n","def load_json(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        return json.load(f)\n","\n","def generate_unique_key(item):\n","    \"\"\"\n","    Tạo ID duy nhất: corpus_id | sentence_idx | span_id\n","    Để ghép đôi chính xác mẫu test bên file Top-5 và file Random.\n","    \"\"\"\n","    try:\n","        info = item.get('query_info', {})\n","        c_id = info.get('corpus_id', 'unknown')\n","        s_idx = info.get('sentence_idx', 'unknown')\n","        sp_id = info.get('span_id', 'unknown')\n","        return f\"{c_id}|{s_idx}|{sp_id}\"\n","    except Exception:\n","        return None\n","\n","def extract_valid_scores(item, key_name):\n","    \"\"\"\n","    Lấy danh sách điểm Help/Harm.\n","    QUAN TRỌNG: Lọc bỏ các giá trị None (do lỗi thiếu vector).\n","    \"\"\"\n","    scores = []\n","    data_list = item.get(key_name, [])\n","\n","    if isinstance(data_list, list):\n","        for entry in data_list:\n","            # Chỉ lấy nếu có key help_harm_score VÀ giá trị không phải None\n","            if 'help_harm_score' in entry and entry['help_harm_score'] is not None:\n","                scores.append(float(entry['help_harm_score']))\n","    return scores\n","\n","def get_random_data_map(layer_idx, random_base_dir):\n","    \"\"\"\n","    Load toàn bộ dữ liệu Random của layer đó vào Dictionary.\n","    Output: { \"ID_unique\": [list_diem_so_random] }\n","    \"\"\"\n","    mapping = {}\n","    # Quét tất cả các folder con (batch 0-500, 500-1000...)\n","    search_pattern = os.path.join(random_base_dir, \"*\", f\"search_results_layer_{layer_idx}.json\")\n","    file_list = glob.glob(search_pattern)\n","\n","    print(f\"  -> Tìm thấy {len(file_list)} file dữ liệu Random cho Layer {layer_idx}\")\n","\n","    for file_path in file_list:\n","        try:\n","            data_list = load_json(file_path)\n","            if isinstance(data_list, list):\n","                for item in data_list:\n","                    key = generate_unique_key(item)\n","                    # File Random key là: 'random_baseline_neighbors'\n","                    scores = extract_valid_scores(item, 'random_baseline_neighbors')\n","                    if key and scores:\n","                        mapping[key] = scores\n","        except Exception as e:\n","            print(f\"Lỗi đọc file {os.path.basename(file_path)}: {e}\")\n","\n","    return mapping\n","\n","def calculate_fi_only(layer_idx):\n","    print(f\"\\n--- Đang xử lý Layer {layer_idx} ---\")\n","\n","    # Load file Top-5\n","    top_k_path = os.path.join(TOP_K_DIR, f\"search_results_layer_{layer_idx}.json\")\n","    if not os.path.exists(top_k_path):\n","        print(f\"Không tìm thấy file Top-K: {top_k_path}\")\n","        return None\n","\n","    top_k_data = load_json(top_k_path)\n","\n","    # Load dữ liệu Random để làm baseline\n","    random_map = get_random_data_map(layer_idx, RANDOM_BASE_DIR)\n","\n","    if not random_map:\n","        print(\"Không load được dữ liệu Random nào. Bỏ qua layer này.\")\n","        return None\n","\n","    total_fi = 0\n","    valid_sample_count = 0\n","\n","    # Tính FI cho từng mẫu\n","    for item in top_k_data:\n","        key = generate_unique_key(item)\n","        if key not in random_map:\n","            continue\n","\n","        # Lấy điểm số Top-5 và Random tương ứng\n","        top_k_scores = extract_valid_scores(item, 'neighbors')\n","        random_scores = random_map[key]\n","\n","        # Kiểm tra dữ liệu rỗng\n","        if not top_k_scores or not random_scores:\n","            continue\n","\n","        # FI = (1/k) * Sum( số lượng random < top_k_i ) / total_random\n","\n","        k = len(top_k_scores)\n","        abs_random = np.abs(random_scores)\n","        sum_ratios = 0\n","\n","        for score in top_k_scores:\n","            abs_top = abs(score)\n","            count_smaller = np.sum(abs_random < abs_top)\n","            ratio = count_smaller / len(random_scores)\n","            sum_ratios += ratio\n","\n","        sample_fi = sum_ratios / k\n","\n","        total_fi += sample_fi\n","        valid_sample_count += 1\n","\n","    # Tính trung bình FI cho cả Layer\n","    final_fi = total_fi / valid_sample_count if valid_sample_count > 0 else 0\n","\n","    return {\n","        \"layer\": layer_idx,\n","        \"FI\": final_fi,\n","        \"processed_samples\": valid_sample_count\n","    }\n","\n","if __name__ == \"__main__\":\n","    results = []\n","\n","    for i in range(1, NUM_LAYERS + 1):\n","        res = calculate_fi_only(i)\n","        if res:\n","            results.append(res)\n","            print(f\"Layer {i}: FI = {res['FI']:.4f} (Dựa trên {res['processed_samples']} mẫu khớp)\")\n","\n","    if results:\n","        avg_fi = np.mean([r['FI'] for r in results])\n","        print(f\"KẾT QUẢ CUỐI CÙNG (Average FI all layers): {avg_fi:.4f}\\n\")\n","\n","        # Nhận xét nhanh dựa trên tài liệu\n","        if avg_fi > 0.5:\n","            print(\"=> Kết luận: Top-k neighbors có ảnh hưởng thực sự lớn hơn Random (FI > 0.5).\")\n","        else:\n","            print(\"=> Kết luận: Top-k neighbors không tốt hơn Random (FI <= 0.5).\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pcwBdXTDp2kb","outputId":"ce12e892-6ab8-4bac-9fdc-ac174322b7b4","executionInfo":{"status":"ok","timestamp":1767450464899,"user_tz":-420,"elapsed":108842,"user":{"displayName":"Thanh Lolo","userId":"10410902574430224151"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Đang xử lý Layer 1 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 1\n","Layer 1: FI = 0.5202 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 2 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 2\n","Layer 2: FI = 0.6763 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 3 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 3\n","Layer 3: FI = 0.7185 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 4 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 4\n","Layer 4: FI = 0.8045 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 5 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 5\n","Layer 5: FI = 0.8166 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 6 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 6\n","Layer 6: FI = 0.8457 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 7 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 7\n","Layer 7: FI = 0.8378 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 8 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 8\n","Layer 8: FI = 0.8865 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 9 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 9\n","Layer 9: FI = 0.9287 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 10 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 10\n","Layer 10: FI = 0.9636 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 11 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 11\n","Layer 11: FI = 0.9553 (Dựa trên 2071 mẫu khớp)\n","\n","--- Đang xử lý Layer 12 ---\n","  -> Tìm thấy 5 file dữ liệu Random cho Layer 12\n","Layer 12: FI = 0.9571 (Dựa trên 2071 mẫu khớp)\n","KẾT QUẢ CUỐI CÙNG (Average FI all layers): 0.8259\n","\n","=> Kết luận: Top-k neighbors có ảnh hưởng thực sự lớn hơn Random (FI > 0.5).\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}