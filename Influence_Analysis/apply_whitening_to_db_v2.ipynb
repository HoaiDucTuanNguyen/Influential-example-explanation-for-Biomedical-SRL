{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1E3emwpaPq_yUgal9K4K1NtMIWkWKqR8M","authorship_tag":"ABX9TyODHr/6tms5rc5SU/mcWyfY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"edcb58bacdf54236b44d706e52c7f2b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ceb599c378644ce9115360ea4df9da9","IPY_MODEL_9a32518cd1004d54839d54b59b634b9f","IPY_MODEL_f32b95056d0645a2a1431f4147d4c902"],"layout":"IPY_MODEL_4d2c0a960cbd4f05adf910f1327d1e89"}},"9ceb599c378644ce9115360ea4df9da9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_805eff4f699346608751e30bf5119828","placeholder":"​","style":"IPY_MODEL_3819d6017f364961b822b5a95beb1a4f","value":"Xử lý từng Layer: 100%"}},"9a32518cd1004d54839d54b59b634b9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_122683b5c87148a4abb81d7bcf93b21b","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65bd84812b64421190b4b534d7f28f09","value":12}},"f32b95056d0645a2a1431f4147d4c902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb81860d4f4148909b06ca04db823009","placeholder":"​","style":"IPY_MODEL_807baf6e05be4792bb22284eb6e09605","value":" 12/12 [00:26&lt;00:00,  2.11s/it]"}},"4d2c0a960cbd4f05adf910f1327d1e89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"805eff4f699346608751e30bf5119828":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3819d6017f364961b822b5a95beb1a4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"122683b5c87148a4abb81d7bcf93b21b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65bd84812b64421190b4b534d7f28f09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb81860d4f4148909b06ca04db823009":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"807baf6e05be4792bb22284eb6e09605":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Bước 2b: “Làm trắng” (Whitening) toàn bộ Database Train (v2 - inner_content)"],"metadata":{"id":"3jkZo410Howv"}},{"cell_type":"markdown","source":["\n","## Mục tiêu của script\n","Script này dùng để:\n","- **Tải database vector gốc (train)** đã gộp sẵn: `combined_train_db_vectors_v2_inner_content.pt`\n","- **Tải dữ liệu Mahalanobis** (mean + whitening matrix theo từng layer) từ Bước 2a: `mahalanobis_data_v2_inner_content.pt`\n","- Thực hiện **whitening** cho toàn bộ vector trong database:\n","  - Với từng layer `layer_1 → layer_12`\n","  - Xử lý theo **batch** để tiết kiệm RAM/VRAM\n","- Lưu ra file database mới đã whiten:\n","  - `combined_train_db_vectors_v2_WHITENED.pt`\n"],"metadata":{"id":"KtCIGd8wHrCm"}},{"cell_type":"markdown","source":["## Thư viện sử dụng\n","- `torch`: load/save tensor, tính toán GPU, matmul, no_grad.\n","- `os`: thao tác đường dẫn file/thư mục.\n","- `tqdm.auto`: progress bar cho vòng lặp theo layer."],"metadata":{"id":"fB8bhNpBHsaM"}},{"cell_type":"markdown","source":["\n","## Cấu hình tham số\n","- `NUM_LAYERS = 12`  \n","  Số layer cần xử lý (layer_1 → layer_12).\n","\n","- `D_FEATURES = 2304`  \n","  Kích thước vector embedding.\n","\n","- `BATCH_SIZE = 512`  \n","  Xử lý theo batch để:\n","  - giảm peak memory khi đưa dữ liệu lên GPU\n","  - tránh tràn VRAM khi `N_samples` lớn\n","\n","- `device = cuda/cpu`  \n","  Nếu có GPU → chạy trên CUDA để whitening nhanh hơn."],"metadata":{"id":"qxCLKMdPHt71"}},{"cell_type":"markdown","source":["## Thiết lập đường dẫn I/O\n","\n","### Input 1 — Database gốc (chưa whiten)\n","- `combined_train_db_vectors_v2_inner_content.pt`  \n","  Chứa tensor `db_vectors` shape:\n","  - `(N_samples, NUM_LAYERS, D_FEATURES)`\n","\n","### Input 2 — Dữ liệu Mahalanobis (từ Bước 2a)\n","- `mahalanobis_data_v2_inner_content.pt`  \n","  Chứa:\n","  - `means[layer_k]` shape `(2304,)`\n","  - `whitening_matrices[layer_k]` shape `(2304, 2304)`\n","\n","### Output — Database đã whiten\n","- `combined_train_db_vectors_v2_WHITENED.pt`  \n","  Lưu tensor `whitened_db_vectors` shape giống input:\n","  - `(N_samples, NUM_LAYERS, D_FEATURES)`"],"metadata":{"id":"Wi7CPWyuHvxk"}},{"cell_type":"markdown","source":["## Luồng thực thi chính (`__main__`)\n","\n","## 1) Tải database gốc\n","- Load:\n","  - `db_vectors = torch.load(db_vectors_file)`\n","- Lấy shape:\n","  - `(N_samples, n_layers, D_features_loaded)`\n","- Kiểm tra đúng cấu hình:\n","  - `n_layers == NUM_LAYERS`\n","  - `D_features_loaded == D_FEATURES`\n","\n","Nếu load lỗi → dừng chương trình."],"metadata":{"id":"iPxdGxFAHxaC"}},{"cell_type":"markdown","source":["## 2) Tải dữ liệu Mahalanobis và chuyển lên GPU\n","- Load:\n","  - `mahalanobis_data = torch.load(mahalanobis_data_file)`\n","- Với từng layer:\n","  - đưa `mean` lên GPU:\n","    - `means[layer_name].to(device)`\n","  - đưa `whitening_matrix` lên GPU:\n","    - `whitening_matrices[layer_name].to(device)`\n","\n","Mục đích:\n","- Khi whitening theo batch, phép trừ mean và matmul chạy trên GPU."],"metadata":{"id":"i8x4NWq7HzB6"}},{"cell_type":"markdown","source":["## 3) Chuẩn bị tensor output để lưu kết quả\n","Tạo tensor mới để chứa database đã whiten:\n","- `whitened_db_vectors = torch.empty_like(db_vectors, device='cpu', dtype=torch.float32)`\n","\n","Điểm quan trọng:\n","- Đặt **trên CPU** để:\n","  - tiết kiệm VRAM\n","  - chỉ đưa từng batch lên GPU rồi đưa kết quả về CPU"],"metadata":{"id":"Bc4rzBDgH0hr"}},{"cell_type":"markdown","source":["## 4) Whitening theo từng layer và theo batch\n","\n","### Tắt gradient để tiết kiệm bộ nhớ và tăng tốc\n","- Bọc toàn bộ bằng:\n","  - `with torch.no_grad():`\n","Vì đây là bước transform dữ liệu, không cần backprop.\n"],"metadata":{"id":"OnmAhbHtH2pZ"}},{"cell_type":"markdown","source":["### Vòng lặp theo layer\n","Với mỗi `i` trong `range(NUM_LAYERS)`:\n","- Đặt:\n","  - `layer_name = f\"layer_{i+1}\"`\n","  - `tensor_index = i`\n","\n","Lấy dữ liệu của layer hiện tại:\n","- `mean = mahalanobis_data['means'][layer_name]` → shape `(2304,)`\n","- `W = mahalanobis_data['whitening_matrices'][layer_name]` → shape `(2304, 2304)"],"metadata":{"id":"80rrbzvYH4DP"}},{"cell_type":"markdown","source":["### Vòng lặp theo batch\n","Với `start_idx` chạy từ `0` đến `N_samples` bước `BATCH_SIZE`:\n","- `end_idx = min(start_idx + BATCH_SIZE, N_samples)`\n","\n","#### 4.1 Trích batch dữ liệu và đưa lên GPU\n","- `X_batch = db_vectors[start_idx:end_idx, tensor_index, :].to(device)`\n","- Shape:\n","  - `(B, 2304)` với `B = end_idx - start_idx`"],"metadata":{"id":"D-NkjL1jH5mG"}},{"cell_type":"markdown","source":["#### 4.2 Center dữ liệu (trừ mean)\n","- `X_centered = X_batch - mean`\n","\n","Ghi chú:\n","- `mean` có shape `(2304,)` sẽ được **broadcast** tự động để trừ cho từng dòng `(B, 2304)`.\n"],"metadata":{"id":"tk0BNSe8H7Uc"}},{"cell_type":"markdown","source":["#### 4.3 Whitening (nhân với ma trận làm trắng)\n","- `X_whitened = torch.matmul(X_centered, W.T)`\n","\n","Giải thích shape:\n","- `X_centered`: `(B, 2304)`\n","- `W.T`: `(2304, 2304)`\n","- Kết quả:\n","  - `(B, 2304)`\n","\n","Tại sao dùng `W.T`?\n","- Do convention: vector là row-vector `(1 x D)`\n","- Whitening: `x_whiten = (x - mean) @ W^T` (để đúng chiều nhân)\n"],"metadata":{"id":"brXQU4qxH9UW"}},{"cell_type":"markdown","source":["#### 4.4 Lưu kết quả về CPU\n","- `whitened_db_vectors[start_idx:end_idx, tensor_index, :] = X_whitened.cpu()`\n","\n","Mục đích:\n","- không giữ toàn bộ output trên GPU\n","- chỉ compute trên GPU theo batch, rồi lưu về CPU"],"metadata":{"id":"v1dmjCTbH-9H"}},{"cell_type":"markdown","source":["## 5) Giải phóng bộ nhớ sau mỗi layer\n","Sau khi xử lý xong 1 layer:\n","- Xoá `mean` và `W` khỏi dict để giải phóng VRAM:\n","  - `del mahalanobis_data['means'][layer_name]`\n","  - `del mahalanobis_data['whitening_matrices'][layer_name]`\n","- Dọn VRAM:\n","  - `torch.cuda.empty_cache()`\n","\n","Điểm lợi:\n","- Nếu ma trận `(2304 x 2304)` khá lớn, giữ 12 layer cùng lúc sẽ tốn nhiều VRAM."],"metadata":{"id":"FElhy3flIAaz"}},{"cell_type":"markdown","source":["## 6) Lưu database đã whiten\n","- `torch.save(whitened_db_vectors, output_db_file)`\n","\n","Output là tensor float32 trên CPU, shape:\n","- `(N_samples, 12, 2304)`"],"metadata":{"id":"IrCjWsjPIBzJ"}},{"cell_type":"markdown","source":["## Kết quả đầu ra\n","Sau khi chạy xong, bạn thu được file:\n","- `combined_train_db_vectors_v2_WHITENED.pt`\n","\n","File này dùng cho các bước sau như:\n","- xây dựng retrieval / nearest neighbor trên embedding đã whiten\n","- tính distance ổn định hơn (giảm tương quan giữa các chiều)\n","- chuẩn hoá theo từng layer để so sánh nhất quán"],"metadata":{"id":"5p2dQyxBIDOD"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381,"referenced_widgets":["edcb58bacdf54236b44d706e52c7f2b0","9ceb599c378644ce9115360ea4df9da9","9a32518cd1004d54839d54b59b634b9f","f32b95056d0645a2a1431f4147d4c902","4d2c0a960cbd4f05adf910f1327d1e89","805eff4f699346608751e30bf5119828","3819d6017f364961b822b5a95beb1a4f","122683b5c87148a4abb81d7bcf93b21b","65bd84812b64421190b4b534d7f28f09","fb81860d4f4148909b06ca04db823009","807baf6e05be4792bb22284eb6e09605"]},"id":"qOskeo5eik8S","executionInfo":{"status":"ok","timestamp":1766241523588,"user_tz":-420,"elapsed":79135,"user":{"displayName":"Trí Trần Đức","userId":"16647398495852939072"}},"outputId":"bc8f3db5-1ecd-4964-8295-7e149a0589cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu Bước 2b: 'Làm trắng' (Whitening) toàn bộ Database Train ---\n","Sử dụng thiết bị: cpu\n","File database input: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/cached_databases/combined_train_db_vectors_v2_inner_content.pt\n","File ma trận input: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/cached_databases/mahalanobis_data_v2_inner_content.pt\n","File database output: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/cached_databases/combined_train_db_vectors_v2_WHITENED.pt\n","\n","Đang tải database gốc: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/cached_databases/combined_train_db_vectors_v2_inner_content.pt...\n"," -> Tải thành công! Shape: (15521, 12, 2304)\n","Đang tải ma trận Mahalanobis: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/cached_databases/mahalanobis_data_v2_inner_content.pt...\n"," -> Tải và chuyển ma trận lên GPU thành công!\n","\n","Bắt đầu 'làm trắng' 15521 vector (theo batch 512)...\n"]},{"output_type":"display_data","data":{"text/plain":["Xử lý từng Layer:   0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edcb58bacdf54236b44d706e52c7f2b0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Đang lưu database đã 'làm trắng'...\n"," -> Đã lưu thành công database đã 'làm trắng' vào:\n","    /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/cached_databases/combined_train_db_vectors_v2_WHITENED.pt\n","\n","--- BƯỚC 2b ('LÀM TRẮNG' DB) HOÀN TẤT! ---\n"]}],"source":["# Import các thư viện cần thiết\n","import torch\n","import os\n","from tqdm.auto import tqdm\n","\n","print(\"--- Bắt đầu Bước 2b: 'Làm trắng' (Whitening) toàn bộ Database Train ---\")\n","\n","# --- 1. CẤU HÌNH ---\n","NUM_LAYERS = 12\n","D_FEATURES = 2304\n","BATCH_SIZE = 512 # Xử lý theo batch để tiết kiệm RAM/VRAM\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Sử dụng thiết bị: {device}\")\n","\n","# --- 2. THIẾT LẬP CÁC ĐƯỜNG DẪN ---\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","db_cache_dir = os.path.join(drive_base_path, 'cached_databases')\n","\n","# --- INPUT 1: Database GỐC (v2 - inner content) ---\n","db_vectors_file = os.path.join(db_cache_dir, 'combined_train_db_vectors_v2_inner_content.pt')\n","\n","# --- INPUT 2: File chứa các ma trận Mahalanobis (từ Bước 2a) ---\n","mahalanobis_data_file = os.path.join(db_cache_dir, 'mahalanobis_data_v2_inner_content.pt')\n","\n","# --- OUTPUT: File database đã \"làm trắng\" (whitened) ---\n","output_db_file = os.path.join(db_cache_dir, 'combined_train_db_vectors_v2_WHITENED.pt') # Tên file mới\n","\n","print(f\"File database input: {db_vectors_file}\")\n","print(f\"File ma trận input: {mahalanobis_data_file}\")\n","print(f\"File database output: {output_db_file}\")\n","\n","# --- 3. THỰC THI CHƯƠNG TRÌNH ---\n","if __name__ == \"__main__\":\n","\n","    # --- Tải Database gốc ---\n","    try:\n","        print(f\"\\nĐang tải database gốc: {db_vectors_file}...\")\n","        db_vectors = torch.load(db_vectors_file)\n","        N_samples, n_layers, D_features_loaded = db_vectors.shape\n","        print(f\" -> Tải thành công! Shape: ({N_samples}, {n_layers}, {D_features_loaded})\")\n","        assert n_layers == NUM_LAYERS and D_features_loaded == D_FEATURES\n","    except Exception as e:\n","        print(f\"LỖI: Không thể tải file database: {e}\")\n","        exit()\n","\n","    # --- Tải ma trận Mahalanobis ---\n","    try:\n","        print(f\"Đang tải ma trận Mahalanobis: {mahalanobis_data_file}...\")\n","        mahalanobis_data = torch.load(mahalanobis_data_file)\n","        # Chuyển ma trận lên GPU\n","        for i in range(NUM_LAYERS):\n","            layer_name = f\"layer_{i+1}\"\n","            mahalanobis_data['means'][layer_name] = mahalanobis_data['means'][layer_name].to(device)\n","            mahalanobis_data['whitening_matrices'][layer_name] = mahalanobis_data['whitening_matrices'][layer_name].to(device)\n","        print(\" -> Tải và chuyển ma trận lên GPU thành công!\")\n","    except Exception as e:\n","        print(f\"LỖI: Không thể tải file ma trận: {e}\")\n","        exit()\n","\n","    # --- \"Làm trắng\" (Whiten) cho từng lớp theo batch ---\n","    print(f\"\\nBắt đầu 'làm trắng' {N_samples} vector (theo batch {BATCH_SIZE})...\")\n","\n","    # Tạo tensor mới để lưu kết quả (đặt trên CPU để tiết kiệm VRAM)\n","    whitened_db_vectors = torch.empty_like(db_vectors, device='cpu', dtype=torch.float32)\n","\n","    with torch.no_grad():\n","        for i in tqdm(range(NUM_LAYERS), desc=\"Xử lý từng Layer\"):\n","            layer_name = f\"layer_{i+1}\"\n","            tensor_index = i\n","\n","            mean = mahalanobis_data['means'][layer_name]\n","            W = mahalanobis_data['whitening_matrices'][layer_name]\n","\n","            # Xử lý theo batch\n","            for start_idx in range(0, N_samples, BATCH_SIZE):\n","                end_idx = min(start_idx + BATCH_SIZE, N_samples)\n","\n","                # [B, 2304]\n","                X_batch = db_vectors[start_idx:end_idx, tensor_index, :].to(device)\n","\n","                # 1. Căn giữa (Center)\n","                X_centered = X_batch - mean # mean [2304] sẽ được broadcast\n","\n","                # 2. \"Làm trắng\" (Whiten)\n","                # [B, 2304] @ [2304, 2304] -> [B, 2304]\n","                X_whitened = torch.matmul(X_centered, W.T)\n","\n","                # 3. Lưu kết quả (chuyển về CPU)\n","                whitened_db_vectors[start_idx:end_idx, tensor_index, :] = X_whitened.cpu()\n","\n","            # Giải phóng ma trận sau khi xong layer\n","            del mahalanobis_data['means'][layer_name]\n","            del mahalanobis_data['whitening_matrices'][layer_name]\n","            torch.cuda.empty_cache()\n","\n","    # --- Lưu kết quả ---\n","    print(\"\\nĐang lưu database đã 'làm trắng'...\")\n","    try:\n","        torch.save(whitened_db_vectors, output_db_file)\n","        print(f\" -> Đã lưu thành công database đã 'làm trắng' vào:\\n    {output_db_file}\")\n","    except Exception as e:\n","        print(f\"LỖI: Không thể lưu file output: {e}\")\n","\n","    print(\"\\n--- BƯỚC 2b ('LÀM TRẮNG' DB) HOÀN TẤT! ---\")"]}]}