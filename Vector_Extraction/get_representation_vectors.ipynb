{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["3eBX0QvbOyWe","6cxxs7hzGUVI","1JPtSwscHGyy","t44OL7MJIZ9z","WB-W5WvmKY-v","3yYj8TNbO2uj","zfKi-2YkO-a9","46pfG-aCPyV_","fp1XWMiycVuo","c0Lq1RdXcyW5","dUIAzed5wLhh","Bck9yJjmwyyI","3YvKwoHKkuVE","wNhGE5RD61Il","w6BkBGOIk7sC","Zm4d2P-l8RZJ","StTxQU-k9kZ6","RM6TUv9YmIyo","Ilrp2GeF-B5b","2beo0OrxmJ28","j358DPd9emFF"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"991fc431045c4cc489d0c4966d63bca3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55b6a08653c14292b1238be5d679b6e4","IPY_MODEL_5ce9605bc6c24c6a82525c633fc26709","IPY_MODEL_58bd006313074d47abccc302c71e1dc6"],"layout":"IPY_MODEL_4c4c4dfd92bc471b9bd7504feef0c9a5"}},"55b6a08653c14292b1238be5d679b6e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40e9593937884673a54902f57b9f2659","placeholder":"​","style":"IPY_MODEL_7b065be7b31a41e3b5689898df674dbd","value":"config.json: "}},"5ce9605bc6c24c6a82525c633fc26709":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c15b3ae1ca764556bc8f46c6b78c56c7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_167856a43f3d42c08e9576af2d12c531","value":1}},"58bd006313074d47abccc302c71e1dc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48e3f4bcbb3f4e73a8aa28479c4efd4c","placeholder":"​","style":"IPY_MODEL_edf96196539b4c9db3fab8e98dc8bf10","value":" 1.11k/? [00:00&lt;00:00, 94.3kB/s]"}},"4c4c4dfd92bc471b9bd7504feef0c9a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40e9593937884673a54902f57b9f2659":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b065be7b31a41e3b5689898df674dbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c15b3ae1ca764556bc8f46c6b78c56c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"167856a43f3d42c08e9576af2d12c531":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48e3f4bcbb3f4e73a8aa28479c4efd4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edf96196539b4c9db3fab8e98dc8bf10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52cdecca0c7c4a1ab7c7aa09193c3dc1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83e64955c6c6414889f108862ba957c7","IPY_MODEL_08821349ef1642ac87f8763acadffd15","IPY_MODEL_b86881fe84c3493989ed5d38cf7f4dde"],"layout":"IPY_MODEL_4a89f694502b4b7083b1c5b5696be79f"}},"83e64955c6c6414889f108862ba957c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5814943f99f746528571fc0dc4da7fa9","placeholder":"​","style":"IPY_MODEL_86d238ce222744cbbfb5f447e32857f9","value":"vocab.txt: "}},"08821349ef1642ac87f8763acadffd15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf376927477c447dbf9f63add7a24dae","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce6fd865bce04fd4b44ab36eeffa65eb","value":1}},"b86881fe84c3493989ed5d38cf7f4dde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6da69a1537f14043ab92e7744511c543","placeholder":"​","style":"IPY_MODEL_f2980825b3934ece90a7e8d17cc7a206","value":" 213k/? [00:00&lt;00:00, 8.93MB/s]"}},"4a89f694502b4b7083b1c5b5696be79f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5814943f99f746528571fc0dc4da7fa9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86d238ce222744cbbfb5f447e32857f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf376927477c447dbf9f63add7a24dae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ce6fd865bce04fd4b44ab36eeffa65eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6da69a1537f14043ab92e7744511c543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2980825b3934ece90a7e8d17cc7a206":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"343a9bad648d42cc938164c514d59109":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_15bbcb45c5ad404291d8fc196b508560","IPY_MODEL_2c907cc39abd46b8a18db1c197bac116","IPY_MODEL_0dc10232179d437fabc847c5a5c3a8fc"],"layout":"IPY_MODEL_275680a264b9488ebd300ec0b7b18f35"}},"15bbcb45c5ad404291d8fc196b508560":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15ced74777e14ed7835b2632d034c583","placeholder":"​","style":"IPY_MODEL_49ed9b2d007a46fc88e461e44cde8103","value":"pytorch_model.bin: 100%"}},"2c907cc39abd46b8a18db1c197bac116":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_388da669ebad4965a6510eeba48ba014","max":435783451,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d047e6af81d41bba0e20536e9d726f5","value":435783451}},"0dc10232179d437fabc847c5a5c3a8fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64061be4e68a46aca3744116f5627150","placeholder":"​","style":"IPY_MODEL_37267da34f1f4510a1c58fea0e336f1e","value":" 436M/436M [00:16&lt;00:00, 25.2MB/s]"}},"275680a264b9488ebd300ec0b7b18f35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15ced74777e14ed7835b2632d034c583":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49ed9b2d007a46fc88e461e44cde8103":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"388da669ebad4965a6510eeba48ba014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d047e6af81d41bba0e20536e9d726f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64061be4e68a46aca3744116f5627150":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37267da34f1f4510a1c58fea0e336f1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a5c3176eefd4bd38cecd7e794399f24":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11a1f2bf37c248b983e035735784bc29","IPY_MODEL_35f94e9f1f764e8995b3369c233fa2d3","IPY_MODEL_438a29bdb3c849be86e6b0eae22f58cc"],"layout":"IPY_MODEL_117c3888d3454999b9a31e0be3e85558"}},"11a1f2bf37c248b983e035735784bc29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b76cf3f68f464d43a8a07579dac08cb9","placeholder":"​","style":"IPY_MODEL_d03c8e75dd0f445a8e30c4fea6066005","value":"model.safetensors: 100%"}},"35f94e9f1f764e8995b3369c233fa2d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bf4540e9701420b85fafedc68acb3f4","max":435755944,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83005a4a424f4a7db92d41b1b3ce4a49","value":435755944}},"438a29bdb3c849be86e6b0eae22f58cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67f7b7434912494f9f546cf063d29eaf","placeholder":"​","style":"IPY_MODEL_cd7341d3b7a8444eb0d3b2965f6dc1a8","value":" 436M/436M [00:15&lt;00:00, 41.3MB/s]"}},"117c3888d3454999b9a31e0be3e85558":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b76cf3f68f464d43a8a07579dac08cb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d03c8e75dd0f445a8e30c4fea6066005":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bf4540e9701420b85fafedc68acb3f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83005a4a424f4a7db92d41b1b3ce4a49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67f7b7434912494f9f546cf063d29eaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd7341d3b7a8444eb0d3b2965f6dc1a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Import các thư viện cần thiết\n","import torch\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel\n","import os\n","import json\n","import glob"],"metadata":{"id":"991Hv4MRGxIU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train Dataset - Mô hình Fine-tuned"],"metadata":{"id":"3eBX0QvbOyWe"}},{"cell_type":"markdown","source":["## Trích xuất hidden states theo từng động từ (GramVar) và lưu thành file `.pt`"],"metadata":{"id":"6cxxs7hzGUVI"}},{"cell_type":"markdown","source":["- **Mục đích**:  \n","  - Tải mô hình BioBERT đã fine-tune và tokenizer.  \n","  - Duyệt toàn bộ **tập train** (GramVar) theo **từng động từ** (mỗi file JSON = một động từ).  \n","  - Với **mỗi câu**, trích xuất **toàn bộ hidden states** (từng lớp của mô hình) và lưu thành các file `.pt`.  \n","  - Tổ chức kết quả theo cấu trúc thư mục: `.../Representation Vector/Train/Finetuned/train_vectors_finetuned_gramvar/<verb_name>/sentence_<i>.pt`."],"metadata":{"id":"j_Uze1ieGWeZ"}},{"cell_type":"markdown","source":["- **Quy trình**"],"metadata":{"id":"rIJYT90tC0oZ"}},{"cell_type":"markdown","source":["1. **Thiết lập & tải mô hình**  \n","     - Khai báo: `final_model_path`, `drive_base_path`, `train_data_dir`, `train_output_base_dir`.  \n","     - Tải `tokenizer` (BioBERT base) và **mô hình đã fine-tuned** từ `final_model_path`.  "],"metadata":{"id":"Qj75vojaGX3b"}},{"cell_type":"code","source":["# Đường dẫn đến thư mục chứa mô hình tốt nhất của bạn trên Google Drive\n","final_model_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Finetuned_Models/biobert-srl-best-model'\n","# Đường dẫn thư mục gốc trên Google Drive để làm việc\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file train theo động từ\n","train_data_dir = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_GramVar/Train'\n","\n","# Đường dẫn đến thư mục cha để lưu các vector của tập train\n","train_output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Train/Finetuned/train_vectors_finetuned_gramvar')\n","\n","# Tải tokenizer và mô hình\n","print(\"Đang tải tokenizer và mô hình fine-tuned...\")\n","tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n","model_ft = AutoModelForTokenClassification.from_pretrained(final_model_path)\n","print(\"Đã tải xong tokenizer và mô hình fine-tuned.\")"],"metadata":{"id":"NylQSk_WGaK5","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1759902753307,"user_tz":-420,"elapsed":11958,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"3551f11f-51c1-43ff-a216-f1be02ed3751"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Đang tải tokenizer và mô hình fine-tuned...\n","Đã tải xong tokenizer và mô hình fine-tuned.\n"]}]},{"cell_type":"markdown","source":["2. **Hàm cốt lõi**  "],"metadata":{"id":"9e4F-bKDGaWs"}},{"cell_type":"markdown","source":[" - `get_hidden_states(model, tokenizer, text)`  \n","       - Tokenize câu, chạy mô hình với `output_hidden_states=True`.  \n","       - Trả về `hidden_states` (tuple các lớp) và `tokens`.  "],"metadata":{"id":"b7mHxgy1Gcd7"}},{"cell_type":"code","source":["def get_hidden_states(model, tokenizer, text):\n","    \"\"\"\n","    Trích xuất các trạng thái ẩn (vector biểu diễn) từ mỗi lớp của mô hình.\n","    \"\"\"\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","    return outputs.hidden_states, tokens"],"metadata":{"id":"9AVpPsKoGePF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" - `process_and_save_per_verb(verb_name, sentences, model, tokenizer, output_dir)`  \n","       - Dùng cho **một động từ**: duyệt từng câu, trích xuất & lưu lần lượt `sentence_0.pt`, `sentence_1.pt`, …"],"metadata":{"id":"jyFh7onzGht1"}},{"cell_type":"code","source":["def process_and_save_per_verb(verb_name, sentences, model, tokenizer, output_dir):\n","    \"\"\"\n","    Xử lý một danh sách câu cho một động từ và lưu ra file riêng.\n","    \"\"\"\n","    print(f\"\\n--- Xử lý cho động từ: {verb_name} ---\")\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    total_sentences = len(sentences)\n","    for i, sentence in enumerate(sentences):\n","        if not sentence.strip():\n","            continue\n","\n","        print(f\"Đang xử lý câu {i+1}/{total_sentences}...\")\n","\n","        hidden_states, tokens = get_hidden_states(model, tokenizer, sentence)\n","        vectors_to_save = {}\n","        for j, layer_rep in enumerate(hidden_states):\n","            vectors_to_save[f\"layer_{j}\"] = layer_rep.squeeze(0)\n","\n","        file_name = f\"sentence_{i}.pt\"\n","        output_path = os.path.join(output_dir, file_name)\n","\n","        torch.save(vectors_to_save, output_path)"],"metadata":{"id":"LVFM0WUpGj9b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. **Thực thi**   \n","  - Dùng `glob` tìm tất cả file `*.json` trong `Split_GramVar/Train`.  \n","  - Từ tên file (ví dụ `begin_1_train_set.json`) suy ra `verb_name = begin_1`.  \n","  - Đọc JSON, lấy danh sách `text` (lọc rỗng), tạo thư mục `train_vectors_finetuned_gramvar/<verb_name>/`.  \n","  - Gọi `process_and_save_per_verb` để lưu toàn bộ hidden states cho từng câu.\n"],"metadata":{"id":"HZDQz56oGnhT"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","    # Xử lý toàn bộ tập dữ liệu train theo từng động từ\n","    try:\n","        # Tìm tất cả các file .json trong thư mục train\n","        json_files = glob.glob(os.path.join(train_data_dir, '*.json'))\n","\n","        if not json_files:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong thư mục: {train_data_dir}\")\n","        else:\n","            print(f\"\\nTìm thấy {len(json_files)} file .json để xử lý.\")\n","\n","        # Lặp qua từng file json\n","        for file_path in json_files:\n","            # Lấy tên file gốc (ví dụ: 'begin_1_train_set.json')\n","            base_name = os.path.basename(file_path)\n","            # Tách các phần bằng dấu '_'\n","            parts = base_name.replace('_train_set.json', '').split('_')\n","            # Nối lại các phần đầu để tạo tên động từ (ví dụ: 'begin_1')\n","            verb_name = \"_\".join(parts)\n","\n","            # Đọc dữ liệu từ file JSON\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Tạo thư mục output riêng cho động từ này\n","            verb_output_dir = os.path.join(train_output_base_dir, verb_name)\n","\n","            # Xử lý và lưu các câu cho động từ hiện tại\n","            process_and_save_per_verb(verb_name, sentences, model_ft, tokenizer, verb_output_dir)\n","\n","    except FileNotFoundError:\n","        print(f\"\\nLỖI: Không tìm thấy thư mục dữ liệu train tại '{train_data_dir}'.\")\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")"],"metadata":{"id":"nSbcus9aGofL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Kết quả**:  \n","  - Sinh cấu trúc output theo từng động từ, ví dụ:  \n","    ```\n","    .../train_vectors_finetuned_gramvar/\n","      └── begin_1/\n","          ├── sentence_0.pt\n","          ├── sentence_1.pt\n","          └── ...\n","    ```  \n","  - Mỗi file `.pt` là một dict:  \n","    - Key: `layer_0`, `layer_1`, … (embedding + các lớp Transformer).  \n","    - Value: tensor kích thước `[num_tokens, hidden_size]` (với BioBERT base: `hidden_size = 768`).  "],"metadata":{"id":"GC-HcadcGoq_"}},{"cell_type":"markdown","source":["- **Lưu ý**:  \n","  - Đảm bảo `final_model_path` và `train_data_dir` trỏ đúng tới Drive.  \n","  - Các câu rỗng sẽ bị bỏ qua (`.strip()`).  \n","  - Nếu muốn tái lập thứ tự xử lý/ghi log, có thể thiết lập seed trước khi duyệt.  \n","  - Các file JSON đầu vào cần có trường `text` trong từng phần tử."],"metadata":{"id":"kjUeGkl7GsFN"}},{"cell_type":"markdown","source":["## Tạo file index cho từng động từ (GramVar)"],"metadata":{"id":"1JPtSwscHGyy"}},{"cell_type":"markdown","source":["- **Mục đích**:  \n","  - Tự động sinh các **file index `.json`** cho từng động từ trong tập **Train**.  \n","  - Mỗi file index liệt kê **toàn bộ câu** và **đường dẫn file vector `.pt`** tương ứng đã/ sẽ được trích xuất.  \n","  - Hỗ trợ tra cứu nhanh: từ (động từ, chỉ số câu) → mở đúng file vector để phân tích."],"metadata":{"id":"ey9WMjW9Hgz2"}},{"cell_type":"markdown","source":["- **Quy trình**"],"metadata":{"id":"r9ixfeuoC3d5"}},{"cell_type":"markdown","source":["  1. **Thiết lập đường dẫn**  \n","     - `train_data_dir`: nơi chứa các file train theo động từ (ví dụ `begin_1_train_set.json`).  \n","     - `train_output_base_dir`: thư mục cha chứa các vector đã lưu theo động từ (mỗi câu = một file `.pt`).  \n","     - `output_index_dir`: nơi lưu **các file index** (mỗi động từ một file JSON).  "],"metadata":{"id":"dYDULnmfHjDw"}},{"cell_type":"code","source":["# Đường dẫn thư mục gốc trên Google Drive để làm việc\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file train gốc (.json)\n","train_data_dir = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_GramVar/Train'\n","\n","# Đường dẫn đến thư mục cha chứa các vector đã được xử lý\n","train_output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Train/Finetuned/train_vectors_finetuned_gramvar')\n","\n","# Đường dẫn đến thư mục để lưu các file index JSON riêng lẻ\n","output_index_dir = os.path.join(drive_base_path, 'Representation Vector/Train_Index/Finetuned/verb_indexes_finetuned_gramvar_train')"],"metadata":{"id":"OYgLr8SLHlEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" 2. **Hàm `create_sentence_index_per_verb()`**  \n","     - Quét tất cả `*.json` trong `train_data_dir`.  \n","     - Suy ra `verb_name` từ tên file (vd: `begin_1_train_set.json` → `begin_1`).  \n","     - Đọc dữ liệu JSON, lấy danh sách câu từ trường `text` (lọc rỗng).  \n","     - Với mỗi câu `i`, tạo entry:  \n","       - `sentence_text`: câu gốc  \n","       - `verb_name`: tên động từ  \n","       - `vector_file_path`: đường dẫn dự kiến đến file vector (vd: `<train_vectors_finetuned_gramvar>/<verb_name>/sentence_i.pt`)  \n","     - Ghi toàn bộ entry ra file `output_index_dir/<verb_name>.json`.  "],"metadata":{"id":"vzK3Ia5mHlR2"}},{"cell_type":"code","source":["def create_sentence_index_per_verb():\n","    \"\"\"\n","    Quét qua các file dữ liệu train và tạo ra một file index .json riêng\n","    cho mỗi động từ.\n","    \"\"\"\n","\n","    print(\"Bắt đầu quá trình tạo file index cho từng động từ...\")\n","\n","    # Tạo thư mục chứa các file index nếu nó chưa tồn tại\n","    os.makedirs(output_index_dir, exist_ok=True)\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục train gốc\n","        json_files = glob.glob(os.path.join(train_data_dir, '*.json'))\n","\n","        if not json_files:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong thư mục: {train_data_dir}\")\n","            return\n","\n","        print(f\"Tìm thấy {len(json_files)} file .json để tạo index.\")\n","\n","        # Lặp qua từng file json\n","        for file_path in json_files:\n","            # Lấy tên động từ từ tên file (ví dụ: 'begin_1')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_train_set.json', '').split('_'))\n","\n","            print(f\"\\n--- Đang xử lý cho động từ: {verb_name} ---\")\n","\n","            # Khởi tạo một danh sách để chứa index cho động từ hiện tại\n","            verb_index_list = []\n","\n","            # Đọc dữ liệu từ file JSON\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Lặp qua từng câu để tạo entry cho file index\n","            for i, sentence in enumerate(sentences):\n","                # Xây dựng đường dẫn dự kiến đến file vector .pt tương ứng\n","                vector_file_name = f\"sentence_{i}.pt\"\n","                vector_file_path = os.path.join(train_output_base_dir, verb_name, vector_file_name)\n","\n","                # Tạo một entry cho file index\n","                index_entry = {\n","                    \"sentence_text\": sentence,\n","                    \"verb_name\": verb_name,\n","                    \"vector_file_path\": vector_file_path\n","                }\n","                verb_index_list.append(index_entry)\n","\n","            # Sau khi xử lý xong các câu của một động từ, lưu ra file JSON riêng\n","            if verb_index_list:\n","                output_verb_index_file = os.path.join(output_index_dir, f\"{verb_name}.json\")\n","                print(f\"Đang lưu file index cho '{verb_name}' vào: {output_verb_index_file}\")\n","                with open(output_verb_index_file, 'w', encoding='utf-8') as f:\n","                    # Lưu danh sách các entry, không phải dictionary\n","                    json.dump(verb_index_list, f, ensure_ascii=False, indent=4)\n","                print(f\"Đã tạo file index cho {len(sentences)} câu của động từ '{verb_name}'.\")\n","\n","        print(\"\\nQUÁ TRÌNH TẠO FILE INDEX ĐÃ HOÀN TẤT!\")\n","\n","    except FileNotFoundError:\n","        print(f\"\\nLỖI: Không tìm thấy thư mục dữ liệu train tại '{train_data_dir}'.\")\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")"],"metadata":{"id":"DP40dciPHnjP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. **Thực thi**  \n","     - Gọi `create_sentence_index_per_verb()` trong `if __name__ == \"__main__\":`.  \n","     - In log tiến trình: số file train được quét, từng động từ đang xử lý, nơi lưu file index, v.v."],"metadata":{"id":"C_1PrlZSHnur"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    create_sentence_index_per_verb()"],"metadata":{"id":"xc_8JwtaHy5b","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1759903167301,"user_tz":-420,"elapsed":29328,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"3886b187-6e66-4842-bb17-268d973c84df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bắt đầu quá trình tạo file index cho từng động từ...\n","Tìm thấy 35 file .json để tạo index.\n","\n","--- Đang xử lý cho động từ: begin_1 ---\n","Đang lưu file index cho 'begin_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/begin_1.json\n","Đã tạo file index cho 96 câu của động từ 'begin_1'.\n","\n","--- Đang xử lý cho động từ: begin_2 ---\n","Đang lưu file index cho 'begin_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/begin_2.json\n","Đã tạo file index cho 137 câu của động từ 'begin_2'.\n","\n","--- Đang xử lý cho động từ: translate_2 ---\n","Đang lưu file index cho 'translate_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/translate_2.json\n","Đã tạo file index cho 149 câu của động từ 'translate_2'.\n","\n","--- Đang xử lý cho động từ: transform_1 ---\n","Đang lưu file index cho 'transform_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/transform_1.json\n","Đã tạo file index cho 94 câu của động từ 'transform_1'.\n","\n","--- Đang xử lý cho động từ: confer ---\n","Đang lưu file index cho 'confer' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/confer.json\n","Đã tạo file index cho 190 câu của động từ 'confer'.\n","\n","--- Đang xử lý cho động từ: delete ---\n","Đang lưu file index cho 'delete' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/delete.json\n","Đã tạo file index cho 61 câu của động từ 'delete'.\n","\n","--- Đang xử lý cho động từ: splice_2 ---\n","Đang lưu file index cho 'splice_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/splice_2.json\n","Đã tạo file index cho 159 câu của động từ 'splice_2'.\n","\n","--- Đang xử lý cho động từ: translate_1 ---\n","Đang lưu file index cho 'translate_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/translate_1.json\n","Đã tạo file index cho 172 câu của động từ 'translate_1'.\n","\n","--- Đang xử lý cho động từ: inhibit ---\n","Đang lưu file index cho 'inhibit' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/inhibit.json\n","Đã tạo file index cho 120 câu của động từ 'inhibit'.\n","\n","--- Đang xử lý cho động từ: proliferate ---\n","Đang lưu file index cho 'proliferate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/proliferate.json\n","Đã tạo file index cho 143 câu của động từ 'proliferate'.\n","\n","--- Đang xử lý cho động từ: result ---\n","Đang lưu file index cho 'result' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/result.json\n","Đã tạo file index cho 241 câu của động từ 'result'.\n","\n","--- Đang xử lý cho động từ: modify ---\n","Đang lưu file index cho 'modify' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/modify.json\n","Đã tạo file index cho 183 câu của động từ 'modify'.\n","\n","--- Đang xử lý cho động từ: lead ---\n","Đang lưu file index cho 'lead' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/lead.json\n","Đã tạo file index cho 84 câu của động từ 'lead'.\n","\n","--- Đang xử lý cho động từ: decrease_2 ---\n","Đang lưu file index cho 'decrease_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/decrease_2.json\n","Đã tạo file index cho 32 câu của động từ 'decrease_2'.\n","\n","--- Đang xử lý cho động từ: splice_1 ---\n","Đang lưu file index cho 'splice_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/splice_1.json\n","Đã tạo file index cho 167 câu của động từ 'splice_1'.\n","\n","--- Đang xử lý cho động từ: lose ---\n","Đang lưu file index cho 'lose' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/lose.json\n","Đã tạo file index cho 142 câu của động từ 'lose'.\n","\n","--- Đang xử lý cho động từ: transform_2 ---\n","Đang lưu file index cho 'transform_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/transform_2.json\n","Đã tạo file index cho 120 câu của động từ 'transform_2'.\n","\n","--- Đang xử lý cho động từ: eliminate ---\n","Đang lưu file index cho 'eliminate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/eliminate.json\n","Đã tạo file index cho 70 câu của động từ 'eliminate'.\n","\n","--- Đang xử lý cho động từ: encode ---\n","Đang lưu file index cho 'encode' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/encode.json\n","Đã tạo file index cho 79 câu của động từ 'encode'.\n","\n","--- Đang xử lý cho động từ: block ---\n","Đang lưu file index cho 'block' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/block.json\n","Đã tạo file index cho 145 câu của động từ 'block'.\n","\n","--- Đang xử lý cho động từ: develop ---\n","Đang lưu file index cho 'develop' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/develop.json\n","Đã tạo file index cho 128 câu của động từ 'develop'.\n","\n","--- Đang xử lý cho động từ: decrease_1 ---\n","Đang lưu file index cho 'decrease_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/decrease_1.json\n","Đã tạo file index cho 201 câu của động từ 'decrease_1'.\n","\n","--- Đang xử lý cho động từ: recognize ---\n","Đang lưu file index cho 'recognize' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/recognize.json\n","Đã tạo file index cho 61 câu của động từ 'recognize'.\n","\n","--- Đang xử lý cho động từ: abolish ---\n","Đang lưu file index cho 'abolish' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/abolish.json\n","Đã tạo file index cho 84 câu của động từ 'abolish'.\n","\n","--- Đang xử lý cho động từ: translate_3 ---\n","Đang lưu file index cho 'translate_3' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/translate_3.json\n","Đã tạo file index cho 237 câu của động từ 'translate_3'.\n","\n","--- Đang xử lý cho động từ: alter ---\n","Đang lưu file index cho 'alter' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/alter.json\n","Đã tạo file index cho 111 câu của động từ 'alter'.\n","\n","--- Đang xử lý cho động từ: express ---\n","Đang lưu file index cho 'express' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/express.json\n","Đã tạo file index cho 113 câu của động từ 'express'.\n","\n","--- Đang xử lý cho động từ: catalyse ---\n","Đang lưu file index cho 'catalyse' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/catalyse.json\n","Đã tạo file index cho 71 câu của động từ 'catalyse'.\n","\n","--- Đang xử lý cho động từ: generate ---\n","Đang lưu file index cho 'generate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/generate.json\n","Đã tạo file index cho 79 câu của động từ 'generate'.\n","\n","--- Đang xử lý cho động từ: initiate ---\n","Đang lưu file index cho 'initiate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/initiate.json\n","Đã tạo file index cho 129 câu của động từ 'initiate'.\n","\n","--- Đang xử lý cho động từ: skip ---\n","Đang lưu file index cho 'skip' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/skip.json\n","Đã tạo file index cho 168 câu của động từ 'skip'.\n","\n","--- Đang xử lý cho động từ: mutate ---\n","Đang lưu file index cho 'mutate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/mutate.json\n","Đã tạo file index cho 360 câu của động từ 'mutate'.\n","\n","--- Đang xử lý cho động từ: truncate ---\n","Đang lưu file index cho 'truncate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/truncate.json\n","Đã tạo file index cho 201 câu của động từ 'truncate'.\n","\n","--- Đang xử lý cho động từ: transcribe ---\n","Đang lưu file index cho 'transcribe' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/transcribe.json\n","Đã tạo file index cho 317 câu của động từ 'transcribe'.\n","\n","--- Đang xử lý cho động từ: disrupt ---\n","Đang lưu file index cho 'disrupt' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_gramvar_finetuned/disrupt.json\n","Đã tạo file index cho 53 câu của động từ 'disrupt'.\n","\n","QUÁ TRÌNH TẠO FILE INDEX ĐÃ HOÀN TẤT!\n"]}]},{"cell_type":"markdown","source":["- **Kết quả**:  \n","  - Thư mục `verb_indexes_finetuned_gramvar_train/` chứa một loạt file JSON, mỗi file ứng với **một động từ**:  \n","    ```\n","    .../verb_indexes_finetuned_gramvar_train\n","      ├── begin_1.json\n","      ├── block.json\n","      └── ...\n","    ```  \n","  - Mỗi file là **một danh sách** các entry có dạng:\n","    ```json\n","    {\n","      \"sentence_text\": \"...\",\n","      \"verb_name\": \"begin_1\",\n","      \"vector_file_path\": \"/.../train_vectors_finetuned_gramvar/begin_1/sentence_0.pt\"\n","    }\n","    ```  \n","  - Sẵn sàng sử dụng cho các bước phân tích sau (ví dụ: load vector theo `verb` + `sentence_index`)."],"metadata":{"id":"Wn2Mq-3tHu9E"}},{"cell_type":"markdown","source":["- **Lưu ý**:  \n","  - Đảm bảo cấu trúc tên file train kết thúc bằng `_train_set.json`.  \n","  - Các vector `.pt` cần được lưu theo quy ước cùng thư mục `train_vectors_finetuned_gramvar/<verb_name>/sentence_i.pt`.  \n","  - Nếu chưa tạo vector, index vẫn có thể sinh ra trước (đường dẫn “dự kiến”); khi trích xuất xong sẽ khớp đúng."],"metadata":{"id":"6Dsvn50GHzYf"}},{"cell_type":"markdown","source":["## Trích xuất hidden states theo từng động từ (ParaVE) và lưu thành file `.pt`"],"metadata":{"id":"t44OL7MJIZ9z"}},{"cell_type":"markdown","source":["- **Mục đích**:  \n","  - Tải mô hình BioBERT đã fine-tune và tokenizer.  \n","  - Duyệt **tập train của ParaVE** theo **từng động từ** (mỗi file JSON ứng với một động từ).  \n","  - Với **mỗi câu**, trích xuất **toàn bộ hidden states** (mọi lớp của mô hình) và lưu thành các file `.pt`.  \n","  - Tổ chức kết quả theo cấu trúc thư mục: `.../Representation Vector/Train/Finetunedtrain_vectors_finetuned_parave/<verb_name>/sentence_<i>.pt`."],"metadata":{"id":"3HE87YvHIcJW"}},{"cell_type":"markdown","source":["- **Quy trình**"],"metadata":{"id":"4GBmH1i6C6Ac"}},{"cell_type":"markdown","source":["  1. **Thiết lập & tải mô hình**  \n","     - Khai báo: `final_model_path`, `drive_base_path`, `train_data_dir` (trỏ vào `Split_ParaVE/Train`), `train_output_base_dir`.  \n","     - Tải `tokenizer` (BioBERT base) và **mô hình đã fine-tuned** từ `final_model_path`.  "],"metadata":{"id":"PRns0XFiIgT8"}},{"cell_type":"code","source":["# Đường dẫn đến thư mục chứa mô hình tốt nhất của bạn trên Google Drive\n","final_model_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Finetuned_Models/biobert-srl-best-model'\n","# Đường dẫn thư mục gốc trên Google Drive để làm việc\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","#  Đường dẫn đến thư mục chứa các file train theo động từ\n","train_data_dir = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_ParaVE/Train'\n","\n","# Đường dẫn đến thư mục cha để lưu các vector của tập train\n","train_output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Train/Finetuned/train_vectors_finetuned_parave')\n","\n","# Tải tokenizer và mô hình\n","print(\"Đang tải tokenizer và mô hình fine-tuned...\")\n","tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n","model_ft = AutoModelForTokenClassification.from_pretrained(final_model_path)\n","print(\"Đã tải xong tokenizer và mô hình fine-tuned.\")"],"metadata":{"id":"4aA4vkzUIpn7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" 2. **Hàm cốt lõi**  "],"metadata":{"id":"OmOZU_pVJ6pY"}},{"cell_type":"markdown","source":["- `get_hidden_states(model, tokenizer, text)`  \n","       - Tokenize câu, gọi mô hình với `output_hidden_states=True`.  \n","       - Trả về `hidden_states` (tuple các lớp) và `tokens`."],"metadata":{"id":"8BDU9XusJ7Zx"}},{"cell_type":"code","source":["def get_hidden_states(model, tokenizer, text):\n","    \"\"\"\n","    Trích xuất các trạng thái ẩn (vector biểu diễn) từ mỗi lớp của mô hình.\n","    \"\"\"\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","    return outputs.hidden_states, tokens"],"metadata":{"id":"gA9WIo5xKBQI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- `process_and_save_per_verb(verb_name, sentences, model, tokenizer, output_dir)`  \n","       - Dùng cho **một động từ**: duyệt từng câu, trích xuất & lưu lần lượt `sentence_0.pt`, `sentence_1.pt`, …"],"metadata":{"id":"T2mG12CLKAbu"}},{"cell_type":"code","source":["def process_and_save_per_verb(verb_name, sentences, model, tokenizer, output_dir):\n","    \"\"\"\n","    Xử lý một danh sách câu cho một động từ và lưu ra file riêng.\n","    \"\"\"\n","    print(f\"\\n--- Xử lý cho động từ: {verb_name} ---\")\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    total_sentences = len(sentences)\n","    for i, sentence in enumerate(sentences):\n","        if not sentence.strip():\n","            continue\n","\n","        print(f\"  Đang xử lý câu {i+1}/{total_sentences}...\")\n","\n","        hidden_states, tokens = get_hidden_states(model, tokenizer, sentence)\n","        vectors_to_save = {}\n","        for j, layer_rep in enumerate(hidden_states):\n","            vectors_to_save[f\"layer_{j}\"] = layer_rep.squeeze(0)\n","\n","        file_name = f\"sentence_{i}.pt\"\n","        output_path = os.path.join(output_dir, file_name)\n","\n","        torch.save(vectors_to_save, output_path)\n","\n","    print(f\"Đã xử lý xong {total_sentences} câu cho động từ '{verb_name}'.\")"],"metadata":{"id":"bEBrz0rLKB8T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. **Thực thi**  \n","- Dùng `glob` tìm tất cả file `*.json` trong `Split_ParaVE/Train`.  \n","- Từ tên file (vd: `begin_1_train_set.json`) suy ra `verb_name = begin_1`.  \n","- Đọc JSON, lấy danh sách `text` (lọc rỗng), tạo thư mục `train_vectors_finetuned_parave/<verb_name>/`.  \n","- Gọi `process_and_save_per_verb` để lưu hidden states cho toàn bộ câu của động từ đó."],"metadata":{"id":"ieE9T7afJyqC"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","    # Xử lý toàn bộ tập dữ liệu train theo từng động từ\n","    try:\n","        # Tìm tất cả các file .json trong thư mục train\n","        json_files = glob.glob(os.path.join(train_data_dir, '*.json'))\n","\n","        if not json_files:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong thư mục: {train_data_dir}\")\n","        else:\n","            print(f\"\\nTìm thấy {len(json_files)} file .json để xử lý.\")\n","\n","        # Lặp qua từng file json\n","        for file_path in json_files:\n","            # Lấy tên file gốc (ví dụ: 'begin_1_train_set.json')\n","            base_name = os.path.basename(file_path)\n","            # Tách các phần bằng dấu '_'\n","            parts = base_name.replace('_train_set.json', '').split('_')\n","            # Nối lại các phần đầu để tạo tên động từ (ví dụ: 'begin_1')\n","            verb_name = \"_\".join(parts)\n","\n","            # Đọc dữ liệu từ file JSON\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Tạo thư mục output riêng cho động từ này\n","            verb_output_dir = os.path.join(train_output_base_dir, verb_name)\n","\n","            # Xử lý và lưu các câu cho động từ hiện tại\n","            process_and_save_per_verb(verb_name, sentences, model_ft, tokenizer, verb_output_dir)\n","\n","    except FileNotFoundError:\n","        print(f\"\\nLỖI: Không tìm thấy thư mục dữ liệu train tại '{train_data_dir}'.\")\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RO1-QoKIp2Hn","executionInfo":{"status":"ok","timestamp":1759505852523,"user_tz":-420,"elapsed":461223,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"56543b59-a07e-47c1-9408-134f2da2acd3","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Đang tải tokenizer và mô hình fine-tuned...\n","Đã tải xong tokenizer và mô hình fine-tuned.\n","\n","--- Xử lý câu test ---\n","Câu: 'the net load of the variant subunit may result in a variant electrophoretic subunit LDH-B .'\n","ĐÃ LƯU THÀNH CÔNG! Vector của câu test được lưu tại: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/test_sentence_vector.pt\n","\n","Tìm thấy 35 file .json để xử lý.\n","\n","--- Xử lý cho động từ: catalyse ---\n","  Đang xử lý câu 1/43...\n","  Đang xử lý câu 2/43...\n","  Đang xử lý câu 3/43...\n","  Đang xử lý câu 4/43...\n","  Đang xử lý câu 5/43...\n","  Đang xử lý câu 6/43...\n","  Đang xử lý câu 7/43...\n","  Đang xử lý câu 8/43...\n","  Đang xử lý câu 9/43...\n","  Đang xử lý câu 10/43...\n","  Đang xử lý câu 11/43...\n","  Đang xử lý câu 12/43...\n","  Đang xử lý câu 13/43...\n","  Đang xử lý câu 14/43...\n","  Đang xử lý câu 15/43...\n","  Đang xử lý câu 16/43...\n","  Đang xử lý câu 17/43...\n","  Đang xử lý câu 18/43...\n","  Đang xử lý câu 19/43...\n","  Đang xử lý câu 20/43...\n","  Đang xử lý câu 21/43...\n","  Đang xử lý câu 22/43...\n","  Đang xử lý câu 23/43...\n","  Đang xử lý câu 24/43...\n","  Đang xử lý câu 25/43...\n","  Đang xử lý câu 26/43...\n","  Đang xử lý câu 27/43...\n","  Đang xử lý câu 28/43...\n","  Đang xử lý câu 29/43...\n","  Đang xử lý câu 30/43...\n","  Đang xử lý câu 31/43...\n","  Đang xử lý câu 32/43...\n","  Đang xử lý câu 33/43...\n","  Đang xử lý câu 34/43...\n","  Đang xử lý câu 35/43...\n","  Đang xử lý câu 36/43...\n","  Đang xử lý câu 37/43...\n","  Đang xử lý câu 38/43...\n","  Đang xử lý câu 39/43...\n","  Đang xử lý câu 40/43...\n","  Đang xử lý câu 41/43...\n","  Đang xử lý câu 42/43...\n","  Đang xử lý câu 43/43...\n","Đã xử lý xong 43 câu cho động từ 'catalyse'.\n","\n","--- Xử lý cho động từ: result ---\n","  Đang xử lý câu 1/218...\n","  Đang xử lý câu 2/218...\n","  Đang xử lý câu 3/218...\n","  Đang xử lý câu 4/218...\n","  Đang xử lý câu 5/218...\n","  Đang xử lý câu 6/218...\n","  Đang xử lý câu 7/218...\n","  Đang xử lý câu 8/218...\n","  Đang xử lý câu 9/218...\n","  Đang xử lý câu 10/218...\n","  Đang xử lý câu 11/218...\n","  Đang xử lý câu 12/218...\n","  Đang xử lý câu 13/218...\n","  Đang xử lý câu 14/218...\n","  Đang xử lý câu 15/218...\n","  Đang xử lý câu 16/218...\n","  Đang xử lý câu 17/218...\n","  Đang xử lý câu 18/218...\n","  Đang xử lý câu 19/218...\n","  Đang xử lý câu 20/218...\n","  Đang xử lý câu 21/218...\n","  Đang xử lý câu 22/218...\n","  Đang xử lý câu 23/218...\n","  Đang xử lý câu 24/218...\n","  Đang xử lý câu 25/218...\n","  Đang xử lý câu 26/218...\n","  Đang xử lý câu 27/218...\n","  Đang xử lý câu 28/218...\n","  Đang xử lý câu 29/218...\n","  Đang xử lý câu 30/218...\n","  Đang xử lý câu 31/218...\n","  Đang xử lý câu 32/218...\n","  Đang xử lý câu 33/218...\n","  Đang xử lý câu 34/218...\n","  Đang xử lý câu 35/218...\n","  Đang xử lý câu 36/218...\n","  Đang xử lý câu 37/218...\n","  Đang xử lý câu 38/218...\n","  Đang xử lý câu 39/218...\n","  Đang xử lý câu 40/218...\n","  Đang xử lý câu 41/218...\n","  Đang xử lý câu 42/218...\n","  Đang xử lý câu 43/218...\n","  Đang xử lý câu 44/218...\n","  Đang xử lý câu 45/218...\n","  Đang xử lý câu 46/218...\n","  Đang xử lý câu 47/218...\n","  Đang xử lý câu 48/218...\n","  Đang xử lý câu 49/218...\n","  Đang xử lý câu 50/218...\n","  Đang xử lý câu 51/218...\n","  Đang xử lý câu 52/218...\n","  Đang xử lý câu 53/218...\n","  Đang xử lý câu 54/218...\n","  Đang xử lý câu 55/218...\n","  Đang xử lý câu 56/218...\n","  Đang xử lý câu 57/218...\n","  Đang xử lý câu 58/218...\n","  Đang xử lý câu 59/218...\n","  Đang xử lý câu 60/218...\n","  Đang xử lý câu 61/218...\n","  Đang xử lý câu 62/218...\n","  Đang xử lý câu 63/218...\n","  Đang xử lý câu 64/218...\n","  Đang xử lý câu 65/218...\n","  Đang xử lý câu 66/218...\n","  Đang xử lý câu 67/218...\n","  Đang xử lý câu 68/218...\n","  Đang xử lý câu 69/218...\n","  Đang xử lý câu 70/218...\n","  Đang xử lý câu 71/218...\n","  Đang xử lý câu 72/218...\n","  Đang xử lý câu 73/218...\n","  Đang xử lý câu 74/218...\n","  Đang xử lý câu 75/218...\n","  Đang xử lý câu 76/218...\n","  Đang xử lý câu 77/218...\n","  Đang xử lý câu 78/218...\n","  Đang xử lý câu 79/218...\n","  Đang xử lý câu 80/218...\n","  Đang xử lý câu 81/218...\n","  Đang xử lý câu 82/218...\n","  Đang xử lý câu 83/218...\n","  Đang xử lý câu 84/218...\n","  Đang xử lý câu 85/218...\n","  Đang xử lý câu 86/218...\n","  Đang xử lý câu 87/218...\n","  Đang xử lý câu 88/218...\n","  Đang xử lý câu 89/218...\n","  Đang xử lý câu 90/218...\n","  Đang xử lý câu 91/218...\n","  Đang xử lý câu 92/218...\n","  Đang xử lý câu 93/218...\n","  Đang xử lý câu 94/218...\n","  Đang xử lý câu 95/218...\n","  Đang xử lý câu 96/218...\n","  Đang xử lý câu 97/218...\n","  Đang xử lý câu 98/218...\n","  Đang xử lý câu 99/218...\n","  Đang xử lý câu 100/218...\n","  Đang xử lý câu 101/218...\n","  Đang xử lý câu 102/218...\n","  Đang xử lý câu 103/218...\n","  Đang xử lý câu 104/218...\n","  Đang xử lý câu 105/218...\n","  Đang xử lý câu 106/218...\n","  Đang xử lý câu 107/218...\n","  Đang xử lý câu 108/218...\n","  Đang xử lý câu 109/218...\n","  Đang xử lý câu 110/218...\n","  Đang xử lý câu 111/218...\n","  Đang xử lý câu 112/218...\n","  Đang xử lý câu 113/218...\n","  Đang xử lý câu 114/218...\n","  Đang xử lý câu 115/218...\n","  Đang xử lý câu 116/218...\n","  Đang xử lý câu 117/218...\n","  Đang xử lý câu 118/218...\n","  Đang xử lý câu 119/218...\n","  Đang xử lý câu 120/218...\n","  Đang xử lý câu 121/218...\n","  Đang xử lý câu 122/218...\n","  Đang xử lý câu 123/218...\n","  Đang xử lý câu 124/218...\n","  Đang xử lý câu 125/218...\n","  Đang xử lý câu 126/218...\n","  Đang xử lý câu 127/218...\n","  Đang xử lý câu 128/218...\n","  Đang xử lý câu 129/218...\n","  Đang xử lý câu 130/218...\n","  Đang xử lý câu 131/218...\n","  Đang xử lý câu 132/218...\n","  Đang xử lý câu 133/218...\n","  Đang xử lý câu 134/218...\n","  Đang xử lý câu 135/218...\n","  Đang xử lý câu 136/218...\n","  Đang xử lý câu 137/218...\n","  Đang xử lý câu 138/218...\n","  Đang xử lý câu 139/218...\n","  Đang xử lý câu 140/218...\n","  Đang xử lý câu 141/218...\n","  Đang xử lý câu 142/218...\n","  Đang xử lý câu 143/218...\n","  Đang xử lý câu 144/218...\n","  Đang xử lý câu 145/218...\n","  Đang xử lý câu 146/218...\n","  Đang xử lý câu 147/218...\n","  Đang xử lý câu 148/218...\n","  Đang xử lý câu 149/218...\n","  Đang xử lý câu 150/218...\n","  Đang xử lý câu 151/218...\n","  Đang xử lý câu 152/218...\n","  Đang xử lý câu 153/218...\n","  Đang xử lý câu 154/218...\n","  Đang xử lý câu 155/218...\n","  Đang xử lý câu 156/218...\n","  Đang xử lý câu 157/218...\n","  Đang xử lý câu 158/218...\n","  Đang xử lý câu 159/218...\n","  Đang xử lý câu 160/218...\n","  Đang xử lý câu 161/218...\n","  Đang xử lý câu 162/218...\n","  Đang xử lý câu 163/218...\n","  Đang xử lý câu 164/218...\n","  Đang xử lý câu 165/218...\n","  Đang xử lý câu 166/218...\n","  Đang xử lý câu 167/218...\n","  Đang xử lý câu 168/218...\n","  Đang xử lý câu 169/218...\n","  Đang xử lý câu 170/218...\n","  Đang xử lý câu 171/218...\n","  Đang xử lý câu 172/218...\n","  Đang xử lý câu 173/218...\n","  Đang xử lý câu 174/218...\n","  Đang xử lý câu 175/218...\n","  Đang xử lý câu 176/218...\n","  Đang xử lý câu 177/218...\n","  Đang xử lý câu 178/218...\n","  Đang xử lý câu 179/218...\n","  Đang xử lý câu 180/218...\n","  Đang xử lý câu 181/218...\n","  Đang xử lý câu 182/218...\n","  Đang xử lý câu 183/218...\n","  Đang xử lý câu 184/218...\n","  Đang xử lý câu 185/218...\n","  Đang xử lý câu 186/218...\n","  Đang xử lý câu 187/218...\n","  Đang xử lý câu 188/218...\n","  Đang xử lý câu 189/218...\n","  Đang xử lý câu 190/218...\n","  Đang xử lý câu 191/218...\n","  Đang xử lý câu 192/218...\n","  Đang xử lý câu 193/218...\n","  Đang xử lý câu 194/218...\n","  Đang xử lý câu 195/218...\n","  Đang xử lý câu 196/218...\n","  Đang xử lý câu 197/218...\n","  Đang xử lý câu 198/218...\n","  Đang xử lý câu 199/218...\n","  Đang xử lý câu 200/218...\n","  Đang xử lý câu 201/218...\n","  Đang xử lý câu 202/218...\n","  Đang xử lý câu 203/218...\n","  Đang xử lý câu 204/218...\n","  Đang xử lý câu 205/218...\n","  Đang xử lý câu 206/218...\n","  Đang xử lý câu 207/218...\n","  Đang xử lý câu 208/218...\n","  Đang xử lý câu 209/218...\n","  Đang xử lý câu 210/218...\n","  Đang xử lý câu 211/218...\n","  Đang xử lý câu 212/218...\n","  Đang xử lý câu 213/218...\n","  Đang xử lý câu 214/218...\n","  Đang xử lý câu 215/218...\n","  Đang xử lý câu 216/218...\n","  Đang xử lý câu 217/218...\n","  Đang xử lý câu 218/218...\n","Đã xử lý xong 218 câu cho động từ 'result'.\n","\n","--- Xử lý cho động từ: develop ---\n","  Đang xử lý câu 1/10...\n","  Đang xử lý câu 2/10...\n","  Đang xử lý câu 3/10...\n","  Đang xử lý câu 4/10...\n","  Đang xử lý câu 5/10...\n","  Đang xử lý câu 6/10...\n","  Đang xử lý câu 7/10...\n","  Đang xử lý câu 8/10...\n","  Đang xử lý câu 9/10...\n","  Đang xử lý câu 10/10...\n","Đã xử lý xong 10 câu cho động từ 'develop'.\n","\n","--- Xử lý cho động từ: eliminate ---\n","  Đang xử lý câu 1/30...\n","  Đang xử lý câu 2/30...\n","  Đang xử lý câu 3/30...\n","  Đang xử lý câu 4/30...\n","  Đang xử lý câu 5/30...\n","  Đang xử lý câu 6/30...\n","  Đang xử lý câu 7/30...\n","  Đang xử lý câu 8/30...\n","  Đang xử lý câu 9/30...\n","  Đang xử lý câu 10/30...\n","  Đang xử lý câu 11/30...\n","  Đang xử lý câu 12/30...\n","  Đang xử lý câu 13/30...\n","  Đang xử lý câu 14/30...\n","  Đang xử lý câu 15/30...\n","  Đang xử lý câu 16/30...\n","  Đang xử lý câu 17/30...\n","  Đang xử lý câu 18/30...\n","  Đang xử lý câu 19/30...\n","  Đang xử lý câu 20/30...\n","  Đang xử lý câu 21/30...\n","  Đang xử lý câu 22/30...\n","  Đang xử lý câu 23/30...\n","  Đang xử lý câu 24/30...\n","  Đang xử lý câu 25/30...\n","  Đang xử lý câu 26/30...\n","  Đang xử lý câu 27/30...\n","  Đang xử lý câu 28/30...\n","  Đang xử lý câu 29/30...\n","  Đang xử lý câu 30/30...\n","Đã xử lý xong 30 câu cho động từ 'eliminate'.\n","\n","--- Xử lý cho động từ: translate_3 ---\n","  Đang xử lý câu 1/186...\n","  Đang xử lý câu 2/186...\n","  Đang xử lý câu 3/186...\n","  Đang xử lý câu 4/186...\n","  Đang xử lý câu 5/186...\n","  Đang xử lý câu 6/186...\n","  Đang xử lý câu 7/186...\n","  Đang xử lý câu 8/186...\n","  Đang xử lý câu 9/186...\n","  Đang xử lý câu 10/186...\n","  Đang xử lý câu 11/186...\n","  Đang xử lý câu 12/186...\n","  Đang xử lý câu 13/186...\n","  Đang xử lý câu 14/186...\n","  Đang xử lý câu 15/186...\n","  Đang xử lý câu 16/186...\n","  Đang xử lý câu 17/186...\n","  Đang xử lý câu 18/186...\n","  Đang xử lý câu 19/186...\n","  Đang xử lý câu 20/186...\n","  Đang xử lý câu 21/186...\n","  Đang xử lý câu 22/186...\n","  Đang xử lý câu 23/186...\n","  Đang xử lý câu 24/186...\n","  Đang xử lý câu 25/186...\n","  Đang xử lý câu 26/186...\n","  Đang xử lý câu 27/186...\n","  Đang xử lý câu 28/186...\n","  Đang xử lý câu 29/186...\n","  Đang xử lý câu 30/186...\n","  Đang xử lý câu 31/186...\n","  Đang xử lý câu 32/186...\n","  Đang xử lý câu 33/186...\n","  Đang xử lý câu 34/186...\n","  Đang xử lý câu 35/186...\n","  Đang xử lý câu 36/186...\n","  Đang xử lý câu 37/186...\n","  Đang xử lý câu 38/186...\n","  Đang xử lý câu 39/186...\n","  Đang xử lý câu 40/186...\n","  Đang xử lý câu 41/186...\n","  Đang xử lý câu 42/186...\n","  Đang xử lý câu 43/186...\n","  Đang xử lý câu 44/186...\n","  Đang xử lý câu 45/186...\n","  Đang xử lý câu 46/186...\n","  Đang xử lý câu 47/186...\n","  Đang xử lý câu 48/186...\n","  Đang xử lý câu 49/186...\n","  Đang xử lý câu 50/186...\n","  Đang xử lý câu 51/186...\n","  Đang xử lý câu 52/186...\n","  Đang xử lý câu 53/186...\n","  Đang xử lý câu 54/186...\n","  Đang xử lý câu 55/186...\n","  Đang xử lý câu 56/186...\n","  Đang xử lý câu 57/186...\n","  Đang xử lý câu 58/186...\n","  Đang xử lý câu 59/186...\n","  Đang xử lý câu 60/186...\n","  Đang xử lý câu 61/186...\n","  Đang xử lý câu 62/186...\n","  Đang xử lý câu 63/186...\n","  Đang xử lý câu 64/186...\n","  Đang xử lý câu 65/186...\n","  Đang xử lý câu 66/186...\n","  Đang xử lý câu 67/186...\n","  Đang xử lý câu 68/186...\n","  Đang xử lý câu 69/186...\n","  Đang xử lý câu 70/186...\n","  Đang xử lý câu 71/186...\n","  Đang xử lý câu 72/186...\n","  Đang xử lý câu 73/186...\n","  Đang xử lý câu 74/186...\n","  Đang xử lý câu 75/186...\n","  Đang xử lý câu 76/186...\n","  Đang xử lý câu 77/186...\n","  Đang xử lý câu 78/186...\n","  Đang xử lý câu 79/186...\n","  Đang xử lý câu 80/186...\n","  Đang xử lý câu 81/186...\n","  Đang xử lý câu 82/186...\n","  Đang xử lý câu 83/186...\n","  Đang xử lý câu 84/186...\n","  Đang xử lý câu 85/186...\n","  Đang xử lý câu 86/186...\n","  Đang xử lý câu 87/186...\n","  Đang xử lý câu 88/186...\n","  Đang xử lý câu 89/186...\n","  Đang xử lý câu 90/186...\n","  Đang xử lý câu 91/186...\n","  Đang xử lý câu 92/186...\n","  Đang xử lý câu 93/186...\n","  Đang xử lý câu 94/186...\n","  Đang xử lý câu 95/186...\n","  Đang xử lý câu 96/186...\n","  Đang xử lý câu 97/186...\n","  Đang xử lý câu 98/186...\n","  Đang xử lý câu 99/186...\n","  Đang xử lý câu 100/186...\n","  Đang xử lý câu 101/186...\n","  Đang xử lý câu 102/186...\n","  Đang xử lý câu 103/186...\n","  Đang xử lý câu 104/186...\n","  Đang xử lý câu 105/186...\n","  Đang xử lý câu 106/186...\n","  Đang xử lý câu 107/186...\n","  Đang xử lý câu 108/186...\n","  Đang xử lý câu 109/186...\n","  Đang xử lý câu 110/186...\n","  Đang xử lý câu 111/186...\n","  Đang xử lý câu 112/186...\n","  Đang xử lý câu 113/186...\n","  Đang xử lý câu 114/186...\n","  Đang xử lý câu 115/186...\n","  Đang xử lý câu 116/186...\n","  Đang xử lý câu 117/186...\n","  Đang xử lý câu 118/186...\n","  Đang xử lý câu 119/186...\n","  Đang xử lý câu 120/186...\n","  Đang xử lý câu 121/186...\n","  Đang xử lý câu 122/186...\n","  Đang xử lý câu 123/186...\n","  Đang xử lý câu 124/186...\n","  Đang xử lý câu 125/186...\n","  Đang xử lý câu 126/186...\n","  Đang xử lý câu 127/186...\n","  Đang xử lý câu 128/186...\n","  Đang xử lý câu 129/186...\n","  Đang xử lý câu 130/186...\n","  Đang xử lý câu 131/186...\n","  Đang xử lý câu 132/186...\n","  Đang xử lý câu 133/186...\n","  Đang xử lý câu 134/186...\n","  Đang xử lý câu 135/186...\n","  Đang xử lý câu 136/186...\n","  Đang xử lý câu 137/186...\n","  Đang xử lý câu 138/186...\n","  Đang xử lý câu 139/186...\n","  Đang xử lý câu 140/186...\n","  Đang xử lý câu 141/186...\n","  Đang xử lý câu 142/186...\n","  Đang xử lý câu 143/186...\n","  Đang xử lý câu 144/186...\n","  Đang xử lý câu 145/186...\n","  Đang xử lý câu 146/186...\n","  Đang xử lý câu 147/186...\n","  Đang xử lý câu 148/186...\n","  Đang xử lý câu 149/186...\n","  Đang xử lý câu 150/186...\n","  Đang xử lý câu 151/186...\n","  Đang xử lý câu 152/186...\n","  Đang xử lý câu 153/186...\n","  Đang xử lý câu 154/186...\n","  Đang xử lý câu 155/186...\n","  Đang xử lý câu 156/186...\n","  Đang xử lý câu 157/186...\n","  Đang xử lý câu 158/186...\n","  Đang xử lý câu 159/186...\n","  Đang xử lý câu 160/186...\n","  Đang xử lý câu 161/186...\n","  Đang xử lý câu 162/186...\n","  Đang xử lý câu 163/186...\n","  Đang xử lý câu 164/186...\n","  Đang xử lý câu 165/186...\n","  Đang xử lý câu 166/186...\n","  Đang xử lý câu 167/186...\n","  Đang xử lý câu 168/186...\n","  Đang xử lý câu 169/186...\n","  Đang xử lý câu 170/186...\n","  Đang xử lý câu 171/186...\n","  Đang xử lý câu 172/186...\n","  Đang xử lý câu 173/186...\n","  Đang xử lý câu 174/186...\n","  Đang xử lý câu 175/186...\n","  Đang xử lý câu 176/186...\n","  Đang xử lý câu 177/186...\n","  Đang xử lý câu 178/186...\n","  Đang xử lý câu 179/186...\n","  Đang xử lý câu 180/186...\n","  Đang xử lý câu 181/186...\n","  Đang xử lý câu 182/186...\n","  Đang xử lý câu 183/186...\n","  Đang xử lý câu 184/186...\n","  Đang xử lý câu 185/186...\n","  Đang xử lý câu 186/186...\n","Đã xử lý xong 186 câu cho động từ 'translate_3'.\n","\n","--- Xử lý cho động từ: transform_1 ---\n","  Đang xử lý câu 1/30...\n","  Đang xử lý câu 2/30...\n","  Đang xử lý câu 3/30...\n","  Đang xử lý câu 4/30...\n","  Đang xử lý câu 5/30...\n","  Đang xử lý câu 6/30...\n","  Đang xử lý câu 7/30...\n","  Đang xử lý câu 8/30...\n","  Đang xử lý câu 9/30...\n","  Đang xử lý câu 10/30...\n","  Đang xử lý câu 11/30...\n","  Đang xử lý câu 12/30...\n","  Đang xử lý câu 13/30...\n","  Đang xử lý câu 14/30...\n","  Đang xử lý câu 15/30...\n","  Đang xử lý câu 16/30...\n","  Đang xử lý câu 17/30...\n","  Đang xử lý câu 18/30...\n","  Đang xử lý câu 19/30...\n","  Đang xử lý câu 20/30...\n","  Đang xử lý câu 21/30...\n","  Đang xử lý câu 22/30...\n","  Đang xử lý câu 23/30...\n","  Đang xử lý câu 24/30...\n","  Đang xử lý câu 25/30...\n","  Đang xử lý câu 26/30...\n","  Đang xử lý câu 27/30...\n","  Đang xử lý câu 28/30...\n","  Đang xử lý câu 29/30...\n","  Đang xử lý câu 30/30...\n","Đã xử lý xong 30 câu cho động từ 'transform_1'.\n","\n","--- Xử lý cho động từ: begin_2 ---\n","  Đang xử lý câu 1/186...\n","  Đang xử lý câu 2/186...\n","  Đang xử lý câu 3/186...\n","  Đang xử lý câu 4/186...\n","  Đang xử lý câu 5/186...\n","  Đang xử lý câu 6/186...\n","  Đang xử lý câu 7/186...\n","  Đang xử lý câu 8/186...\n","  Đang xử lý câu 9/186...\n","  Đang xử lý câu 10/186...\n","  Đang xử lý câu 11/186...\n","  Đang xử lý câu 12/186...\n","  Đang xử lý câu 13/186...\n","  Đang xử lý câu 14/186...\n","  Đang xử lý câu 15/186...\n","  Đang xử lý câu 16/186...\n","  Đang xử lý câu 17/186...\n","  Đang xử lý câu 18/186...\n","  Đang xử lý câu 19/186...\n","  Đang xử lý câu 20/186...\n","  Đang xử lý câu 21/186...\n","  Đang xử lý câu 22/186...\n","  Đang xử lý câu 23/186...\n","  Đang xử lý câu 24/186...\n","  Đang xử lý câu 25/186...\n","  Đang xử lý câu 26/186...\n","  Đang xử lý câu 27/186...\n","  Đang xử lý câu 28/186...\n","  Đang xử lý câu 29/186...\n","  Đang xử lý câu 30/186...\n","  Đang xử lý câu 31/186...\n","  Đang xử lý câu 32/186...\n","  Đang xử lý câu 33/186...\n","  Đang xử lý câu 34/186...\n","  Đang xử lý câu 35/186...\n","  Đang xử lý câu 36/186...\n","  Đang xử lý câu 37/186...\n","  Đang xử lý câu 38/186...\n","  Đang xử lý câu 39/186...\n","  Đang xử lý câu 40/186...\n","  Đang xử lý câu 41/186...\n","  Đang xử lý câu 42/186...\n","  Đang xử lý câu 43/186...\n","  Đang xử lý câu 44/186...\n","  Đang xử lý câu 45/186...\n","  Đang xử lý câu 46/186...\n","  Đang xử lý câu 47/186...\n","  Đang xử lý câu 48/186...\n","  Đang xử lý câu 49/186...\n","  Đang xử lý câu 50/186...\n","  Đang xử lý câu 51/186...\n","  Đang xử lý câu 52/186...\n","  Đang xử lý câu 53/186...\n","  Đang xử lý câu 54/186...\n","  Đang xử lý câu 55/186...\n","  Đang xử lý câu 56/186...\n","  Đang xử lý câu 57/186...\n","  Đang xử lý câu 58/186...\n","  Đang xử lý câu 59/186...\n","  Đang xử lý câu 60/186...\n","  Đang xử lý câu 61/186...\n","  Đang xử lý câu 62/186...\n","  Đang xử lý câu 63/186...\n","  Đang xử lý câu 64/186...\n","  Đang xử lý câu 65/186...\n","  Đang xử lý câu 66/186...\n","  Đang xử lý câu 67/186...\n","  Đang xử lý câu 68/186...\n","  Đang xử lý câu 69/186...\n","  Đang xử lý câu 70/186...\n","  Đang xử lý câu 71/186...\n","  Đang xử lý câu 72/186...\n","  Đang xử lý câu 73/186...\n","  Đang xử lý câu 74/186...\n","  Đang xử lý câu 75/186...\n","  Đang xử lý câu 76/186...\n","  Đang xử lý câu 77/186...\n","  Đang xử lý câu 78/186...\n","  Đang xử lý câu 79/186...\n","  Đang xử lý câu 80/186...\n","  Đang xử lý câu 81/186...\n","  Đang xử lý câu 82/186...\n","  Đang xử lý câu 83/186...\n","  Đang xử lý câu 84/186...\n","  Đang xử lý câu 85/186...\n","  Đang xử lý câu 86/186...\n","  Đang xử lý câu 87/186...\n","  Đang xử lý câu 88/186...\n","  Đang xử lý câu 89/186...\n","  Đang xử lý câu 90/186...\n","  Đang xử lý câu 91/186...\n","  Đang xử lý câu 92/186...\n","  Đang xử lý câu 93/186...\n","  Đang xử lý câu 94/186...\n","  Đang xử lý câu 95/186...\n","  Đang xử lý câu 96/186...\n","  Đang xử lý câu 97/186...\n","  Đang xử lý câu 98/186...\n","  Đang xử lý câu 99/186...\n","  Đang xử lý câu 100/186...\n","  Đang xử lý câu 101/186...\n","  Đang xử lý câu 102/186...\n","  Đang xử lý câu 103/186...\n","  Đang xử lý câu 104/186...\n","  Đang xử lý câu 105/186...\n","  Đang xử lý câu 106/186...\n","  Đang xử lý câu 107/186...\n","  Đang xử lý câu 108/186...\n","  Đang xử lý câu 109/186...\n","  Đang xử lý câu 110/186...\n","  Đang xử lý câu 111/186...\n","  Đang xử lý câu 112/186...\n","  Đang xử lý câu 113/186...\n","  Đang xử lý câu 114/186...\n","  Đang xử lý câu 115/186...\n","  Đang xử lý câu 116/186...\n","  Đang xử lý câu 117/186...\n","  Đang xử lý câu 118/186...\n","  Đang xử lý câu 119/186...\n","  Đang xử lý câu 120/186...\n","  Đang xử lý câu 121/186...\n","  Đang xử lý câu 122/186...\n","  Đang xử lý câu 123/186...\n","  Đang xử lý câu 124/186...\n","  Đang xử lý câu 125/186...\n","  Đang xử lý câu 126/186...\n","  Đang xử lý câu 127/186...\n","  Đang xử lý câu 128/186...\n","  Đang xử lý câu 129/186...\n","  Đang xử lý câu 130/186...\n","  Đang xử lý câu 131/186...\n","  Đang xử lý câu 132/186...\n","  Đang xử lý câu 133/186...\n","  Đang xử lý câu 134/186...\n","  Đang xử lý câu 135/186...\n","  Đang xử lý câu 136/186...\n","  Đang xử lý câu 137/186...\n","  Đang xử lý câu 138/186...\n","  Đang xử lý câu 139/186...\n","  Đang xử lý câu 140/186...\n","  Đang xử lý câu 141/186...\n","  Đang xử lý câu 142/186...\n","  Đang xử lý câu 143/186...\n","  Đang xử lý câu 144/186...\n","  Đang xử lý câu 145/186...\n","  Đang xử lý câu 146/186...\n","  Đang xử lý câu 147/186...\n","  Đang xử lý câu 148/186...\n","  Đang xử lý câu 149/186...\n","  Đang xử lý câu 150/186...\n","  Đang xử lý câu 151/186...\n","  Đang xử lý câu 152/186...\n","  Đang xử lý câu 153/186...\n","  Đang xử lý câu 154/186...\n","  Đang xử lý câu 155/186...\n","  Đang xử lý câu 156/186...\n","  Đang xử lý câu 157/186...\n","  Đang xử lý câu 158/186...\n","  Đang xử lý câu 159/186...\n","  Đang xử lý câu 160/186...\n","  Đang xử lý câu 161/186...\n","  Đang xử lý câu 162/186...\n","  Đang xử lý câu 163/186...\n","  Đang xử lý câu 164/186...\n","  Đang xử lý câu 165/186...\n","  Đang xử lý câu 166/186...\n","  Đang xử lý câu 167/186...\n","  Đang xử lý câu 168/186...\n","  Đang xử lý câu 169/186...\n","  Đang xử lý câu 170/186...\n","  Đang xử lý câu 171/186...\n","  Đang xử lý câu 172/186...\n","  Đang xử lý câu 173/186...\n","  Đang xử lý câu 174/186...\n","  Đang xử lý câu 175/186...\n","  Đang xử lý câu 176/186...\n","  Đang xử lý câu 177/186...\n","  Đang xử lý câu 178/186...\n","  Đang xử lý câu 179/186...\n","  Đang xử lý câu 180/186...\n","  Đang xử lý câu 181/186...\n","  Đang xử lý câu 182/186...\n","  Đang xử lý câu 183/186...\n","  Đang xử lý câu 184/186...\n","  Đang xử lý câu 185/186...\n","  Đang xử lý câu 186/186...\n","Đã xử lý xong 186 câu cho động từ 'begin_2'.\n","\n","--- Xử lý cho động từ: block ---\n","  Đang xử lý câu 1/115...\n","  Đang xử lý câu 2/115...\n","  Đang xử lý câu 3/115...\n","  Đang xử lý câu 4/115...\n","  Đang xử lý câu 5/115...\n","  Đang xử lý câu 6/115...\n","  Đang xử lý câu 7/115...\n","  Đang xử lý câu 8/115...\n","  Đang xử lý câu 9/115...\n","  Đang xử lý câu 10/115...\n","  Đang xử lý câu 11/115...\n","  Đang xử lý câu 12/115...\n","  Đang xử lý câu 13/115...\n","  Đang xử lý câu 14/115...\n","  Đang xử lý câu 15/115...\n","  Đang xử lý câu 16/115...\n","  Đang xử lý câu 17/115...\n","  Đang xử lý câu 18/115...\n","  Đang xử lý câu 19/115...\n","  Đang xử lý câu 20/115...\n","  Đang xử lý câu 21/115...\n","  Đang xử lý câu 22/115...\n","  Đang xử lý câu 23/115...\n","  Đang xử lý câu 24/115...\n","  Đang xử lý câu 25/115...\n","  Đang xử lý câu 26/115...\n","  Đang xử lý câu 27/115...\n","  Đang xử lý câu 28/115...\n","  Đang xử lý câu 29/115...\n","  Đang xử lý câu 30/115...\n","  Đang xử lý câu 31/115...\n","  Đang xử lý câu 32/115...\n","  Đang xử lý câu 33/115...\n","  Đang xử lý câu 34/115...\n","  Đang xử lý câu 35/115...\n","  Đang xử lý câu 36/115...\n","  Đang xử lý câu 37/115...\n","  Đang xử lý câu 38/115...\n","  Đang xử lý câu 39/115...\n","  Đang xử lý câu 40/115...\n","  Đang xử lý câu 41/115...\n","  Đang xử lý câu 42/115...\n","  Đang xử lý câu 43/115...\n","  Đang xử lý câu 44/115...\n","  Đang xử lý câu 45/115...\n","  Đang xử lý câu 46/115...\n","  Đang xử lý câu 47/115...\n","  Đang xử lý câu 48/115...\n","  Đang xử lý câu 49/115...\n","  Đang xử lý câu 50/115...\n","  Đang xử lý câu 51/115...\n","  Đang xử lý câu 52/115...\n","  Đang xử lý câu 53/115...\n","  Đang xử lý câu 54/115...\n","  Đang xử lý câu 55/115...\n","  Đang xử lý câu 56/115...\n","  Đang xử lý câu 57/115...\n","  Đang xử lý câu 58/115...\n","  Đang xử lý câu 59/115...\n","  Đang xử lý câu 60/115...\n","  Đang xử lý câu 61/115...\n","  Đang xử lý câu 62/115...\n","  Đang xử lý câu 63/115...\n","  Đang xử lý câu 64/115...\n","  Đang xử lý câu 65/115...\n","  Đang xử lý câu 66/115...\n","  Đang xử lý câu 67/115...\n","  Đang xử lý câu 68/115...\n","  Đang xử lý câu 69/115...\n","  Đang xử lý câu 70/115...\n","  Đang xử lý câu 71/115...\n","  Đang xử lý câu 72/115...\n","  Đang xử lý câu 73/115...\n","  Đang xử lý câu 74/115...\n","  Đang xử lý câu 75/115...\n","  Đang xử lý câu 76/115...\n","  Đang xử lý câu 77/115...\n","  Đang xử lý câu 78/115...\n","  Đang xử lý câu 79/115...\n","  Đang xử lý câu 80/115...\n","  Đang xử lý câu 81/115...\n","  Đang xử lý câu 82/115...\n","  Đang xử lý câu 83/115...\n","  Đang xử lý câu 84/115...\n","  Đang xử lý câu 85/115...\n","  Đang xử lý câu 86/115...\n","  Đang xử lý câu 87/115...\n","  Đang xử lý câu 88/115...\n","  Đang xử lý câu 89/115...\n","  Đang xử lý câu 90/115...\n","  Đang xử lý câu 91/115...\n","  Đang xử lý câu 92/115...\n","  Đang xử lý câu 93/115...\n","  Đang xử lý câu 94/115...\n","  Đang xử lý câu 95/115...\n","  Đang xử lý câu 96/115...\n","  Đang xử lý câu 97/115...\n","  Đang xử lý câu 98/115...\n","  Đang xử lý câu 99/115...\n","  Đang xử lý câu 100/115...\n","  Đang xử lý câu 101/115...\n","  Đang xử lý câu 102/115...\n","  Đang xử lý câu 103/115...\n","  Đang xử lý câu 104/115...\n","  Đang xử lý câu 105/115...\n","  Đang xử lý câu 106/115...\n","  Đang xử lý câu 107/115...\n","  Đang xử lý câu 108/115...\n","  Đang xử lý câu 109/115...\n","  Đang xử lý câu 110/115...\n","  Đang xử lý câu 111/115...\n","  Đang xử lý câu 112/115...\n","  Đang xử lý câu 113/115...\n","  Đang xử lý câu 114/115...\n","  Đang xử lý câu 115/115...\n","Đã xử lý xong 115 câu cho động từ 'block'.\n","\n","--- Xử lý cho động từ: abolish ---\n","  Đang xử lý câu 1/64...\n","  Đang xử lý câu 2/64...\n","  Đang xử lý câu 3/64...\n","  Đang xử lý câu 4/64...\n","  Đang xử lý câu 5/64...\n","  Đang xử lý câu 6/64...\n","  Đang xử lý câu 7/64...\n","  Đang xử lý câu 8/64...\n","  Đang xử lý câu 9/64...\n","  Đang xử lý câu 10/64...\n","  Đang xử lý câu 11/64...\n","  Đang xử lý câu 12/64...\n","  Đang xử lý câu 13/64...\n","  Đang xử lý câu 14/64...\n","  Đang xử lý câu 15/64...\n","  Đang xử lý câu 16/64...\n","  Đang xử lý câu 17/64...\n","  Đang xử lý câu 18/64...\n","  Đang xử lý câu 19/64...\n","  Đang xử lý câu 20/64...\n","  Đang xử lý câu 21/64...\n","  Đang xử lý câu 22/64...\n","  Đang xử lý câu 23/64...\n","  Đang xử lý câu 24/64...\n","  Đang xử lý câu 25/64...\n","  Đang xử lý câu 26/64...\n","  Đang xử lý câu 27/64...\n","  Đang xử lý câu 28/64...\n","  Đang xử lý câu 29/64...\n","  Đang xử lý câu 30/64...\n","  Đang xử lý câu 31/64...\n","  Đang xử lý câu 32/64...\n","  Đang xử lý câu 33/64...\n","  Đang xử lý câu 34/64...\n","  Đang xử lý câu 35/64...\n","  Đang xử lý câu 36/64...\n","  Đang xử lý câu 37/64...\n","  Đang xử lý câu 38/64...\n","  Đang xử lý câu 39/64...\n","  Đang xử lý câu 40/64...\n","  Đang xử lý câu 41/64...\n","  Đang xử lý câu 42/64...\n","  Đang xử lý câu 43/64...\n","  Đang xử lý câu 44/64...\n","  Đang xử lý câu 45/64...\n","  Đang xử lý câu 46/64...\n","  Đang xử lý câu 47/64...\n","  Đang xử lý câu 48/64...\n","  Đang xử lý câu 49/64...\n","  Đang xử lý câu 50/64...\n","  Đang xử lý câu 51/64...\n","  Đang xử lý câu 52/64...\n","  Đang xử lý câu 53/64...\n","  Đang xử lý câu 54/64...\n","  Đang xử lý câu 55/64...\n","  Đang xử lý câu 56/64...\n","  Đang xử lý câu 57/64...\n","  Đang xử lý câu 58/64...\n","  Đang xử lý câu 59/64...\n","  Đang xử lý câu 60/64...\n","  Đang xử lý câu 61/64...\n","  Đang xử lý câu 62/64...\n","  Đang xử lý câu 63/64...\n","  Đang xử lý câu 64/64...\n","Đã xử lý xong 64 câu cho động từ 'abolish'.\n","\n","--- Xử lý cho động từ: translate_2 ---\n","  Đang xử lý câu 1/138...\n","  Đang xử lý câu 2/138...\n","  Đang xử lý câu 3/138...\n","  Đang xử lý câu 4/138...\n","  Đang xử lý câu 5/138...\n","  Đang xử lý câu 6/138...\n","  Đang xử lý câu 7/138...\n","  Đang xử lý câu 8/138...\n","  Đang xử lý câu 9/138...\n","  Đang xử lý câu 10/138...\n","  Đang xử lý câu 11/138...\n","  Đang xử lý câu 12/138...\n","  Đang xử lý câu 13/138...\n","  Đang xử lý câu 14/138...\n","  Đang xử lý câu 15/138...\n","  Đang xử lý câu 16/138...\n","  Đang xử lý câu 17/138...\n","  Đang xử lý câu 18/138...\n","  Đang xử lý câu 19/138...\n","  Đang xử lý câu 20/138...\n","  Đang xử lý câu 21/138...\n","  Đang xử lý câu 22/138...\n","  Đang xử lý câu 23/138...\n","  Đang xử lý câu 24/138...\n","  Đang xử lý câu 25/138...\n","  Đang xử lý câu 26/138...\n","  Đang xử lý câu 27/138...\n","  Đang xử lý câu 28/138...\n","  Đang xử lý câu 29/138...\n","  Đang xử lý câu 30/138...\n","  Đang xử lý câu 31/138...\n","  Đang xử lý câu 32/138...\n","  Đang xử lý câu 33/138...\n","  Đang xử lý câu 34/138...\n","  Đang xử lý câu 35/138...\n","  Đang xử lý câu 36/138...\n","  Đang xử lý câu 37/138...\n","  Đang xử lý câu 38/138...\n","  Đang xử lý câu 39/138...\n","  Đang xử lý câu 40/138...\n","  Đang xử lý câu 41/138...\n","  Đang xử lý câu 42/138...\n","  Đang xử lý câu 43/138...\n","  Đang xử lý câu 44/138...\n","  Đang xử lý câu 45/138...\n","  Đang xử lý câu 46/138...\n","  Đang xử lý câu 47/138...\n","  Đang xử lý câu 48/138...\n","  Đang xử lý câu 49/138...\n","  Đang xử lý câu 50/138...\n","  Đang xử lý câu 51/138...\n","  Đang xử lý câu 52/138...\n","  Đang xử lý câu 53/138...\n","  Đang xử lý câu 54/138...\n","  Đang xử lý câu 55/138...\n","  Đang xử lý câu 56/138...\n","  Đang xử lý câu 57/138...\n","  Đang xử lý câu 58/138...\n","  Đang xử lý câu 59/138...\n","  Đang xử lý câu 60/138...\n","  Đang xử lý câu 61/138...\n","  Đang xử lý câu 62/138...\n","  Đang xử lý câu 63/138...\n","  Đang xử lý câu 64/138...\n","  Đang xử lý câu 65/138...\n","  Đang xử lý câu 66/138...\n","  Đang xử lý câu 67/138...\n","  Đang xử lý câu 68/138...\n","  Đang xử lý câu 69/138...\n","  Đang xử lý câu 70/138...\n","  Đang xử lý câu 71/138...\n","  Đang xử lý câu 72/138...\n","  Đang xử lý câu 73/138...\n","  Đang xử lý câu 74/138...\n","  Đang xử lý câu 75/138...\n","  Đang xử lý câu 76/138...\n","  Đang xử lý câu 77/138...\n","  Đang xử lý câu 78/138...\n","  Đang xử lý câu 79/138...\n","  Đang xử lý câu 80/138...\n","  Đang xử lý câu 81/138...\n","  Đang xử lý câu 82/138...\n","  Đang xử lý câu 83/138...\n","  Đang xử lý câu 84/138...\n","  Đang xử lý câu 85/138...\n","  Đang xử lý câu 86/138...\n","  Đang xử lý câu 87/138...\n","  Đang xử lý câu 88/138...\n","  Đang xử lý câu 89/138...\n","  Đang xử lý câu 90/138...\n","  Đang xử lý câu 91/138...\n","  Đang xử lý câu 92/138...\n","  Đang xử lý câu 93/138...\n","  Đang xử lý câu 94/138...\n","  Đang xử lý câu 95/138...\n","  Đang xử lý câu 96/138...\n","  Đang xử lý câu 97/138...\n","  Đang xử lý câu 98/138...\n","  Đang xử lý câu 99/138...\n","  Đang xử lý câu 100/138...\n","  Đang xử lý câu 101/138...\n","  Đang xử lý câu 102/138...\n","  Đang xử lý câu 103/138...\n","  Đang xử lý câu 104/138...\n","  Đang xử lý câu 105/138...\n","  Đang xử lý câu 106/138...\n","  Đang xử lý câu 107/138...\n","  Đang xử lý câu 108/138...\n","  Đang xử lý câu 109/138...\n","  Đang xử lý câu 110/138...\n","  Đang xử lý câu 111/138...\n","  Đang xử lý câu 112/138...\n","  Đang xử lý câu 113/138...\n","  Đang xử lý câu 114/138...\n","  Đang xử lý câu 115/138...\n","  Đang xử lý câu 116/138...\n","  Đang xử lý câu 117/138...\n","  Đang xử lý câu 118/138...\n","  Đang xử lý câu 119/138...\n","  Đang xử lý câu 120/138...\n","  Đang xử lý câu 121/138...\n","  Đang xử lý câu 122/138...\n","  Đang xử lý câu 123/138...\n","  Đang xử lý câu 124/138...\n","  Đang xử lý câu 125/138...\n","  Đang xử lý câu 126/138...\n","  Đang xử lý câu 127/138...\n","  Đang xử lý câu 128/138...\n","  Đang xử lý câu 129/138...\n","  Đang xử lý câu 130/138...\n","  Đang xử lý câu 131/138...\n","  Đang xử lý câu 132/138...\n","  Đang xử lý câu 133/138...\n","  Đang xử lý câu 134/138...\n","  Đang xử lý câu 135/138...\n","  Đang xử lý câu 136/138...\n","  Đang xử lý câu 137/138...\n","  Đang xử lý câu 138/138...\n","Đã xử lý xong 138 câu cho động từ 'translate_2'.\n","\n","--- Xử lý cho động từ: inhibit ---\n","  Đang xử lý câu 1/63...\n","  Đang xử lý câu 2/63...\n","  Đang xử lý câu 3/63...\n","  Đang xử lý câu 4/63...\n","  Đang xử lý câu 5/63...\n","  Đang xử lý câu 6/63...\n","  Đang xử lý câu 7/63...\n","  Đang xử lý câu 8/63...\n","  Đang xử lý câu 9/63...\n","  Đang xử lý câu 10/63...\n","  Đang xử lý câu 11/63...\n","  Đang xử lý câu 12/63...\n","  Đang xử lý câu 13/63...\n","  Đang xử lý câu 14/63...\n","  Đang xử lý câu 15/63...\n","  Đang xử lý câu 16/63...\n","  Đang xử lý câu 17/63...\n","  Đang xử lý câu 18/63...\n","  Đang xử lý câu 19/63...\n","  Đang xử lý câu 20/63...\n","  Đang xử lý câu 21/63...\n","  Đang xử lý câu 22/63...\n","  Đang xử lý câu 23/63...\n","  Đang xử lý câu 24/63...\n","  Đang xử lý câu 25/63...\n","  Đang xử lý câu 26/63...\n","  Đang xử lý câu 27/63...\n","  Đang xử lý câu 28/63...\n","  Đang xử lý câu 29/63...\n","  Đang xử lý câu 30/63...\n","  Đang xử lý câu 31/63...\n","  Đang xử lý câu 32/63...\n","  Đang xử lý câu 33/63...\n","  Đang xử lý câu 34/63...\n","  Đang xử lý câu 35/63...\n","  Đang xử lý câu 36/63...\n","  Đang xử lý câu 37/63...\n","  Đang xử lý câu 38/63...\n","  Đang xử lý câu 39/63...\n","  Đang xử lý câu 40/63...\n","  Đang xử lý câu 41/63...\n","  Đang xử lý câu 42/63...\n","  Đang xử lý câu 43/63...\n","  Đang xử lý câu 44/63...\n","  Đang xử lý câu 45/63...\n","  Đang xử lý câu 46/63...\n","  Đang xử lý câu 47/63...\n","  Đang xử lý câu 48/63...\n","  Đang xử lý câu 49/63...\n","  Đang xử lý câu 50/63...\n","  Đang xử lý câu 51/63...\n","  Đang xử lý câu 52/63...\n","  Đang xử lý câu 53/63...\n","  Đang xử lý câu 54/63...\n","  Đang xử lý câu 55/63...\n","  Đang xử lý câu 56/63...\n","  Đang xử lý câu 57/63...\n","  Đang xử lý câu 58/63...\n","  Đang xử lý câu 59/63...\n","  Đang xử lý câu 60/63...\n","  Đang xử lý câu 61/63...\n","  Đang xử lý câu 62/63...\n","  Đang xử lý câu 63/63...\n","Đã xử lý xong 63 câu cho động từ 'inhibit'.\n","\n","--- Xử lý cho động từ: begin_1 ---\n","  Đang xử lý câu 1/25...\n","  Đang xử lý câu 2/25...\n","  Đang xử lý câu 3/25...\n","  Đang xử lý câu 4/25...\n","  Đang xử lý câu 5/25...\n","  Đang xử lý câu 6/25...\n","  Đang xử lý câu 7/25...\n","  Đang xử lý câu 8/25...\n","  Đang xử lý câu 9/25...\n","  Đang xử lý câu 10/25...\n","  Đang xử lý câu 11/25...\n","  Đang xử lý câu 12/25...\n","  Đang xử lý câu 13/25...\n","  Đang xử lý câu 14/25...\n","  Đang xử lý câu 15/25...\n","  Đang xử lý câu 16/25...\n","  Đang xử lý câu 17/25...\n","  Đang xử lý câu 18/25...\n","  Đang xử lý câu 19/25...\n","  Đang xử lý câu 20/25...\n","  Đang xử lý câu 21/25...\n","  Đang xử lý câu 22/25...\n","  Đang xử lý câu 23/25...\n","  Đang xử lý câu 24/25...\n","  Đang xử lý câu 25/25...\n","Đã xử lý xong 25 câu cho động từ 'begin_1'.\n","\n","--- Xử lý cho động từ: splice_2 ---\n","  Đang xử lý câu 1/39...\n","  Đang xử lý câu 2/39...\n","  Đang xử lý câu 3/39...\n","  Đang xử lý câu 4/39...\n","  Đang xử lý câu 5/39...\n","  Đang xử lý câu 6/39...\n","  Đang xử lý câu 7/39...\n","  Đang xử lý câu 8/39...\n","  Đang xử lý câu 9/39...\n","  Đang xử lý câu 10/39...\n","  Đang xử lý câu 11/39...\n","  Đang xử lý câu 12/39...\n","  Đang xử lý câu 13/39...\n","  Đang xử lý câu 14/39...\n","  Đang xử lý câu 15/39...\n","  Đang xử lý câu 16/39...\n","  Đang xử lý câu 17/39...\n","  Đang xử lý câu 18/39...\n","  Đang xử lý câu 19/39...\n","  Đang xử lý câu 20/39...\n","  Đang xử lý câu 21/39...\n","  Đang xử lý câu 22/39...\n","  Đang xử lý câu 23/39...\n","  Đang xử lý câu 24/39...\n","  Đang xử lý câu 25/39...\n","  Đang xử lý câu 26/39...\n","  Đang xử lý câu 27/39...\n","  Đang xử lý câu 28/39...\n","  Đang xử lý câu 29/39...\n","  Đang xử lý câu 30/39...\n","  Đang xử lý câu 31/39...\n","  Đang xử lý câu 32/39...\n","  Đang xử lý câu 33/39...\n","  Đang xử lý câu 34/39...\n","  Đang xử lý câu 35/39...\n","  Đang xử lý câu 36/39...\n","  Đang xử lý câu 37/39...\n","  Đang xử lý câu 38/39...\n","  Đang xử lý câu 39/39...\n","Đã xử lý xong 39 câu cho động từ 'splice_2'.\n","\n","--- Xử lý cho động từ: modify ---\n","  Đang xử lý câu 1/7...\n","  Đang xử lý câu 2/7...\n","  Đang xử lý câu 3/7...\n","  Đang xử lý câu 4/7...\n","  Đang xử lý câu 5/7...\n","  Đang xử lý câu 6/7...\n","  Đang xử lý câu 7/7...\n","Đã xử lý xong 7 câu cho động từ 'modify'.\n","\n","--- Xử lý cho động từ: lose ---\n","  Đang xử lý câu 1/36...\n","  Đang xử lý câu 2/36...\n","  Đang xử lý câu 3/36...\n","  Đang xử lý câu 4/36...\n","  Đang xử lý câu 5/36...\n","  Đang xử lý câu 6/36...\n","  Đang xử lý câu 7/36...\n","  Đang xử lý câu 8/36...\n","  Đang xử lý câu 9/36...\n","  Đang xử lý câu 10/36...\n","  Đang xử lý câu 11/36...\n","  Đang xử lý câu 12/36...\n","  Đang xử lý câu 13/36...\n","  Đang xử lý câu 14/36...\n","  Đang xử lý câu 15/36...\n","  Đang xử lý câu 16/36...\n","  Đang xử lý câu 17/36...\n","  Đang xử lý câu 18/36...\n","  Đang xử lý câu 19/36...\n","  Đang xử lý câu 20/36...\n","  Đang xử lý câu 21/36...\n","  Đang xử lý câu 22/36...\n","  Đang xử lý câu 23/36...\n","  Đang xử lý câu 24/36...\n","  Đang xử lý câu 25/36...\n","  Đang xử lý câu 26/36...\n","  Đang xử lý câu 27/36...\n","  Đang xử lý câu 28/36...\n","  Đang xử lý câu 29/36...\n","  Đang xử lý câu 30/36...\n","  Đang xử lý câu 31/36...\n","  Đang xử lý câu 32/36...\n","  Đang xử lý câu 33/36...\n","  Đang xử lý câu 34/36...\n","  Đang xử lý câu 35/36...\n","  Đang xử lý câu 36/36...\n","Đã xử lý xong 36 câu cho động từ 'lose'.\n","\n","--- Xử lý cho động từ: lead ---\n","  Đang xử lý câu 1/80...\n","  Đang xử lý câu 2/80...\n","  Đang xử lý câu 3/80...\n","  Đang xử lý câu 4/80...\n","  Đang xử lý câu 5/80...\n","  Đang xử lý câu 6/80...\n","  Đang xử lý câu 7/80...\n","  Đang xử lý câu 8/80...\n","  Đang xử lý câu 9/80...\n","  Đang xử lý câu 10/80...\n","  Đang xử lý câu 11/80...\n","  Đang xử lý câu 12/80...\n","  Đang xử lý câu 13/80...\n","  Đang xử lý câu 14/80...\n","  Đang xử lý câu 15/80...\n","  Đang xử lý câu 16/80...\n","  Đang xử lý câu 17/80...\n","  Đang xử lý câu 18/80...\n","  Đang xử lý câu 19/80...\n","  Đang xử lý câu 20/80...\n","  Đang xử lý câu 21/80...\n","  Đang xử lý câu 22/80...\n","  Đang xử lý câu 23/80...\n","  Đang xử lý câu 24/80...\n","  Đang xử lý câu 25/80...\n","  Đang xử lý câu 26/80...\n","  Đang xử lý câu 27/80...\n","  Đang xử lý câu 28/80...\n","  Đang xử lý câu 29/80...\n","  Đang xử lý câu 30/80...\n","  Đang xử lý câu 31/80...\n","  Đang xử lý câu 32/80...\n","  Đang xử lý câu 33/80...\n","  Đang xử lý câu 34/80...\n","  Đang xử lý câu 35/80...\n","  Đang xử lý câu 36/80...\n","  Đang xử lý câu 37/80...\n","  Đang xử lý câu 38/80...\n","  Đang xử lý câu 39/80...\n","  Đang xử lý câu 40/80...\n","  Đang xử lý câu 41/80...\n","  Đang xử lý câu 42/80...\n","  Đang xử lý câu 43/80...\n","  Đang xử lý câu 44/80...\n","  Đang xử lý câu 45/80...\n","  Đang xử lý câu 46/80...\n","  Đang xử lý câu 47/80...\n","  Đang xử lý câu 48/80...\n","  Đang xử lý câu 49/80...\n","  Đang xử lý câu 50/80...\n","  Đang xử lý câu 51/80...\n","  Đang xử lý câu 52/80...\n","  Đang xử lý câu 53/80...\n","  Đang xử lý câu 54/80...\n","  Đang xử lý câu 55/80...\n","  Đang xử lý câu 56/80...\n","  Đang xử lý câu 57/80...\n","  Đang xử lý câu 58/80...\n","  Đang xử lý câu 59/80...\n","  Đang xử lý câu 60/80...\n","  Đang xử lý câu 61/80...\n","  Đang xử lý câu 62/80...\n","  Đang xử lý câu 63/80...\n","  Đang xử lý câu 64/80...\n","  Đang xử lý câu 65/80...\n","  Đang xử lý câu 66/80...\n","  Đang xử lý câu 67/80...\n","  Đang xử lý câu 68/80...\n","  Đang xử lý câu 69/80...\n","  Đang xử lý câu 70/80...\n","  Đang xử lý câu 71/80...\n","  Đang xử lý câu 72/80...\n","  Đang xử lý câu 73/80...\n","  Đang xử lý câu 74/80...\n","  Đang xử lý câu 75/80...\n","  Đang xử lý câu 76/80...\n","  Đang xử lý câu 77/80...\n","  Đang xử lý câu 78/80...\n","  Đang xử lý câu 79/80...\n","  Đang xử lý câu 80/80...\n","Đã xử lý xong 80 câu cho động từ 'lead'.\n","\n","--- Xử lý cho động từ: encode ---\n","  Đang xử lý câu 1/29...\n","  Đang xử lý câu 2/29...\n","  Đang xử lý câu 3/29...\n","  Đang xử lý câu 4/29...\n","  Đang xử lý câu 5/29...\n","  Đang xử lý câu 6/29...\n","  Đang xử lý câu 7/29...\n","  Đang xử lý câu 8/29...\n","  Đang xử lý câu 9/29...\n","  Đang xử lý câu 10/29...\n","  Đang xử lý câu 11/29...\n","  Đang xử lý câu 12/29...\n","  Đang xử lý câu 13/29...\n","  Đang xử lý câu 14/29...\n","  Đang xử lý câu 15/29...\n","  Đang xử lý câu 16/29...\n","  Đang xử lý câu 17/29...\n","  Đang xử lý câu 18/29...\n","  Đang xử lý câu 19/29...\n","  Đang xử lý câu 20/29...\n","  Đang xử lý câu 21/29...\n","  Đang xử lý câu 22/29...\n","  Đang xử lý câu 23/29...\n","  Đang xử lý câu 24/29...\n","  Đang xử lý câu 25/29...\n","  Đang xử lý câu 26/29...\n","  Đang xử lý câu 27/29...\n","  Đang xử lý câu 28/29...\n","  Đang xử lý câu 29/29...\n","Đã xử lý xong 29 câu cho động từ 'encode'.\n","\n","--- Xử lý cho động từ: proliferate ---\n","  Đang xử lý câu 1/33...\n","  Đang xử lý câu 2/33...\n","  Đang xử lý câu 3/33...\n","  Đang xử lý câu 4/33...\n","  Đang xử lý câu 5/33...\n","  Đang xử lý câu 6/33...\n","  Đang xử lý câu 7/33...\n","  Đang xử lý câu 8/33...\n","  Đang xử lý câu 9/33...\n","  Đang xử lý câu 10/33...\n","  Đang xử lý câu 11/33...\n","  Đang xử lý câu 12/33...\n","  Đang xử lý câu 13/33...\n","  Đang xử lý câu 14/33...\n","  Đang xử lý câu 15/33...\n","  Đang xử lý câu 16/33...\n","  Đang xử lý câu 17/33...\n","  Đang xử lý câu 18/33...\n","  Đang xử lý câu 19/33...\n","  Đang xử lý câu 20/33...\n","  Đang xử lý câu 21/33...\n","  Đang xử lý câu 22/33...\n","  Đang xử lý câu 23/33...\n","  Đang xử lý câu 24/33...\n","  Đang xử lý câu 25/33...\n","  Đang xử lý câu 26/33...\n","  Đang xử lý câu 27/33...\n","  Đang xử lý câu 28/33...\n","  Đang xử lý câu 29/33...\n","  Đang xử lý câu 30/33...\n","  Đang xử lý câu 31/33...\n","  Đang xử lý câu 32/33...\n","  Đang xử lý câu 33/33...\n","Đã xử lý xong 33 câu cho động từ 'proliferate'.\n","\n","--- Xử lý cho động từ: express ---\n","  Đang xử lý câu 1/86...\n","  Đang xử lý câu 2/86...\n","  Đang xử lý câu 3/86...\n","  Đang xử lý câu 4/86...\n","  Đang xử lý câu 5/86...\n","  Đang xử lý câu 6/86...\n","  Đang xử lý câu 7/86...\n","  Đang xử lý câu 8/86...\n","  Đang xử lý câu 9/86...\n","  Đang xử lý câu 10/86...\n","  Đang xử lý câu 11/86...\n","  Đang xử lý câu 12/86...\n","  Đang xử lý câu 13/86...\n","  Đang xử lý câu 14/86...\n","  Đang xử lý câu 15/86...\n","  Đang xử lý câu 16/86...\n","  Đang xử lý câu 17/86...\n","  Đang xử lý câu 18/86...\n","  Đang xử lý câu 19/86...\n","  Đang xử lý câu 20/86...\n","  Đang xử lý câu 21/86...\n","  Đang xử lý câu 22/86...\n","  Đang xử lý câu 23/86...\n","  Đang xử lý câu 24/86...\n","  Đang xử lý câu 25/86...\n","  Đang xử lý câu 26/86...\n","  Đang xử lý câu 27/86...\n","  Đang xử lý câu 28/86...\n","  Đang xử lý câu 29/86...\n","  Đang xử lý câu 30/86...\n","  Đang xử lý câu 31/86...\n","  Đang xử lý câu 32/86...\n","  Đang xử lý câu 33/86...\n","  Đang xử lý câu 34/86...\n","  Đang xử lý câu 35/86...\n","  Đang xử lý câu 36/86...\n","  Đang xử lý câu 37/86...\n","  Đang xử lý câu 38/86...\n","  Đang xử lý câu 39/86...\n","  Đang xử lý câu 40/86...\n","  Đang xử lý câu 41/86...\n","  Đang xử lý câu 42/86...\n","  Đang xử lý câu 43/86...\n","  Đang xử lý câu 44/86...\n","  Đang xử lý câu 45/86...\n","  Đang xử lý câu 46/86...\n","  Đang xử lý câu 47/86...\n","  Đang xử lý câu 48/86...\n","  Đang xử lý câu 49/86...\n","  Đang xử lý câu 50/86...\n","  Đang xử lý câu 51/86...\n","  Đang xử lý câu 52/86...\n","  Đang xử lý câu 53/86...\n","  Đang xử lý câu 54/86...\n","  Đang xử lý câu 55/86...\n","  Đang xử lý câu 56/86...\n","  Đang xử lý câu 57/86...\n","  Đang xử lý câu 58/86...\n","  Đang xử lý câu 59/86...\n","  Đang xử lý câu 60/86...\n","  Đang xử lý câu 61/86...\n","  Đang xử lý câu 62/86...\n","  Đang xử lý câu 63/86...\n","  Đang xử lý câu 64/86...\n","  Đang xử lý câu 65/86...\n","  Đang xử lý câu 66/86...\n","  Đang xử lý câu 67/86...\n","  Đang xử lý câu 68/86...\n","  Đang xử lý câu 69/86...\n","  Đang xử lý câu 70/86...\n","  Đang xử lý câu 71/86...\n","  Đang xử lý câu 72/86...\n","  Đang xử lý câu 73/86...\n","  Đang xử lý câu 74/86...\n","  Đang xử lý câu 75/86...\n","  Đang xử lý câu 76/86...\n","  Đang xử lý câu 77/86...\n","  Đang xử lý câu 78/86...\n","  Đang xử lý câu 79/86...\n","  Đang xử lý câu 80/86...\n","  Đang xử lý câu 81/86...\n","  Đang xử lý câu 82/86...\n","  Đang xử lý câu 83/86...\n","  Đang xử lý câu 84/86...\n","  Đang xử lý câu 85/86...\n","  Đang xử lý câu 86/86...\n","Đã xử lý xong 86 câu cho động từ 'express'.\n","\n","--- Xử lý cho động từ: alter ---\n","  Đang xử lý câu 1/72...\n","  Đang xử lý câu 2/72...\n","  Đang xử lý câu 3/72...\n","  Đang xử lý câu 4/72...\n","  Đang xử lý câu 5/72...\n","  Đang xử lý câu 6/72...\n","  Đang xử lý câu 7/72...\n","  Đang xử lý câu 8/72...\n","  Đang xử lý câu 9/72...\n","  Đang xử lý câu 10/72...\n","  Đang xử lý câu 11/72...\n","  Đang xử lý câu 12/72...\n","  Đang xử lý câu 13/72...\n","  Đang xử lý câu 14/72...\n","  Đang xử lý câu 15/72...\n","  Đang xử lý câu 16/72...\n","  Đang xử lý câu 17/72...\n","  Đang xử lý câu 18/72...\n","  Đang xử lý câu 19/72...\n","  Đang xử lý câu 20/72...\n","  Đang xử lý câu 21/72...\n","  Đang xử lý câu 22/72...\n","  Đang xử lý câu 23/72...\n","  Đang xử lý câu 24/72...\n","  Đang xử lý câu 25/72...\n","  Đang xử lý câu 26/72...\n","  Đang xử lý câu 27/72...\n","  Đang xử lý câu 28/72...\n","  Đang xử lý câu 29/72...\n","  Đang xử lý câu 30/72...\n","  Đang xử lý câu 31/72...\n","  Đang xử lý câu 32/72...\n","  Đang xử lý câu 33/72...\n","  Đang xử lý câu 34/72...\n","  Đang xử lý câu 35/72...\n","  Đang xử lý câu 36/72...\n","  Đang xử lý câu 37/72...\n","  Đang xử lý câu 38/72...\n","  Đang xử lý câu 39/72...\n","  Đang xử lý câu 40/72...\n","  Đang xử lý câu 41/72...\n","  Đang xử lý câu 42/72...\n","  Đang xử lý câu 43/72...\n","  Đang xử lý câu 44/72...\n","  Đang xử lý câu 45/72...\n","  Đang xử lý câu 46/72...\n","  Đang xử lý câu 47/72...\n","  Đang xử lý câu 48/72...\n","  Đang xử lý câu 49/72...\n","  Đang xử lý câu 50/72...\n","  Đang xử lý câu 51/72...\n","  Đang xử lý câu 52/72...\n","  Đang xử lý câu 53/72...\n","  Đang xử lý câu 54/72...\n","  Đang xử lý câu 55/72...\n","  Đang xử lý câu 56/72...\n","  Đang xử lý câu 57/72...\n","  Đang xử lý câu 58/72...\n","  Đang xử lý câu 59/72...\n","  Đang xử lý câu 60/72...\n","  Đang xử lý câu 61/72...\n","  Đang xử lý câu 62/72...\n","  Đang xử lý câu 63/72...\n","  Đang xử lý câu 64/72...\n","  Đang xử lý câu 65/72...\n","  Đang xử lý câu 66/72...\n","  Đang xử lý câu 67/72...\n","  Đang xử lý câu 68/72...\n","  Đang xử lý câu 69/72...\n","  Đang xử lý câu 70/72...\n","  Đang xử lý câu 71/72...\n","  Đang xử lý câu 72/72...\n","Đã xử lý xong 72 câu cho động từ 'alter'.\n","\n","--- Xử lý cho động từ: decrease_1 ---\n","  Đang xử lý câu 1/136...\n","  Đang xử lý câu 2/136...\n","  Đang xử lý câu 3/136...\n","  Đang xử lý câu 4/136...\n","  Đang xử lý câu 5/136...\n","  Đang xử lý câu 6/136...\n","  Đang xử lý câu 7/136...\n","  Đang xử lý câu 8/136...\n","  Đang xử lý câu 9/136...\n","  Đang xử lý câu 10/136...\n","  Đang xử lý câu 11/136...\n","  Đang xử lý câu 12/136...\n","  Đang xử lý câu 13/136...\n","  Đang xử lý câu 14/136...\n","  Đang xử lý câu 15/136...\n","  Đang xử lý câu 16/136...\n","  Đang xử lý câu 17/136...\n","  Đang xử lý câu 18/136...\n","  Đang xử lý câu 19/136...\n","  Đang xử lý câu 20/136...\n","  Đang xử lý câu 21/136...\n","  Đang xử lý câu 22/136...\n","  Đang xử lý câu 23/136...\n","  Đang xử lý câu 24/136...\n","  Đang xử lý câu 25/136...\n","  Đang xử lý câu 26/136...\n","  Đang xử lý câu 27/136...\n","  Đang xử lý câu 28/136...\n","  Đang xử lý câu 29/136...\n","  Đang xử lý câu 30/136...\n","  Đang xử lý câu 31/136...\n","  Đang xử lý câu 32/136...\n","  Đang xử lý câu 33/136...\n","  Đang xử lý câu 34/136...\n","  Đang xử lý câu 35/136...\n","  Đang xử lý câu 36/136...\n","  Đang xử lý câu 37/136...\n","  Đang xử lý câu 38/136...\n","  Đang xử lý câu 39/136...\n","  Đang xử lý câu 40/136...\n","  Đang xử lý câu 41/136...\n","  Đang xử lý câu 42/136...\n","  Đang xử lý câu 43/136...\n","  Đang xử lý câu 44/136...\n","  Đang xử lý câu 45/136...\n","  Đang xử lý câu 46/136...\n","  Đang xử lý câu 47/136...\n","  Đang xử lý câu 48/136...\n","  Đang xử lý câu 49/136...\n","  Đang xử lý câu 50/136...\n","  Đang xử lý câu 51/136...\n","  Đang xử lý câu 52/136...\n","  Đang xử lý câu 53/136...\n","  Đang xử lý câu 54/136...\n","  Đang xử lý câu 55/136...\n","  Đang xử lý câu 56/136...\n","  Đang xử lý câu 57/136...\n","  Đang xử lý câu 58/136...\n","  Đang xử lý câu 59/136...\n","  Đang xử lý câu 60/136...\n","  Đang xử lý câu 61/136...\n","  Đang xử lý câu 62/136...\n","  Đang xử lý câu 63/136...\n","  Đang xử lý câu 64/136...\n","  Đang xử lý câu 65/136...\n","  Đang xử lý câu 66/136...\n","  Đang xử lý câu 67/136...\n","  Đang xử lý câu 68/136...\n","  Đang xử lý câu 69/136...\n","  Đang xử lý câu 70/136...\n","  Đang xử lý câu 71/136...\n","  Đang xử lý câu 72/136...\n","  Đang xử lý câu 73/136...\n","  Đang xử lý câu 74/136...\n","  Đang xử lý câu 75/136...\n","  Đang xử lý câu 76/136...\n","  Đang xử lý câu 77/136...\n","  Đang xử lý câu 78/136...\n","  Đang xử lý câu 79/136...\n","  Đang xử lý câu 80/136...\n","  Đang xử lý câu 81/136...\n","  Đang xử lý câu 82/136...\n","  Đang xử lý câu 83/136...\n","  Đang xử lý câu 84/136...\n","  Đang xử lý câu 85/136...\n","  Đang xử lý câu 86/136...\n","  Đang xử lý câu 87/136...\n","  Đang xử lý câu 88/136...\n","  Đang xử lý câu 89/136...\n","  Đang xử lý câu 90/136...\n","  Đang xử lý câu 91/136...\n","  Đang xử lý câu 92/136...\n","  Đang xử lý câu 93/136...\n","  Đang xử lý câu 94/136...\n","  Đang xử lý câu 95/136...\n","  Đang xử lý câu 96/136...\n","  Đang xử lý câu 97/136...\n","  Đang xử lý câu 98/136...\n","  Đang xử lý câu 99/136...\n","  Đang xử lý câu 100/136...\n","  Đang xử lý câu 101/136...\n","  Đang xử lý câu 102/136...\n","  Đang xử lý câu 103/136...\n","  Đang xử lý câu 104/136...\n","  Đang xử lý câu 105/136...\n","  Đang xử lý câu 106/136...\n","  Đang xử lý câu 107/136...\n","  Đang xử lý câu 108/136...\n","  Đang xử lý câu 109/136...\n","  Đang xử lý câu 110/136...\n","  Đang xử lý câu 111/136...\n","  Đang xử lý câu 112/136...\n","  Đang xử lý câu 113/136...\n","  Đang xử lý câu 114/136...\n","  Đang xử lý câu 115/136...\n","  Đang xử lý câu 116/136...\n","  Đang xử lý câu 117/136...\n","  Đang xử lý câu 118/136...\n","  Đang xử lý câu 119/136...\n","  Đang xử lý câu 120/136...\n","  Đang xử lý câu 121/136...\n","  Đang xử lý câu 122/136...\n","  Đang xử lý câu 123/136...\n","  Đang xử lý câu 124/136...\n","  Đang xử lý câu 125/136...\n","  Đang xử lý câu 126/136...\n","  Đang xử lý câu 127/136...\n","  Đang xử lý câu 128/136...\n","  Đang xử lý câu 129/136...\n","  Đang xử lý câu 130/136...\n","  Đang xử lý câu 131/136...\n","  Đang xử lý câu 132/136...\n","  Đang xử lý câu 133/136...\n","  Đang xử lý câu 134/136...\n","  Đang xử lý câu 135/136...\n","  Đang xử lý câu 136/136...\n","Đã xử lý xong 136 câu cho động từ 'decrease_1'.\n","\n","--- Xử lý cho động từ: recognize ---\n","  Đang xử lý câu 1/29...\n","  Đang xử lý câu 2/29...\n","  Đang xử lý câu 3/29...\n","  Đang xử lý câu 4/29...\n","  Đang xử lý câu 5/29...\n","  Đang xử lý câu 6/29...\n","  Đang xử lý câu 7/29...\n","  Đang xử lý câu 8/29...\n","  Đang xử lý câu 9/29...\n","  Đang xử lý câu 10/29...\n","  Đang xử lý câu 11/29...\n","  Đang xử lý câu 12/29...\n","  Đang xử lý câu 13/29...\n","  Đang xử lý câu 14/29...\n","  Đang xử lý câu 15/29...\n","  Đang xử lý câu 16/29...\n","  Đang xử lý câu 17/29...\n","  Đang xử lý câu 18/29...\n","  Đang xử lý câu 19/29...\n","  Đang xử lý câu 20/29...\n","  Đang xử lý câu 21/29...\n","  Đang xử lý câu 22/29...\n","  Đang xử lý câu 23/29...\n","  Đang xử lý câu 24/29...\n","  Đang xử lý câu 25/29...\n","  Đang xử lý câu 26/29...\n","  Đang xử lý câu 27/29...\n","  Đang xử lý câu 28/29...\n","  Đang xử lý câu 29/29...\n","Đã xử lý xong 29 câu cho động từ 'recognize'.\n","\n","--- Xử lý cho động từ: translate_1 ---\n","  Đang xử lý câu 1/42...\n","  Đang xử lý câu 2/42...\n","  Đang xử lý câu 3/42...\n","  Đang xử lý câu 4/42...\n","  Đang xử lý câu 5/42...\n","  Đang xử lý câu 6/42...\n","  Đang xử lý câu 7/42...\n","  Đang xử lý câu 8/42...\n","  Đang xử lý câu 9/42...\n","  Đang xử lý câu 10/42...\n","  Đang xử lý câu 11/42...\n","  Đang xử lý câu 12/42...\n","  Đang xử lý câu 13/42...\n","  Đang xử lý câu 14/42...\n","  Đang xử lý câu 15/42...\n","  Đang xử lý câu 16/42...\n","  Đang xử lý câu 17/42...\n","  Đang xử lý câu 18/42...\n","  Đang xử lý câu 19/42...\n","  Đang xử lý câu 20/42...\n","  Đang xử lý câu 21/42...\n","  Đang xử lý câu 22/42...\n","  Đang xử lý câu 23/42...\n","  Đang xử lý câu 24/42...\n","  Đang xử lý câu 25/42...\n","  Đang xử lý câu 26/42...\n","  Đang xử lý câu 27/42...\n","  Đang xử lý câu 28/42...\n","  Đang xử lý câu 29/42...\n","  Đang xử lý câu 30/42...\n","  Đang xử lý câu 31/42...\n","  Đang xử lý câu 32/42...\n","  Đang xử lý câu 33/42...\n","  Đang xử lý câu 34/42...\n","  Đang xử lý câu 35/42...\n","  Đang xử lý câu 36/42...\n","  Đang xử lý câu 37/42...\n","  Đang xử lý câu 38/42...\n","  Đang xử lý câu 39/42...\n","  Đang xử lý câu 40/42...\n","  Đang xử lý câu 41/42...\n","  Đang xử lý câu 42/42...\n","Đã xử lý xong 42 câu cho động từ 'translate_1'.\n","\n","--- Xử lý cho động từ: confer ---\n","  Đang xử lý câu 1/193...\n","  Đang xử lý câu 2/193...\n","  Đang xử lý câu 3/193...\n","  Đang xử lý câu 4/193...\n","  Đang xử lý câu 5/193...\n","  Đang xử lý câu 6/193...\n","  Đang xử lý câu 7/193...\n","  Đang xử lý câu 8/193...\n","  Đang xử lý câu 9/193...\n","  Đang xử lý câu 10/193...\n","  Đang xử lý câu 11/193...\n","  Đang xử lý câu 12/193...\n","  Đang xử lý câu 13/193...\n","  Đang xử lý câu 14/193...\n","  Đang xử lý câu 15/193...\n","  Đang xử lý câu 16/193...\n","  Đang xử lý câu 17/193...\n","  Đang xử lý câu 18/193...\n","  Đang xử lý câu 19/193...\n","  Đang xử lý câu 20/193...\n","  Đang xử lý câu 21/193...\n","  Đang xử lý câu 22/193...\n","  Đang xử lý câu 23/193...\n","  Đang xử lý câu 24/193...\n","  Đang xử lý câu 25/193...\n","  Đang xử lý câu 26/193...\n","  Đang xử lý câu 27/193...\n","  Đang xử lý câu 28/193...\n","  Đang xử lý câu 29/193...\n","  Đang xử lý câu 30/193...\n","  Đang xử lý câu 31/193...\n","  Đang xử lý câu 32/193...\n","  Đang xử lý câu 33/193...\n","  Đang xử lý câu 34/193...\n","  Đang xử lý câu 35/193...\n","  Đang xử lý câu 36/193...\n","  Đang xử lý câu 37/193...\n","  Đang xử lý câu 38/193...\n","  Đang xử lý câu 39/193...\n","  Đang xử lý câu 40/193...\n","  Đang xử lý câu 41/193...\n","  Đang xử lý câu 42/193...\n","  Đang xử lý câu 43/193...\n","  Đang xử lý câu 44/193...\n","  Đang xử lý câu 45/193...\n","  Đang xử lý câu 46/193...\n","  Đang xử lý câu 47/193...\n","  Đang xử lý câu 48/193...\n","  Đang xử lý câu 49/193...\n","  Đang xử lý câu 50/193...\n","  Đang xử lý câu 51/193...\n","  Đang xử lý câu 52/193...\n","  Đang xử lý câu 53/193...\n","  Đang xử lý câu 54/193...\n","  Đang xử lý câu 55/193...\n","  Đang xử lý câu 56/193...\n","  Đang xử lý câu 57/193...\n","  Đang xử lý câu 58/193...\n","  Đang xử lý câu 59/193...\n","  Đang xử lý câu 60/193...\n","  Đang xử lý câu 61/193...\n","  Đang xử lý câu 62/193...\n","  Đang xử lý câu 63/193...\n","  Đang xử lý câu 64/193...\n","  Đang xử lý câu 65/193...\n","  Đang xử lý câu 66/193...\n","  Đang xử lý câu 67/193...\n","  Đang xử lý câu 68/193...\n","  Đang xử lý câu 69/193...\n","  Đang xử lý câu 70/193...\n","  Đang xử lý câu 71/193...\n","  Đang xử lý câu 72/193...\n","  Đang xử lý câu 73/193...\n","  Đang xử lý câu 74/193...\n","  Đang xử lý câu 75/193...\n","  Đang xử lý câu 76/193...\n","  Đang xử lý câu 77/193...\n","  Đang xử lý câu 78/193...\n","  Đang xử lý câu 79/193...\n","  Đang xử lý câu 80/193...\n","  Đang xử lý câu 81/193...\n","  Đang xử lý câu 82/193...\n","  Đang xử lý câu 83/193...\n","  Đang xử lý câu 84/193...\n","  Đang xử lý câu 85/193...\n","  Đang xử lý câu 86/193...\n","  Đang xử lý câu 87/193...\n","  Đang xử lý câu 88/193...\n","  Đang xử lý câu 89/193...\n","  Đang xử lý câu 90/193...\n","  Đang xử lý câu 91/193...\n","  Đang xử lý câu 92/193...\n","  Đang xử lý câu 93/193...\n","  Đang xử lý câu 94/193...\n","  Đang xử lý câu 95/193...\n","  Đang xử lý câu 96/193...\n","  Đang xử lý câu 97/193...\n","  Đang xử lý câu 98/193...\n","  Đang xử lý câu 99/193...\n","  Đang xử lý câu 100/193...\n","  Đang xử lý câu 101/193...\n","  Đang xử lý câu 102/193...\n","  Đang xử lý câu 103/193...\n","  Đang xử lý câu 104/193...\n","  Đang xử lý câu 105/193...\n","  Đang xử lý câu 106/193...\n","  Đang xử lý câu 107/193...\n","  Đang xử lý câu 108/193...\n","  Đang xử lý câu 109/193...\n","  Đang xử lý câu 110/193...\n","  Đang xử lý câu 111/193...\n","  Đang xử lý câu 112/193...\n","  Đang xử lý câu 113/193...\n","  Đang xử lý câu 114/193...\n","  Đang xử lý câu 115/193...\n","  Đang xử lý câu 116/193...\n","  Đang xử lý câu 117/193...\n","  Đang xử lý câu 118/193...\n","  Đang xử lý câu 119/193...\n","  Đang xử lý câu 120/193...\n","  Đang xử lý câu 121/193...\n","  Đang xử lý câu 122/193...\n","  Đang xử lý câu 123/193...\n","  Đang xử lý câu 124/193...\n","  Đang xử lý câu 125/193...\n","  Đang xử lý câu 126/193...\n","  Đang xử lý câu 127/193...\n","  Đang xử lý câu 128/193...\n","  Đang xử lý câu 129/193...\n","  Đang xử lý câu 130/193...\n","  Đang xử lý câu 131/193...\n","  Đang xử lý câu 132/193...\n","  Đang xử lý câu 133/193...\n","  Đang xử lý câu 134/193...\n","  Đang xử lý câu 135/193...\n","  Đang xử lý câu 136/193...\n","  Đang xử lý câu 137/193...\n","  Đang xử lý câu 138/193...\n","  Đang xử lý câu 139/193...\n","  Đang xử lý câu 140/193...\n","  Đang xử lý câu 141/193...\n","  Đang xử lý câu 142/193...\n","  Đang xử lý câu 143/193...\n","  Đang xử lý câu 144/193...\n","  Đang xử lý câu 145/193...\n","  Đang xử lý câu 146/193...\n","  Đang xử lý câu 147/193...\n","  Đang xử lý câu 148/193...\n","  Đang xử lý câu 149/193...\n","  Đang xử lý câu 150/193...\n","  Đang xử lý câu 151/193...\n","  Đang xử lý câu 152/193...\n","  Đang xử lý câu 153/193...\n","  Đang xử lý câu 154/193...\n","  Đang xử lý câu 155/193...\n","  Đang xử lý câu 156/193...\n","  Đang xử lý câu 157/193...\n","  Đang xử lý câu 158/193...\n","  Đang xử lý câu 159/193...\n","  Đang xử lý câu 160/193...\n","  Đang xử lý câu 161/193...\n","  Đang xử lý câu 162/193...\n","  Đang xử lý câu 163/193...\n","  Đang xử lý câu 164/193...\n","  Đang xử lý câu 165/193...\n","  Đang xử lý câu 166/193...\n","  Đang xử lý câu 167/193...\n","  Đang xử lý câu 168/193...\n","  Đang xử lý câu 169/193...\n","  Đang xử lý câu 170/193...\n","  Đang xử lý câu 171/193...\n","  Đang xử lý câu 172/193...\n","  Đang xử lý câu 173/193...\n","  Đang xử lý câu 174/193...\n","  Đang xử lý câu 175/193...\n","  Đang xử lý câu 176/193...\n","  Đang xử lý câu 177/193...\n","  Đang xử lý câu 178/193...\n","  Đang xử lý câu 179/193...\n","  Đang xử lý câu 180/193...\n","  Đang xử lý câu 181/193...\n","  Đang xử lý câu 182/193...\n","  Đang xử lý câu 183/193...\n","  Đang xử lý câu 184/193...\n","  Đang xử lý câu 185/193...\n","  Đang xử lý câu 186/193...\n","  Đang xử lý câu 187/193...\n","  Đang xử lý câu 188/193...\n","  Đang xử lý câu 189/193...\n","  Đang xử lý câu 190/193...\n","  Đang xử lý câu 191/193...\n","  Đang xử lý câu 192/193...\n","  Đang xử lý câu 193/193...\n","Đã xử lý xong 193 câu cho động từ 'confer'.\n","\n","--- Xử lý cho động từ: delete ---\n","  Đang xử lý câu 1/26...\n","  Đang xử lý câu 2/26...\n","  Đang xử lý câu 3/26...\n","  Đang xử lý câu 4/26...\n","  Đang xử lý câu 5/26...\n","  Đang xử lý câu 6/26...\n","  Đang xử lý câu 7/26...\n","  Đang xử lý câu 8/26...\n","  Đang xử lý câu 9/26...\n","  Đang xử lý câu 10/26...\n","  Đang xử lý câu 11/26...\n","  Đang xử lý câu 12/26...\n","  Đang xử lý câu 13/26...\n","  Đang xử lý câu 14/26...\n","  Đang xử lý câu 15/26...\n","  Đang xử lý câu 16/26...\n","  Đang xử lý câu 17/26...\n","  Đang xử lý câu 18/26...\n","  Đang xử lý câu 19/26...\n","  Đang xử lý câu 20/26...\n","  Đang xử lý câu 21/26...\n","  Đang xử lý câu 22/26...\n","  Đang xử lý câu 23/26...\n","  Đang xử lý câu 24/26...\n","  Đang xử lý câu 25/26...\n","  Đang xử lý câu 26/26...\n","Đã xử lý xong 26 câu cho động từ 'delete'.\n","\n","--- Xử lý cho động từ: splice_1 ---\n","  Đang xử lý câu 1/60...\n","  Đang xử lý câu 2/60...\n","  Đang xử lý câu 3/60...\n","  Đang xử lý câu 4/60...\n","  Đang xử lý câu 5/60...\n","  Đang xử lý câu 6/60...\n","  Đang xử lý câu 7/60...\n","  Đang xử lý câu 8/60...\n","  Đang xử lý câu 9/60...\n","  Đang xử lý câu 10/60...\n","  Đang xử lý câu 11/60...\n","  Đang xử lý câu 12/60...\n","  Đang xử lý câu 13/60...\n","  Đang xử lý câu 14/60...\n","  Đang xử lý câu 15/60...\n","  Đang xử lý câu 16/60...\n","  Đang xử lý câu 17/60...\n","  Đang xử lý câu 18/60...\n","  Đang xử lý câu 19/60...\n","  Đang xử lý câu 20/60...\n","  Đang xử lý câu 21/60...\n","  Đang xử lý câu 22/60...\n","  Đang xử lý câu 23/60...\n","  Đang xử lý câu 24/60...\n","  Đang xử lý câu 25/60...\n","  Đang xử lý câu 26/60...\n","  Đang xử lý câu 27/60...\n","  Đang xử lý câu 28/60...\n","  Đang xử lý câu 29/60...\n","  Đang xử lý câu 30/60...\n","  Đang xử lý câu 31/60...\n","  Đang xử lý câu 32/60...\n","  Đang xử lý câu 33/60...\n","  Đang xử lý câu 34/60...\n","  Đang xử lý câu 35/60...\n","  Đang xử lý câu 36/60...\n","  Đang xử lý câu 37/60...\n","  Đang xử lý câu 38/60...\n","  Đang xử lý câu 39/60...\n","  Đang xử lý câu 40/60...\n","  Đang xử lý câu 41/60...\n","  Đang xử lý câu 42/60...\n","  Đang xử lý câu 43/60...\n","  Đang xử lý câu 44/60...\n","  Đang xử lý câu 45/60...\n","  Đang xử lý câu 46/60...\n","  Đang xử lý câu 47/60...\n","  Đang xử lý câu 48/60...\n","  Đang xử lý câu 49/60...\n","  Đang xử lý câu 50/60...\n","  Đang xử lý câu 51/60...\n","  Đang xử lý câu 52/60...\n","  Đang xử lý câu 53/60...\n","  Đang xử lý câu 54/60...\n","  Đang xử lý câu 55/60...\n","  Đang xử lý câu 56/60...\n","  Đang xử lý câu 57/60...\n","  Đang xử lý câu 58/60...\n","  Đang xử lý câu 59/60...\n","  Đang xử lý câu 60/60...\n","Đã xử lý xong 60 câu cho động từ 'splice_1'.\n","\n","--- Xử lý cho động từ: transform_2 ---\n","  Đang xử lý câu 1/90...\n","  Đang xử lý câu 2/90...\n","  Đang xử lý câu 3/90...\n","  Đang xử lý câu 4/90...\n","  Đang xử lý câu 5/90...\n","  Đang xử lý câu 6/90...\n","  Đang xử lý câu 7/90...\n","  Đang xử lý câu 8/90...\n","  Đang xử lý câu 9/90...\n","  Đang xử lý câu 10/90...\n","  Đang xử lý câu 11/90...\n","  Đang xử lý câu 12/90...\n","  Đang xử lý câu 13/90...\n","  Đang xử lý câu 14/90...\n","  Đang xử lý câu 15/90...\n","  Đang xử lý câu 16/90...\n","  Đang xử lý câu 17/90...\n","  Đang xử lý câu 18/90...\n","  Đang xử lý câu 19/90...\n","  Đang xử lý câu 20/90...\n","  Đang xử lý câu 21/90...\n","  Đang xử lý câu 22/90...\n","  Đang xử lý câu 23/90...\n","  Đang xử lý câu 24/90...\n","  Đang xử lý câu 25/90...\n","  Đang xử lý câu 26/90...\n","  Đang xử lý câu 27/90...\n","  Đang xử lý câu 28/90...\n","  Đang xử lý câu 29/90...\n","  Đang xử lý câu 30/90...\n","  Đang xử lý câu 31/90...\n","  Đang xử lý câu 32/90...\n","  Đang xử lý câu 33/90...\n","  Đang xử lý câu 34/90...\n","  Đang xử lý câu 35/90...\n","  Đang xử lý câu 36/90...\n","  Đang xử lý câu 37/90...\n","  Đang xử lý câu 38/90...\n","  Đang xử lý câu 39/90...\n","  Đang xử lý câu 40/90...\n","  Đang xử lý câu 41/90...\n","  Đang xử lý câu 42/90...\n","  Đang xử lý câu 43/90...\n","  Đang xử lý câu 44/90...\n","  Đang xử lý câu 45/90...\n","  Đang xử lý câu 46/90...\n","  Đang xử lý câu 47/90...\n","  Đang xử lý câu 48/90...\n","  Đang xử lý câu 49/90...\n","  Đang xử lý câu 50/90...\n","  Đang xử lý câu 51/90...\n","  Đang xử lý câu 52/90...\n","  Đang xử lý câu 53/90...\n","  Đang xử lý câu 54/90...\n","  Đang xử lý câu 55/90...\n","  Đang xử lý câu 56/90...\n","  Đang xử lý câu 57/90...\n","  Đang xử lý câu 58/90...\n","  Đang xử lý câu 59/90...\n","  Đang xử lý câu 60/90...\n","  Đang xử lý câu 61/90...\n","  Đang xử lý câu 62/90...\n","  Đang xử lý câu 63/90...\n","  Đang xử lý câu 64/90...\n","  Đang xử lý câu 65/90...\n","  Đang xử lý câu 66/90...\n","  Đang xử lý câu 67/90...\n","  Đang xử lý câu 68/90...\n","  Đang xử lý câu 69/90...\n","  Đang xử lý câu 70/90...\n","  Đang xử lý câu 71/90...\n","  Đang xử lý câu 72/90...\n","  Đang xử lý câu 73/90...\n","  Đang xử lý câu 74/90...\n","  Đang xử lý câu 75/90...\n","  Đang xử lý câu 76/90...\n","  Đang xử lý câu 77/90...\n","  Đang xử lý câu 78/90...\n","  Đang xử lý câu 79/90...\n","  Đang xử lý câu 80/90...\n","  Đang xử lý câu 81/90...\n","  Đang xử lý câu 82/90...\n","  Đang xử lý câu 83/90...\n","  Đang xử lý câu 84/90...\n","  Đang xử lý câu 85/90...\n","  Đang xử lý câu 86/90...\n","  Đang xử lý câu 87/90...\n","  Đang xử lý câu 88/90...\n","  Đang xử lý câu 89/90...\n","  Đang xử lý câu 90/90...\n","Đã xử lý xong 90 câu cho động từ 'transform_2'.\n","\n","--- Xử lý cho động từ: generate ---\n","  Đang xử lý câu 1/77...\n","  Đang xử lý câu 2/77...\n","  Đang xử lý câu 3/77...\n","  Đang xử lý câu 4/77...\n","  Đang xử lý câu 5/77...\n","  Đang xử lý câu 6/77...\n","  Đang xử lý câu 7/77...\n","  Đang xử lý câu 8/77...\n","  Đang xử lý câu 9/77...\n","  Đang xử lý câu 10/77...\n","  Đang xử lý câu 11/77...\n","  Đang xử lý câu 12/77...\n","  Đang xử lý câu 13/77...\n","  Đang xử lý câu 14/77...\n","  Đang xử lý câu 15/77...\n","  Đang xử lý câu 16/77...\n","  Đang xử lý câu 17/77...\n","  Đang xử lý câu 18/77...\n","  Đang xử lý câu 19/77...\n","  Đang xử lý câu 20/77...\n","  Đang xử lý câu 21/77...\n","  Đang xử lý câu 22/77...\n","  Đang xử lý câu 23/77...\n","  Đang xử lý câu 24/77...\n","  Đang xử lý câu 25/77...\n","  Đang xử lý câu 26/77...\n","  Đang xử lý câu 27/77...\n","  Đang xử lý câu 28/77...\n","  Đang xử lý câu 29/77...\n","  Đang xử lý câu 30/77...\n","  Đang xử lý câu 31/77...\n","  Đang xử lý câu 32/77...\n","  Đang xử lý câu 33/77...\n","  Đang xử lý câu 34/77...\n","  Đang xử lý câu 35/77...\n","  Đang xử lý câu 36/77...\n","  Đang xử lý câu 37/77...\n","  Đang xử lý câu 38/77...\n","  Đang xử lý câu 39/77...\n","  Đang xử lý câu 40/77...\n","  Đang xử lý câu 41/77...\n","  Đang xử lý câu 42/77...\n","  Đang xử lý câu 43/77...\n","  Đang xử lý câu 44/77...\n","  Đang xử lý câu 45/77...\n","  Đang xử lý câu 46/77...\n","  Đang xử lý câu 47/77...\n","  Đang xử lý câu 48/77...\n","  Đang xử lý câu 49/77...\n","  Đang xử lý câu 50/77...\n","  Đang xử lý câu 51/77...\n","  Đang xử lý câu 52/77...\n","  Đang xử lý câu 53/77...\n","  Đang xử lý câu 54/77...\n","  Đang xử lý câu 55/77...\n","  Đang xử lý câu 56/77...\n","  Đang xử lý câu 57/77...\n","  Đang xử lý câu 58/77...\n","  Đang xử lý câu 59/77...\n","  Đang xử lý câu 60/77...\n","  Đang xử lý câu 61/77...\n","  Đang xử lý câu 62/77...\n","  Đang xử lý câu 63/77...\n","  Đang xử lý câu 64/77...\n","  Đang xử lý câu 65/77...\n","  Đang xử lý câu 66/77...\n","  Đang xử lý câu 67/77...\n","  Đang xử lý câu 68/77...\n","  Đang xử lý câu 69/77...\n","  Đang xử lý câu 70/77...\n","  Đang xử lý câu 71/77...\n","  Đang xử lý câu 72/77...\n","  Đang xử lý câu 73/77...\n","  Đang xử lý câu 74/77...\n","  Đang xử lý câu 75/77...\n","  Đang xử lý câu 76/77...\n","  Đang xử lý câu 77/77...\n","Đã xử lý xong 77 câu cho động từ 'generate'.\n","\n","--- Xử lý cho động từ: decrease_2 ---\n","  Đang xử lý câu 1/41...\n","  Đang xử lý câu 2/41...\n","  Đang xử lý câu 3/41...\n","  Đang xử lý câu 4/41...\n","  Đang xử lý câu 5/41...\n","  Đang xử lý câu 6/41...\n","  Đang xử lý câu 7/41...\n","  Đang xử lý câu 8/41...\n","  Đang xử lý câu 9/41...\n","  Đang xử lý câu 10/41...\n","  Đang xử lý câu 11/41...\n","  Đang xử lý câu 12/41...\n","  Đang xử lý câu 13/41...\n","  Đang xử lý câu 14/41...\n","  Đang xử lý câu 15/41...\n","  Đang xử lý câu 16/41...\n","  Đang xử lý câu 17/41...\n","  Đang xử lý câu 18/41...\n","  Đang xử lý câu 19/41...\n","  Đang xử lý câu 20/41...\n","  Đang xử lý câu 21/41...\n","  Đang xử lý câu 22/41...\n","  Đang xử lý câu 23/41...\n","  Đang xử lý câu 24/41...\n","  Đang xử lý câu 25/41...\n","  Đang xử lý câu 26/41...\n","  Đang xử lý câu 27/41...\n","  Đang xử lý câu 28/41...\n","  Đang xử lý câu 29/41...\n","  Đang xử lý câu 30/41...\n","  Đang xử lý câu 31/41...\n","  Đang xử lý câu 32/41...\n","  Đang xử lý câu 33/41...\n","  Đang xử lý câu 34/41...\n","  Đang xử lý câu 35/41...\n","  Đang xử lý câu 36/41...\n","  Đang xử lý câu 37/41...\n","  Đang xử lý câu 38/41...\n","  Đang xử lý câu 39/41...\n","  Đang xử lý câu 40/41...\n","  Đang xử lý câu 41/41...\n","Đã xử lý xong 41 câu cho động từ 'decrease_2'.\n","\n","--- Xử lý cho động từ: initiate ---\n","  Đang xử lý câu 1/35...\n","  Đang xử lý câu 2/35...\n","  Đang xử lý câu 3/35...\n","  Đang xử lý câu 4/35...\n","  Đang xử lý câu 5/35...\n","  Đang xử lý câu 6/35...\n","  Đang xử lý câu 7/35...\n","  Đang xử lý câu 8/35...\n","  Đang xử lý câu 9/35...\n","  Đang xử lý câu 10/35...\n","  Đang xử lý câu 11/35...\n","  Đang xử lý câu 12/35...\n","  Đang xử lý câu 13/35...\n","  Đang xử lý câu 14/35...\n","  Đang xử lý câu 15/35...\n","  Đang xử lý câu 16/35...\n","  Đang xử lý câu 17/35...\n","  Đang xử lý câu 18/35...\n","  Đang xử lý câu 19/35...\n","  Đang xử lý câu 20/35...\n","  Đang xử lý câu 21/35...\n","  Đang xử lý câu 22/35...\n","  Đang xử lý câu 23/35...\n","  Đang xử lý câu 24/35...\n","  Đang xử lý câu 25/35...\n","  Đang xử lý câu 26/35...\n","  Đang xử lý câu 27/35...\n","  Đang xử lý câu 28/35...\n","  Đang xử lý câu 29/35...\n","  Đang xử lý câu 30/35...\n","  Đang xử lý câu 31/35...\n","  Đang xử lý câu 32/35...\n","  Đang xử lý câu 33/35...\n","  Đang xử lý câu 34/35...\n","  Đang xử lý câu 35/35...\n","Đã xử lý xong 35 câu cho động từ 'initiate'.\n","\n","--- Xử lý cho động từ: truncate ---\n","  Đang xử lý câu 1/20...\n","  Đang xử lý câu 2/20...\n","  Đang xử lý câu 3/20...\n","  Đang xử lý câu 4/20...\n","  Đang xử lý câu 5/20...\n","  Đang xử lý câu 6/20...\n","  Đang xử lý câu 7/20...\n","  Đang xử lý câu 8/20...\n","  Đang xử lý câu 9/20...\n","  Đang xử lý câu 10/20...\n","  Đang xử lý câu 11/20...\n","  Đang xử lý câu 12/20...\n","  Đang xử lý câu 13/20...\n","  Đang xử lý câu 14/20...\n","  Đang xử lý câu 15/20...\n","  Đang xử lý câu 16/20...\n","  Đang xử lý câu 17/20...\n","  Đang xử lý câu 18/20...\n","  Đang xử lý câu 19/20...\n","  Đang xử lý câu 20/20...\n","Đã xử lý xong 20 câu cho động từ 'truncate'.\n","\n","--- Xử lý cho động từ: transcribe ---\n","  Đang xử lý câu 1/62...\n","  Đang xử lý câu 2/62...\n","  Đang xử lý câu 3/62...\n","  Đang xử lý câu 4/62...\n","  Đang xử lý câu 5/62...\n","  Đang xử lý câu 6/62...\n","  Đang xử lý câu 7/62...\n","  Đang xử lý câu 8/62...\n","  Đang xử lý câu 9/62...\n","  Đang xử lý câu 10/62...\n","  Đang xử lý câu 11/62...\n","  Đang xử lý câu 12/62...\n","  Đang xử lý câu 13/62...\n","  Đang xử lý câu 14/62...\n","  Đang xử lý câu 15/62...\n","  Đang xử lý câu 16/62...\n","  Đang xử lý câu 17/62...\n","  Đang xử lý câu 18/62...\n","  Đang xử lý câu 19/62...\n","  Đang xử lý câu 20/62...\n","  Đang xử lý câu 21/62...\n","  Đang xử lý câu 22/62...\n","  Đang xử lý câu 23/62...\n","  Đang xử lý câu 24/62...\n","  Đang xử lý câu 25/62...\n","  Đang xử lý câu 26/62...\n","  Đang xử lý câu 27/62...\n","  Đang xử lý câu 28/62...\n","  Đang xử lý câu 29/62...\n","  Đang xử lý câu 30/62...\n","  Đang xử lý câu 31/62...\n","  Đang xử lý câu 32/62...\n","  Đang xử lý câu 33/62...\n","  Đang xử lý câu 34/62...\n","  Đang xử lý câu 35/62...\n","  Đang xử lý câu 36/62...\n","  Đang xử lý câu 37/62...\n","  Đang xử lý câu 38/62...\n","  Đang xử lý câu 39/62...\n","  Đang xử lý câu 40/62...\n","  Đang xử lý câu 41/62...\n","  Đang xử lý câu 42/62...\n","  Đang xử lý câu 43/62...\n","  Đang xử lý câu 44/62...\n","  Đang xử lý câu 45/62...\n","  Đang xử lý câu 46/62...\n","  Đang xử lý câu 47/62...\n","  Đang xử lý câu 48/62...\n","  Đang xử lý câu 49/62...\n","  Đang xử lý câu 50/62...\n","  Đang xử lý câu 51/62...\n","  Đang xử lý câu 52/62...\n","  Đang xử lý câu 53/62...\n","  Đang xử lý câu 54/62...\n","  Đang xử lý câu 55/62...\n","  Đang xử lý câu 56/62...\n","  Đang xử lý câu 57/62...\n","  Đang xử lý câu 58/62...\n","  Đang xử lý câu 59/62...\n","  Đang xử lý câu 60/62...\n","  Đang xử lý câu 61/62...\n","  Đang xử lý câu 62/62...\n","Đã xử lý xong 62 câu cho động từ 'transcribe'.\n","\n","--- Xử lý cho động từ: disrupt ---\n","  Đang xử lý câu 1/23...\n","  Đang xử lý câu 2/23...\n","  Đang xử lý câu 3/23...\n","  Đang xử lý câu 4/23...\n","  Đang xử lý câu 5/23...\n","  Đang xử lý câu 6/23...\n","  Đang xử lý câu 7/23...\n","  Đang xử lý câu 8/23...\n","  Đang xử lý câu 9/23...\n","  Đang xử lý câu 10/23...\n","  Đang xử lý câu 11/23...\n","  Đang xử lý câu 12/23...\n","  Đang xử lý câu 13/23...\n","  Đang xử lý câu 14/23...\n","  Đang xử lý câu 15/23...\n","  Đang xử lý câu 16/23...\n","  Đang xử lý câu 17/23...\n","  Đang xử lý câu 18/23...\n","  Đang xử lý câu 19/23...\n","  Đang xử lý câu 20/23...\n","  Đang xử lý câu 21/23...\n","  Đang xử lý câu 22/23...\n","  Đang xử lý câu 23/23...\n","Đã xử lý xong 23 câu cho động từ 'disrupt'.\n","\n","--- Xử lý cho động từ: skip ---\n","  Đang xử lý câu 1/61...\n","  Đang xử lý câu 2/61...\n","  Đang xử lý câu 3/61...\n","  Đang xử lý câu 4/61...\n","  Đang xử lý câu 5/61...\n","  Đang xử lý câu 6/61...\n","  Đang xử lý câu 7/61...\n","  Đang xử lý câu 8/61...\n","  Đang xử lý câu 9/61...\n","  Đang xử lý câu 10/61...\n","  Đang xử lý câu 11/61...\n","  Đang xử lý câu 12/61...\n","  Đang xử lý câu 13/61...\n","  Đang xử lý câu 14/61...\n","  Đang xử lý câu 15/61...\n","  Đang xử lý câu 16/61...\n","  Đang xử lý câu 17/61...\n","  Đang xử lý câu 18/61...\n","  Đang xử lý câu 19/61...\n","  Đang xử lý câu 20/61...\n","  Đang xử lý câu 21/61...\n","  Đang xử lý câu 22/61...\n","  Đang xử lý câu 23/61...\n","  Đang xử lý câu 24/61...\n","  Đang xử lý câu 25/61...\n","  Đang xử lý câu 26/61...\n","  Đang xử lý câu 27/61...\n","  Đang xử lý câu 28/61...\n","  Đang xử lý câu 29/61...\n","  Đang xử lý câu 30/61...\n","  Đang xử lý câu 31/61...\n","  Đang xử lý câu 32/61...\n","  Đang xử lý câu 33/61...\n","  Đang xử lý câu 34/61...\n","  Đang xử lý câu 35/61...\n","  Đang xử lý câu 36/61...\n","  Đang xử lý câu 37/61...\n","  Đang xử lý câu 38/61...\n","  Đang xử lý câu 39/61...\n","  Đang xử lý câu 40/61...\n","  Đang xử lý câu 41/61...\n","  Đang xử lý câu 42/61...\n","  Đang xử lý câu 43/61...\n","  Đang xử lý câu 44/61...\n","  Đang xử lý câu 45/61...\n","  Đang xử lý câu 46/61...\n","  Đang xử lý câu 47/61...\n","  Đang xử lý câu 48/61...\n","  Đang xử lý câu 49/61...\n","  Đang xử lý câu 50/61...\n","  Đang xử lý câu 51/61...\n","  Đang xử lý câu 52/61...\n","  Đang xử lý câu 53/61...\n","  Đang xử lý câu 54/61...\n","  Đang xử lý câu 55/61...\n","  Đang xử lý câu 56/61...\n","  Đang xử lý câu 57/61...\n","  Đang xử lý câu 58/61...\n","  Đang xử lý câu 59/61...\n","  Đang xử lý câu 60/61...\n","  Đang xử lý câu 61/61...\n","Đã xử lý xong 61 câu cho động từ 'skip'.\n","\n","--- Xử lý cho động từ: mutate ---\n","  Đang xử lý câu 1/45...\n","  Đang xử lý câu 2/45...\n","  Đang xử lý câu 3/45...\n","  Đang xử lý câu 4/45...\n","  Đang xử lý câu 5/45...\n","  Đang xử lý câu 6/45...\n","  Đang xử lý câu 7/45...\n","  Đang xử lý câu 8/45...\n","  Đang xử lý câu 9/45...\n","  Đang xử lý câu 10/45...\n","  Đang xử lý câu 11/45...\n","  Đang xử lý câu 12/45...\n","  Đang xử lý câu 13/45...\n","  Đang xử lý câu 14/45...\n","  Đang xử lý câu 15/45...\n","  Đang xử lý câu 16/45...\n","  Đang xử lý câu 17/45...\n","  Đang xử lý câu 18/45...\n","  Đang xử lý câu 19/45...\n","  Đang xử lý câu 20/45...\n","  Đang xử lý câu 21/45...\n","  Đang xử lý câu 22/45...\n","  Đang xử lý câu 23/45...\n","  Đang xử lý câu 24/45...\n","  Đang xử lý câu 25/45...\n","  Đang xử lý câu 26/45...\n","  Đang xử lý câu 27/45...\n","  Đang xử lý câu 28/45...\n","  Đang xử lý câu 29/45...\n","  Đang xử lý câu 30/45...\n","  Đang xử lý câu 31/45...\n","  Đang xử lý câu 32/45...\n","  Đang xử lý câu 33/45...\n","  Đang xử lý câu 34/45...\n","  Đang xử lý câu 35/45...\n","  Đang xử lý câu 36/45...\n","  Đang xử lý câu 37/45...\n","  Đang xử lý câu 38/45...\n","  Đang xử lý câu 39/45...\n","  Đang xử lý câu 40/45...\n","  Đang xử lý câu 41/45...\n","  Đang xử lý câu 42/45...\n","  Đang xử lý câu 43/45...\n","  Đang xử lý câu 44/45...\n","  Đang xử lý câu 45/45...\n","Đã xử lý xong 45 câu cho động từ 'mutate'.\n"]}]},{"cell_type":"markdown","source":["- **Kết quả**:  \n","  - Sinh cấu trúc output theo từng động từ, ví dụ:  \n","    ```\n","    .../train_vectors_finetuned_parave/\n","      └── begin_1/\n","          ├── sentence_0.pt\n","          ├── sentence_1.pt\n","          └── ...\n","    ```  \n","  - Mỗi file `.pt` là một dict:  \n","    - Key: `layer_0`, `layer_1`, … (embedding + các lớp Transformer).  \n","    - Value: tensor kích thước `[num_tokens, hidden_size]` (BioBERT base: `hidden_size = 768`)."],"metadata":{"id":"p2nA1diYKNN4"}},{"cell_type":"markdown","source":["- **Lưu ý**:  \n","  - Đảm bảo `final_model_path` và `train_data_dir` chính xác (ParaVE, không phải GramVar).  \n","  - Các câu rỗng được bỏ qua (`.strip()`).  \n","  - Nếu cần tái lập trình tự xử lý/ghi log, có thể đặt seed trước khi duyệt.  \n","  - Các file JSON đầu vào cần có trường `text` trong từng phần tử."],"metadata":{"id":"a9S_9RKnKQCb"}},{"cell_type":"markdown","source":["## Tạo file index cho từng động từ (ParaVE)"],"metadata":{"id":"WB-W5WvmKY-v"}},{"cell_type":"markdown","source":["- **Mục đích**:  \n","  - Sinh các **file index `.json`** cho từng động từ trong **tập Train của ParaVE**.  \n","  - Mỗi file index liệt kê **câu gốc** và **đường dẫn file vector `.pt`** tương ứng (sẽ/đã được trích xuất).  \n","  - Hỗ trợ tra cứu nhanh: (động từ, chỉ số câu) → mở đúng file vector để phân tích."],"metadata":{"id":"ADdjicIDKeGq"}},{"cell_type":"markdown","source":["- **Quy trình**"],"metadata":{"id":"pAXi6Z6tC8jF"}},{"cell_type":"markdown","source":[" 1. **Thiết lập đường dẫn**  \n","     - `train_data_dir`: thư mục chứa các file train của ParaVE (mỗi file một động từ, ví dụ `begin_1_train_set.json`).  \n","     - `train_output_base_dir`: thư mục cha sẽ chứa vector `.pt` theo cấu trúc `<verb_name>/sentence_<i>.pt`.  \n","     - `output_index_dir`: nơi lưu **các file index** (mỗi động từ một file JSON)."],"metadata":{"id":"x0NK4bIZKfyi"}},{"cell_type":"code","source":["# Đường dẫn thư mục gốc trên Google Drive để làm việc\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file train gốc (.json)\n","train_data_dir = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_ParaVE/Train'\n","\n","# Đường dẫn đến thư mục cha chứa các vector đã được xử lý\n","train_output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Train/Finetuned/train_vectors_finetuned_parave/')\n","\n","# Đường dẫn đến thư mục để lưu các file index JSON riêng lẻ\n","output_index_dir = os.path.join(drive_base_path, 'verb_indexes_finetuned_parave_train')"],"metadata":{"id":"NCFW8ROpKuzJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" 2. **Hàm `create_sentence_index_per_verb()`**  \n","     - Quét tất cả file `*.json` trong `train_data_dir`.  \n","     - Suy ra `verb_name` từ tên file (vd: `begin_1_train_set.json` → `begin_1`).  \n","     - Đọc JSON, lấy danh sách các câu từ trường `text` (lọc rỗng).  \n","     - Với mỗi câu `i`, tạo một entry gồm:  \n","       - `sentence_text`: câu gốc.  \n","       - `verb_name`: tên động từ.  \n","       - `vector_file_path`: đường dẫn dự kiến tới vector `.pt` (vd: `.../verb_indexes_finetuned_parave_train/begin_1/sentence_0.pt`).  \n","     - Ghi toàn bộ entry ra file `output_index_dir/<verb_name>.json` (định dạng danh sách, `indent=4`)."],"metadata":{"id":"awOF5OOVKu_H"}},{"cell_type":"code","source":["def create_sentence_index_per_verb():\n","    \"\"\"\n","    Quét qua các file dữ liệu train và tạo ra một file index .json riêng\n","    cho mỗi động từ.\n","    \"\"\"\n","\n","    print(\"Bắt đầu quá trình tạo file index cho từng động từ...\")\n","\n","    # Tạo thư mục chứa các file index nếu nó chưa tồn tại\n","    os.makedirs(output_index_dir, exist_ok=True)\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục train gốc\n","        json_files = glob.glob(os.path.join(train_data_dir, '*.json'))\n","\n","        if not json_files:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong thư mục: {train_data_dir}\")\n","            return\n","\n","        print(f\"Tìm thấy {len(json_files)} file .json để tạo index.\")\n","\n","        # Lặp qua từng file json\n","        for file_path in json_files:\n","            # Lấy tên động từ từ tên file (ví dụ: 'begin_1')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_train_set.json', '').split('_'))\n","\n","            print(f\"\\n--- Đang xử lý cho động từ: {verb_name} ---\")\n","\n","            # Khởi tạo một danh sách để chứa index cho động từ hiện tại\n","            verb_index_list = []\n","\n","            # Đọc dữ liệu từ file JSON\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Lặp qua từng câu để tạo entry cho file index\n","            for i, sentence in enumerate(sentences):\n","                # Xây dựng đường dẫn dự kiến đến file vector .pt tương ứng\n","                vector_file_name = f\"sentence_{i}.pt\"\n","                vector_file_path = os.path.join(train_output_base_dir, verb_name, vector_file_name)\n","\n","                # Tạo một entry cho file index\n","                index_entry = {\n","                    \"sentence_text\": sentence,\n","                    \"verb_name\": verb_name,\n","                    \"vector_file_path\": vector_file_path\n","                }\n","                verb_index_list.append(index_entry)\n","\n","            # Sau khi xử lý xong các câu của một động từ, lưu ra file JSON riêng\n","            if verb_index_list:\n","                output_verb_index_file = os.path.join(output_index_dir, f\"{verb_name}.json\")\n","                print(f\"Đang lưu file index cho '{verb_name}' vào: {output_verb_index_file}\")\n","                with open(output_verb_index_file, 'w', encoding='utf-8') as f:\n","                    # Lưu danh sách các entry, không phải dictionary\n","                    json.dump(verb_index_list, f, ensure_ascii=False, indent=4)\n","                print(f\"Đã tạo file index cho {len(sentences)} câu của động từ '{verb_name}'.\")\n","\n","        print(\"\\nQUÁ TRÌNH TẠO FILE INDEX ĐÃ HOÀN TẤT!\")\n","\n","    except FileNotFoundError:\n","        print(f\"\\nLỖI: Không tìm thấy thư mục dữ liệu train tại '{train_data_dir}'.\")\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")"],"metadata":{"id":"FzWSk2Y2Kvxn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. **Thực thi**  \n","     - Gọi `create_sentence_index_per_verb()` trong khối `if __name__ == \"__main__\":`.  \n","     - In log tiến trình (số file train, tên động từ đang xử lý, đường dẫn file index đã lưu, v.v.)."],"metadata":{"id":"UApmQ53xKv9z"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    create_sentence_index_per_verb()"],"metadata":{"id":"m-gXkImrKzgn","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1759903236428,"user_tz":-420,"elapsed":32248,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"0eadb084-a0dc-4b0c-e0c5-f871872e2be7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bắt đầu quá trình tạo file index cho từng động từ...\n","Tìm thấy 35 file .json để tạo index.\n","\n","--- Đang xử lý cho động từ: catalyse ---\n","Đang lưu file index cho 'catalyse' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/catalyse.json\n","Đã tạo file index cho 43 câu của động từ 'catalyse'.\n","\n","--- Đang xử lý cho động từ: result ---\n","Đang lưu file index cho 'result' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/result.json\n","Đã tạo file index cho 218 câu của động từ 'result'.\n","\n","--- Đang xử lý cho động từ: develop ---\n","Đang lưu file index cho 'develop' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/develop.json\n","Đã tạo file index cho 10 câu của động từ 'develop'.\n","\n","--- Đang xử lý cho động từ: eliminate ---\n","Đang lưu file index cho 'eliminate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/eliminate.json\n","Đã tạo file index cho 30 câu của động từ 'eliminate'.\n","\n","--- Đang xử lý cho động từ: translate_3 ---\n","Đang lưu file index cho 'translate_3' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/translate_3.json\n","Đã tạo file index cho 186 câu của động từ 'translate_3'.\n","\n","--- Đang xử lý cho động từ: transform_1 ---\n","Đang lưu file index cho 'transform_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/transform_1.json\n","Đã tạo file index cho 30 câu của động từ 'transform_1'.\n","\n","--- Đang xử lý cho động từ: begin_2 ---\n","Đang lưu file index cho 'begin_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/begin_2.json\n","Đã tạo file index cho 186 câu của động từ 'begin_2'.\n","\n","--- Đang xử lý cho động từ: block ---\n","Đang lưu file index cho 'block' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/block.json\n","Đã tạo file index cho 115 câu của động từ 'block'.\n","\n","--- Đang xử lý cho động từ: abolish ---\n","Đang lưu file index cho 'abolish' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/abolish.json\n","Đã tạo file index cho 64 câu của động từ 'abolish'.\n","\n","--- Đang xử lý cho động từ: translate_2 ---\n","Đang lưu file index cho 'translate_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/translate_2.json\n","Đã tạo file index cho 138 câu của động từ 'translate_2'.\n","\n","--- Đang xử lý cho động từ: inhibit ---\n","Đang lưu file index cho 'inhibit' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/inhibit.json\n","Đã tạo file index cho 63 câu của động từ 'inhibit'.\n","\n","--- Đang xử lý cho động từ: begin_1 ---\n","Đang lưu file index cho 'begin_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/begin_1.json\n","Đã tạo file index cho 25 câu của động từ 'begin_1'.\n","\n","--- Đang xử lý cho động từ: splice_2 ---\n","Đang lưu file index cho 'splice_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/splice_2.json\n","Đã tạo file index cho 39 câu của động từ 'splice_2'.\n","\n","--- Đang xử lý cho động từ: modify ---\n","Đang lưu file index cho 'modify' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/modify.json\n","Đã tạo file index cho 7 câu của động từ 'modify'.\n","\n","--- Đang xử lý cho động từ: lose ---\n","Đang lưu file index cho 'lose' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/lose.json\n","Đã tạo file index cho 36 câu của động từ 'lose'.\n","\n","--- Đang xử lý cho động từ: lead ---\n","Đang lưu file index cho 'lead' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/lead.json\n","Đã tạo file index cho 80 câu của động từ 'lead'.\n","\n","--- Đang xử lý cho động từ: encode ---\n","Đang lưu file index cho 'encode' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/encode.json\n","Đã tạo file index cho 29 câu của động từ 'encode'.\n","\n","--- Đang xử lý cho động từ: proliferate ---\n","Đang lưu file index cho 'proliferate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/proliferate.json\n","Đã tạo file index cho 33 câu của động từ 'proliferate'.\n","\n","--- Đang xử lý cho động từ: express ---\n","Đang lưu file index cho 'express' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/express.json\n","Đã tạo file index cho 86 câu của động từ 'express'.\n","\n","--- Đang xử lý cho động từ: alter ---\n","Đang lưu file index cho 'alter' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/alter.json\n","Đã tạo file index cho 72 câu của động từ 'alter'.\n","\n","--- Đang xử lý cho động từ: decrease_1 ---\n","Đang lưu file index cho 'decrease_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/decrease_1.json\n","Đã tạo file index cho 136 câu của động từ 'decrease_1'.\n","\n","--- Đang xử lý cho động từ: recognize ---\n","Đang lưu file index cho 'recognize' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/recognize.json\n","Đã tạo file index cho 29 câu của động từ 'recognize'.\n","\n","--- Đang xử lý cho động từ: translate_1 ---\n","Đang lưu file index cho 'translate_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/translate_1.json\n","Đã tạo file index cho 42 câu của động từ 'translate_1'.\n","\n","--- Đang xử lý cho động từ: confer ---\n","Đang lưu file index cho 'confer' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/confer.json\n","Đã tạo file index cho 193 câu của động từ 'confer'.\n","\n","--- Đang xử lý cho động từ: delete ---\n","Đang lưu file index cho 'delete' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/delete.json\n","Đã tạo file index cho 26 câu của động từ 'delete'.\n","\n","--- Đang xử lý cho động từ: splice_1 ---\n","Đang lưu file index cho 'splice_1' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/splice_1.json\n","Đã tạo file index cho 60 câu của động từ 'splice_1'.\n","\n","--- Đang xử lý cho động từ: transform_2 ---\n","Đang lưu file index cho 'transform_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/transform_2.json\n","Đã tạo file index cho 90 câu của động từ 'transform_2'.\n","\n","--- Đang xử lý cho động từ: generate ---\n","Đang lưu file index cho 'generate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/generate.json\n","Đã tạo file index cho 77 câu của động từ 'generate'.\n","\n","--- Đang xử lý cho động từ: decrease_2 ---\n","Đang lưu file index cho 'decrease_2' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/decrease_2.json\n","Đã tạo file index cho 41 câu của động từ 'decrease_2'.\n","\n","--- Đang xử lý cho động từ: initiate ---\n","Đang lưu file index cho 'initiate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/initiate.json\n","Đã tạo file index cho 35 câu của động từ 'initiate'.\n","\n","--- Đang xử lý cho động từ: truncate ---\n","Đang lưu file index cho 'truncate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/truncate.json\n","Đã tạo file index cho 20 câu của động từ 'truncate'.\n","\n","--- Đang xử lý cho động từ: transcribe ---\n","Đang lưu file index cho 'transcribe' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/transcribe.json\n","Đã tạo file index cho 62 câu của động từ 'transcribe'.\n","\n","--- Đang xử lý cho động từ: disrupt ---\n","Đang lưu file index cho 'disrupt' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/disrupt.json\n","Đã tạo file index cho 23 câu của động từ 'disrupt'.\n","\n","--- Đang xử lý cho động từ: skip ---\n","Đang lưu file index cho 'skip' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/skip.json\n","Đã tạo file index cho 61 câu của động từ 'skip'.\n","\n","--- Đang xử lý cho động từ: mutate ---\n","Đang lưu file index cho 'mutate' vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_parave_finetuned/mutate.json\n","Đã tạo file index cho 45 câu của động từ 'mutate'.\n","\n","QUÁ TRÌNH TẠO FILE INDEX ĐÃ HOÀN TẤT!\n"]}]},{"cell_type":"markdown","source":["- **Kết quả**:  \n","  - Thư mục `verb_indexes_finetuned_parave_train/` chứa nhiều file JSON, mỗi file ứng với **một động từ**:  \n","    ```\n","    .../verb_indexes_finetuned_parave_train/\n","      ├── begin_1.json\n","      ├── block.json\n","      └── ...\n","    ```\n","  - Mỗi file là **một danh sách** entry có dạng:\n","    ```json\n","    {\n","      \"sentence_text\": \"...\",\n","      \"verb_name\": \"begin_1\",\n","      \"vector_file_path\": \"/.../train_vectors_finetuned_parave/begin_1/sentence_0.pt\"\n","    }\n","    ```\n","  - Sẵn sàng dùng cho các bước phân tích sau: nạp nhanh vector theo `verb_name` + `sentence_index`."],"metadata":{"id":"yjQ-ZxYRKzv7"}},{"cell_type":"markdown","source":["- **Lưu ý**:  \n","  - Cấu trúc & logic **tương tự GramVar**, chỉ khác **đường dẫn nguồn** (ParaVE) và **thư mục output** (`train_vectors_by_verb_parave`).  \n","  - Index có thể tạo **trước** khi trích xuất vector; khi vector `.pt` được sinh đúng cấu trúc, đường dẫn trong index sẽ khớp.  "],"metadata":{"id":"OnpAl76fK2RS"}},{"cell_type":"markdown","source":["# Train Dataset - Mô hình Pre-trained"],"metadata":{"id":"3yYj8TNbO2uj"}},{"cell_type":"markdown","source":["## Trích xuất hidden states theo từng động từ (GramVar) và lưu thành file `.pt`"],"metadata":{"id":"zfKi-2YkO-a9"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Lấy biểu diễn ẩn (hidden states) của **mọi lớp** từ mô hình BioBERT gốc (`dmis-lab/biobert-base-cased-v1.2`) cho **từng câu** trong tập train.\n","  - Lưu lại dưới dạng file `.pt` theo **từng động từ** để dùng cho các bước thống kê/chuẩn hoá/huấn luyện downstream."],"metadata":{"id":"V-BsFPTFPEsd"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục JSON của tập train tách theo động từ:  \n","    `.../Clean_Dataset/Corpus/Split_GramVar/Train/*.json`\n","  - Mỗi JSON là danh sách các item có khoá `text`."],"metadata":{"id":"m4X6c-00PO7Q"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Cấu trúc thư mục vector:  \n","    `.../train_vectors_pretrained_gramvar/<verb_name>/sentence_<i>.pt`\n","  - Mỗi file `.pt` là **dict** có 13 khoá:\n","    - `layer_0, layer_1, ..., layer_12` → Tensor `[num_tokens, 768]` (biểu diễn của từng token ở lớp tương ứng)."],"metadata":{"id":"Wb2CWCS0PP3X"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. **Tải mô hình & tokenizer pre-trained**\n","     - Model: `AutoModel.from_pretrained(PRETRAINED_MODEL_NAME)` (không có head phân loại).\n","     - Tokenizer: `AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`.\n","  2. **Đọc dữ liệu** theo từng file động từ trong thư mục `Train/`.\n","  3. **Lặp từng câu**:\n","     - Tokenize câu → forward model với `output_hidden_states=True`.\n","     - Lấy `outputs.hidden_states` (tuple 13 lớp).\n","     - Tạo dict `{f'layer_{i}': layer_tensor.squeeze(0)}`.\n","     - Lưu `sentence_<i>.pt` vào thư mục của động từ tương ứng.\n","  4. **Lặp tới hết** tất cả JSON / tất cả câu."],"metadata":{"id":"ct3CaDxMPSR7"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN VÀ TẢI MÔ HÌNH ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu GỐC\n","train_data_dir = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_GramVar/Train'\n","\n","# Đường dẫn đến thư mục MỚI để lưu các vector từ mô hình PRE-TRAINED\n","output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Train/Pretrained/train_vectors_pretrained_gramvar')\n","os.makedirs(output_base_dir, exist_ok=True)\n","\n","# Tải tokenizer và mô hình pre-trained\n","print(\"--- Bắt đầu tải mô hình Pre-trained (BioBERT gốc) ---\")\n","try:\n","    # Tên của mô hình pre-train trên Hugging Face Hub\n","    PRETRAINED_MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.2\"\n","\n","    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n","    # QUAN TRỌNG: Sử dụng AutoModel để tải mô hình gốc, không có lớp phân loại\n","    model_pt = AutoModel.from_pretrained(PRETRAINED_MODEL_NAME)\n","    print(\" -> Tải thành công tokenizer và mô hình pre-trained!\")\n","except Exception as e:\n","    print(f\"LỖI: Không thể tải mô hình từ Hugging Face. Lỗi: {e}\")\n","    exit()\n","\n","# --- 2. HÀM XỬ LÝ CỐT LÕI ---\n","\n","def get_hidden_states_from_pretrained(model, tokenizer, text):\n","    \"\"\"\n","    Trích xuất các trạng thái ẩn (vector biểu diễn) từ mỗi lớp của mô hình pre-trained.\n","    \"\"\"\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    model.eval()\n","    with torch.no_grad():\n","        # Yêu cầu mô hình trả về kết quả từ tất cả các lớp\n","        outputs = model(**inputs, output_hidden_states=True)\n","\n","    # outputs.hidden_states là một tuple chứa các tensor biểu diễn từ mỗi lớp\n","    return outputs.hidden_states\n","\n","# --- 3. THỰC THI CHƯƠNG TRÌNH ---\n","\n","if __name__ == \"__main__\":\n","    print(\"\\n--- Bắt đầu quá trình trích xuất vector từ mô hình Pre-trained ---\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục dữ liệu gốc\n","        json_files = glob.glob(os.path.join(train_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{train_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json gốc\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_train_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang xử lý động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Tạo thư mục con tương ứng trong thư mục output\n","            output_verb_dir = os.path.join(output_base_dir, verb_name)\n","            os.makedirs(output_verb_dir, exist_ok=True)\n","\n","            # Đọc dữ liệu từ file JSON gốc\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Lặp qua từng câu\n","            for i, sentence_text in enumerate(sentences):\n","                # Trích xuất vector từ mô hình pre-trained\n","                hidden_states = get_hidden_states_from_pretrained(model_pt, tokenizer, sentence_text)\n","\n","                # Chuẩn bị dữ liệu để lưu\n","                vectors_to_save = {}\n","                for j, layer_rep in enumerate(hidden_states):\n","                    vectors_to_save[f\"layer_{j}\"] = layer_rep.squeeze(0)\n","\n","                # Xây dựng đường dẫn lưu file mới\n","                file_name = f\"sentence_{i}.pt\"\n","                output_path = os.path.join(output_verb_dir, file_name)\n","\n","                # Lưu file\n","                torch.save(vectors_to_save, output_path)\n","\n","            print(f\"  -> Đã trích xuất và lưu {len(sentences)} file vector cho động từ '{verb_name}'.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TRÍCH XUẤT VECTOR PRE-TRAINED ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"id":"KDNq9KizPPaq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Lưu ý**\n","  - Dùng `AutoModel` (không phải `AutoModelForTokenClassification`) để đảm bảo trích xuất **biểu diễn gốc** của BioBERT.\n","  - `num_tokens` tuỳ vào cách tokenizer tách từ & đặc biệt có token `[CLS]`, `[SEP]`.\n","  - Nếu chạy trên CPU sẽ chậm; có thể thêm `.to('cuda')` cho model & input nếu có GPU.\n","  - Thư mục đích `train_vectors_pretrained_gramvar` sẽ được tạo tự động."],"metadata":{"id":"ujVq6ZM5Palj"}},{"cell_type":"markdown","source":["## Tạo file index cho từng động từ (GramVar)"],"metadata":{"id":"46pfG-aCPyV_"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Đọc các file JSON *train* của **GramVar** (theo từng động từ) để lấy **câu gốc**.\n","  - Ghép mỗi câu với **đường dẫn vector `.pt`** đã trích từ mô hình BioBERT *pre-trained*.\n","  - Lưu thành **một file index `.json`/động từ** để tra cứu nhanh cho các bước chuẩn hoá, phân tích hoặc huấn luyện."],"metadata":{"id":"Bs_n8N4NP2Pu"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục chứa các file JSON câu gốc (theo động từ):  \n","    `.../Clean_Dataset/Corpus/Split_GramVar/Train/*.json`\n","  - Thư mục chứa vector `.pt` đã trích sẵn:  \n","    `.../train_vectors_pretrained_gramvar/<verb_name>/sentence_<i>.pt`"],"metadata":{"id":"z9AkIEJOP5Ls"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Thư mục index:  \n","    `.../verb_indexes_pretrained_gramvar_train/`\n","  - Mỗi động từ có **1 file** `<verb_name>.json` là **list** các entry:\n","    ```json\n","    [\n","        {\n","            \"sentence_text\": \"<câu gốc>\",\n","            \"verb_name\": \"<verb_name>\",\n","            \"vector_file_path\": \".../train_vectors_pretrained_gramvar/<verb_name>/sentence_<i>.pt\"\n","        },\n","        ...\n","    ]\n","    ```"],"metadata":{"id":"fMsYr8DvQFXC"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. Quét tất cả các file `.json` trong thư mục `Split_GramVar/Train/`.\n","  2. Suy ra `verb_name` từ tên file (ví dụ: `bind_1_train_set.json` → `bind_1`).\n","  3. Đọc danh sách các câu (`text`) trong file JSON.\n","  4. Với từng câu **i**:\n","     - Ghép đường dẫn tương ứng đến file vector `.pt` đã tạo.\n","     - Tạo entry gồm `sentence_text`, `verb_name`, `vector_file_path`.\n","  5. Ghi toàn bộ danh sách entry ra file index `<verb_name>.json` trong thư mục output."],"metadata":{"id":"4TW9qxj0QfaI"}},{"cell_type":"code","source":["# Import các thư viện cần thiết\n","import os\n","import json\n","import glob\n","\n","# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu GỐC (nơi lấy câu)\n","train_data_dir = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_GramVar/Train'\n","\n","# Đường dẫn đến thư mục chứa các file vector .pt ĐÃ ĐƯỢC TẠO từ mô hình pre-trained\n","vectors_base_dir = os.path.join(drive_base_path, 'Representation Vector/Train/Pretrained/train_vectors_pretrained_gramvar')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các file index\n","index_output_dir = os.path.join(drive_base_path, 'Representation Vector/Train_Index/Pretrained/verb_indexes_pretrained_gramvar_train')\n","os.makedirs(index_output_dir, exist_ok=True)\n","\n","\n","# --- 2. THỰC THI CHƯƠNG TRÌNH TẠO INDEX ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"--- Bắt đầu quá trình tạo index cho các vector Pre-trained ---\")\n","    print(f\"Đọc dữ liệu gốc từ: {train_data_dir}\")\n","    print(f\"Lưu file index vào: {index_output_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục dữ liệu gốc\n","        json_files = glob.glob(os.path.join(train_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{train_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json gốc\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_train_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang tạo index cho động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Đọc dữ liệu từ file JSON gốc\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Chuẩn bị một danh sách để chứa các mục index cho động từ này\n","            verb_index_list = []\n","\n","            # Lặp qua từng câu để tạo mục index\n","            for i, sentence_text in enumerate(sentences):\n","                # Xây dựng đường dẫn đến file vector .pt tương ứng\n","                vector_file_name = f\"sentence_{i}.pt\"\n","                vector_file_path = os.path.join(vectors_base_dir, verb_name, vector_file_name)\n","\n","                # ***THAY ĐỔI: Thêm \"verb_name\" vào mục index***\n","                index_entry = {\n","                    \"sentence_text\": sentence_text,\n","                    \"verb_name\": verb_name,\n","                    \"vector_file_path\": vector_file_path\n","                }\n","\n","                # Thêm mục index vào danh sách\n","                verb_index_list.append(index_entry)\n","\n","            # Sau khi xử lý hết các câu, lưu danh sách index vào file .json riêng\n","            output_index_path = os.path.join(index_output_dir, f\"{verb_name}.json\")\n","            with open(output_index_path, 'w', encoding='utf-8') as f:\n","                json.dump(verb_index_list, f, indent=4, ensure_ascii=False)\n","\n","            print(f\"  -> Đã tạo và lưu file index '{verb_name}.json' với {len(verb_index_list)} mục.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TẠO INDEX ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"id":"-aZ8Nr-_Pa5z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Cấu trúc thư mục đầu ra kỳ vọng**\n","```text\n",".../train_vectors_pretrained_gramvar/\n","├── begin_1/\n","│   ├── sentence_0.pt\n","│   ├── sentence_1.pt\n","│   └── ...\n","├── bind_2/\n","│   ├── sentence_0.pt\n","│   └── ...\n","└── ...\n","\n",".../verb_indexes_pretrained_gramvar_train/\n","├── begin_1.json\n","├── bind_2.json\n","└── ..."],"metadata":{"id":"9uigH4h_QiC9"}},{"cell_type":"markdown","source":["## Trích xuất hidden states theo từng động từ (ParaVE) và lưu thành file `.pt`"],"metadata":{"id":"fp1XWMiycVuo"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Lấy biểu diễn ẩn (*hidden states*) của **mọi lớp** trong mô hình BioBERT gốc (`dmis-lab/biobert-base-cased-v1.2`) cho **từng câu** trong tập train của **ParaVE**.  \n","  - Lưu lại dưới dạng file `.pt` theo **từng động từ** để phục vụ các bước:\n","    - Tính **mean/std normalization**,  \n","    - So sánh với biểu diễn từ mô hình fine-tuned,  \n","    - Huấn luyện mô hình downstream (ví dụ: SRL, argument classification,...)."],"metadata":{"id":"fzGQ3FdLcZg3"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục JSON chứa dữ liệu train tách theo động từ:  \n","    `.../Clean_Dataset/Corpus/Split_ParaVE/Train/*.json`\n","  - Mỗi file `.json` là danh sách các đối tượng có khóa `text` — chứa câu cần xử lý."],"metadata":{"id":"abSVwyeMcbYl"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Cấu trúc thư mục vector được tạo tự động:  \n","    `.../train_vectors_pretrained_parave/<verb_name>/sentence_<i>.pt`\n","  - Mỗi file `.pt` là **dictionary 13 khóa**:\n","    - `layer_0, layer_1, ..., layer_12` → `Tensor[num_tokens, 768]`  \n","      (biểu diễn của từng token ở mỗi lớp trong BioBERT)."],"metadata":{"id":"eaxoCSe4cdS7"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. **Tải mô hình & tokenizer pre-trained**\n","     - Model:  \n","       `AutoModel.from_pretrained(PRETRAINED_MODEL_NAME)` (không có classification head).  \n","     - Tokenizer:  \n","       `AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\n","  2. **Đọc dữ liệu** từ từng file JSON trong thư mục `Train/`.\n","  3. **Lặp từng câu:**\n","     - Tokenize câu → forward mô hình với `output_hidden_states=True`.  \n","     - Nhận `outputs.hidden_states` (tuple gồm 13 lớp).  \n","     - Tạo dict: `{f'layer_{i}': layer_tensor.squeeze(0)}`.  \n","     - Lưu `sentence_<i>.pt` trong thư mục riêng của từng động từ.\n","  4. **Lặp cho toàn bộ động từ** đến khi hoàn tất."],"metadata":{"id":"ZVDYUTgocd6-"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN VÀ TẢI MÔ HÌNH ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu GỐC\n","train_data_dir = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_ParaVE/Train'\n","\n","# Đường dẫn đến thư mục MỚI để lưu các vector từ mô hình PRE-TRAINED\n","output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Train/Pretrained/train_vectors_pretrained_parave')\n","os.makedirs(output_base_dir, exist_ok=True)\n","\n","# Tải tokenizer và mô hình pre-trained\n","print(\"--- Bắt đầu tải mô hình Pre-trained (BioBERT gốc) ---\")\n","try:\n","    # Tên của mô hình pre-train trên Hugging Face Hub\n","    PRETRAINED_MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.2\"\n","\n","    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n","    # QUAN TRỌNG: Sử dụng AutoModel để tải mô hình gốc, không có lớp phân loại\n","    model_pt = AutoModel.from_pretrained(PRETRAINED_MODEL_NAME)\n","    print(\" -> Tải thành công tokenizer và mô hình pre-trained!\")\n","except Exception as e:\n","    print(f\"LỖI: Không thể tải mô hình từ Hugging Face. Lỗi: {e}\")\n","    exit()\n","\n","# --- 2. HÀM XỬ LÝ CỐT LÕI ---\n","\n","def get_hidden_states_from_pretrained(model, tokenizer, text):\n","    \"\"\"\n","    Trích xuất các trạng thái ẩn (vector biểu diễn) từ mỗi lớp của mô hình pre-trained.\n","    \"\"\"\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    model.eval()\n","    with torch.no_grad():\n","        # Yêu cầu mô hình trả về kết quả từ tất cả các lớp\n","        outputs = model(**inputs, output_hidden_states=True)\n","\n","    # outputs.hidden_states là một tuple chứa các tensor biểu diễn từ mỗi lớp\n","    return outputs.hidden_states\n","\n","# --- 3. THỰC THI CHƯƠNG TRÌNH ---\n","\n","if __name__ == \"__main__\":\n","    print(\"\\n--- Bắt đầu quá trình trích xuất vector từ mô hình Pre-trained ---\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục dữ liệu gốc\n","        json_files = glob.glob(os.path.join(train_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{train_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json gốc\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_train_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang xử lý động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Tạo thư mục con tương ứng trong thư mục output\n","            output_verb_dir = os.path.join(output_base_dir, verb_name)\n","            os.makedirs(output_verb_dir, exist_ok=True)\n","\n","            # Đọc dữ liệu từ file JSON gốc\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Lặp qua từng câu\n","            for i, sentence_text in enumerate(sentences):\n","                # Trích xuất vector từ mô hình pre-trained\n","                hidden_states = get_hidden_states_from_pretrained(model_pt, tokenizer, sentence_text)\n","\n","                # Chuẩn bị dữ liệu để lưu\n","                vectors_to_save = {}\n","                for j, layer_rep in enumerate(hidden_states):\n","                    vectors_to_save[f\"layer_{j}\"] = layer_rep.squeeze(0)\n","\n","                # Xây dựng đường dẫn lưu file mới\n","                file_name = f\"sentence_{i}.pt\"\n","                output_path = os.path.join(output_verb_dir, file_name)\n","\n","                # Lưu file\n","                torch.save(vectors_to_save, output_path)\n","\n","            print(f\"  -> Đã trích xuất và lưu {len(sentences)} file vector cho động từ '{verb_name}'.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TRÍCH XUẤT VECTOR PRE-TRAINED ĐÃ HOÀN TẤT! ---\")"],"metadata":{"id":"05PPZ_27cXDt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Lưu ý**\n","  - Dùng `AutoModel` thay vì `AutoModelForTokenClassification` để trích xuất biểu diễn **thuần gốc** của BioBERT.  \n","  - Mỗi tensor có `num_tokens` phụ thuộc vào tokenizer (bao gồm `[CLS]`, `[SEP]`).  \n","  - Nếu có GPU, nên thêm:  \n","    ```python\n","    model.to('cuda')\n","    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n","    ```  \n","  - Thư mục đầu ra `train_vectors_pretrained_parave` sẽ được tự động tạo."],"metadata":{"id":"2JR-VnRVciw6"}},{"cell_type":"markdown","source":["- **Kết quả**\n","  - Cấu trúc đầu ra:\n","    ```\n","    .../train_vectors_pretrained_parave/\n","    ├── block_1/\n","    │   ├── sentence_0.pt\n","    │   ├── sentence_1.pt\n","    │   └── ...\n","    ├── affect_2/\n","    │   ├── sentence_0.pt\n","    │   └── ...\n","    └── ...\n","    \n","  - Mỗi file `.pt` là **dict** gồm 13 lớp:\n","    ```python\n","    {\n","        'layer_0': tensor([...]),  # [num_tokens, 768]\n","        ...\n","        'layer_12': tensor([...])\n","    }\n","    ```\n","  - Có thể kiểm tra nhanh:\n","    ```python\n","    data = torch.load(\"train_vectors_pretrained_parave/block_1/sentence_0.pt\")\n","    print(data.keys())            # dict_keys(['layer_0', ..., 'layer_12'])\n","    print(data['layer_12'].shape) # torch.Size([num_tokens, 768])\n","    print(data['layer_12'][0][:10])  # 10 giá trị đầu của token [CLS]\n","    ```"],"metadata":{"id":"qhPL68dPcl81"}},{"cell_type":"markdown","source":["## Tạo file index cho từng động từ (ParaVE)"],"metadata":{"id":"c0Lq1RdXcyW5"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Đọc các file JSON *train* của ParaVE (theo từng động từ) để lấy **câu gốc**.\n","  - Ghép mỗi câu với **đường dẫn vector `.pt`** đã trích từ BioBERT *pre-trained*.\n","  - Lưu thành **một file index `.json`/động từ** để tra cứu nhanh ở các bước sau (chuẩn hoá, phân tích, huấn luyện…)."],"metadata":{"id":"vPiDykv7c1jk"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục câu gốc (theo động từ):  \n","    `.../Clean_Dataset/Corpus/Split_ParaVE/Train/*.json`\n","  - Thư mục chứa vector đã trích sẵn (pre-trained):  \n","    `.../train_vectors_pretrained_parave/<verb_name>/sentence_<i>.pt`"],"metadata":{"id":"IciLMgRBc35X"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Thư mục index:  \n","    `.../verb_indexes_pretrained_parave_train/`\n","  - Mỗi động từ có **1 file**: `<verb_name>.json`, là một **list** các entry:\n","    ```json\n","    [\n","      {\n","        \"sentence_text\": \"<câu gốc>\",\n","        \"verb_name\": \"<verb_name>\",\n","        \"vector_file_path\": \".../train_vectors_pretrained_parave/<verb_name>/sentence_<i>.pt\"\n","      },\n","      ...\n","    ]\n","    ```"],"metadata":{"id":"mDqP4tocc4vS"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. Duyệt tất cả `*.json` trong `Split_ParaVE/Train/`.\n","  2. Suy ra `verb_name` từ tên file (ví dụ: `begin_1_train_set.json` → `begin_1`).\n","  3. Đọc list câu `text`.\n","  4. Với từng câu **i**:\n","     - Ghép đường dẫn vector: `train_vectors_pretrained_parave/<verb_name>/sentence_i.pt`.\n","     - Tạo entry chứa `sentence_text`, `verb_name`, `vector_file_path`.\n","  5. Ghi **list** entry ra file `verb_indexes_pretrained_parave_train/<verb_name>.json`.\n"],"metadata":{"id":"TNhjna9Sc70C"}},{"cell_type":"code","source":["# Import các thư viện cần thiết\n","import os\n","import json\n","import glob\n","\n","# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu GỐC (nơi lấy câu)\n","train_data_dir = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_ParaVE/Train'\n","\n","# Đường dẫn đến thư mục chứa các file vector .pt ĐÃ ĐƯỢC TẠO từ mô hình pre-trained\n","vectors_base_dir = os.path.join(drive_base_path, 'Representation Vector/Train/Pretrained/train_vectors_pretrained_parave')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các file index\n","index_output_dir = os.path.join(drive_base_path, 'Representation Vector/Train_Index/Pretrained/verb_indexes_pretrained_parave_train')\n","os.makedirs(index_output_dir, exist_ok=True)\n","\n","\n","# --- 2. THỰC THI CHƯƠNG TRÌNH TẠO INDEX ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"--- Bắt đầu quá trình tạo index cho các vector Pre-trained ---\")\n","    print(f\"Đọc dữ liệu gốc từ: {train_data_dir}\")\n","    print(f\"Lưu file index vào: {index_output_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục dữ liệu gốc\n","        json_files = glob.glob(os.path.join(train_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{train_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json gốc\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_train_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang tạo index cho động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Đọc dữ liệu từ file JSON gốc\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Chuẩn bị một danh sách để chứa các mục index cho động từ này\n","            verb_index_list = []\n","\n","            # Lặp qua từng câu để tạo mục index\n","            for i, sentence_text in enumerate(sentences):\n","                # Xây dựng đường dẫn đến file vector .pt tương ứng\n","                vector_file_name = f\"sentence_{i}.pt\"\n","                vector_file_path = os.path.join(vectors_base_dir, verb_name, vector_file_name)\n","\n","                # ***THAY ĐỔI: Thêm \"verb_name\" vào mục index***\n","                index_entry = {\n","                    \"sentence_text\": sentence_text,\n","                    \"verb_name\": verb_name,\n","                    \"vector_file_path\": vector_file_path\n","                }\n","\n","                # Thêm mục index vào danh sách\n","                verb_index_list.append(index_entry)\n","\n","            # Sau khi xử lý hết các câu, lưu danh sách index vào file .json riêng\n","            output_index_path = os.path.join(index_output_dir, f\"{verb_name}.json\")\n","            with open(output_index_path, 'w', encoding='utf-8') as f:\n","                json.dump(verb_index_list, f, indent=4, ensure_ascii=False)\n","\n","            print(f\"  -> Đã tạo và lưu file index '{verb_name}.json' với {len(verb_index_list)} mục.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TẠO INDEX ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"id":"BkilCtCjczxv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test Dataset - Mô hình Fine-tuned"],"metadata":{"id":"dUIAzed5wLhh"}},{"cell_type":"markdown","source":["## Trích xuất hidden states theo từng động từ (GramVar) và lưu thành file `.pt`"],"metadata":{"id":"Bck9yJjmwyyI"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Lấy biểu diễn ẩn (hidden states) của **mọi lớp (13 lớp)** từ mô hình **BioBERT đã fine-tuned** (`biobert-srl-best-model`) cho **từng câu trong tập TEST**.\n","  - Lưu các vector `.pt` để phục vụ đánh giá và so sánh với các tập train/validation."],"metadata":{"id":"QYwpP4aQwz8L"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục dữ liệu test (chia theo động từ):  \n","    `.../Clean_Dataset/Corpus/Split_GramVar/Test/*.json`\n","  - Mỗi file JSON chứa danh sách các item có khoá `\"text\"`."],"metadata":{"id":"hpOWMHiBw2Yx"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Cấu trúc thư mục lưu vector:\n","    ```\n","    .../test_vectors_finetuned_gramvar/<verb_name>/sentence_<i>.pt\n","    ```\n","  - Mỗi file `.pt` là **dict** gồm 13 khoá:\n","    ```\n","    {\n","      'layer_0': Tensor[num_tokens, 768],\n","      'layer_1': Tensor[num_tokens, 768],\n","      ...\n","      'layer_12': Tensor[num_tokens, 768]\n","    }\n","    ```"],"metadata":{"id":"u9HD3gB3w3wa"}},{"cell_type":"markdown","source":["- **Quy trình thực hiện**\n","  1. **Tải mô hình và tokenizer**\n","     - Tokenizer: `AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")`\n","     - Model: `AutoModelForTokenClassification.from_pretrained(final_model_path)`  \n","       *(đây là model đã fine-tuned cho Semantic Role Labeling - SRL)*\n","\n","  2. **Đọc dữ liệu TEST**\n","     - Mỗi file `.json` tương ứng một **động từ** (ví dụ `abolish_test_set.json` → `abolish`).\n","     - Lặp qua tất cả câu trong file, lấy nội dung `text`.\n","\n","  3. **Trích xuất biểu diễn ẩn**\n","     - Tokenize từng câu → forward model với `output_hidden_states=True`.\n","     - Lấy `outputs.hidden_states` (tuple gồm 13 lớp).\n","     - Lưu vào dict `{f'layer_{i}': layer_tensor.squeeze(0)}`.\n","\n","  4. **Lưu vector ra file `.pt`**\n","     - Lưu riêng từng câu, đặt tên `sentence_<i>.pt` trong thư mục theo tên động từ.\n"],"metadata":{"id":"KuGzQ0Bgw6cQ"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN VÀ TẢI MÔ HÌNH ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa mô hình fine-tuned\n","final_model_path = os.path.join(drive_base_path, 'Finetuned_Models/biobert-srl-best-model')\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu TEST\n","test_data_dir = os.path.join(drive_base_path, 'Clean_Dataset/Corpus/Split_GramVar/Test')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các vector của tập TEST\n","output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Test/Finetuned/test_vectors_finetuned_gramvar')\n","os.makedirs(output_base_dir, exist_ok=True)\n","\n","# Tải tokenizer và mô hình fine-tuned\n","print(\"--- Bắt đầu tải mô hình Fine-tuned ---\")\n","try:\n","    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n","    model_ft = AutoModelForTokenClassification.from_pretrained(final_model_path)\n","    print(\" -> Tải thành công tokenizer và mô hình fine-tuned!\")\n","except Exception as e:\n","    print(f\"LỖI: Không thể tải mô hình. Lỗi: {e}\")\n","    exit()\n","\n","# --- 2. HÀM XỬ LÝ CỐT LÕI ---\n","\n","def get_hidden_states(model, tokenizer, text):\n","    \"\"\"\n","    Trích xuất các trạng thái ẩn (vector biểu diễn) từ mỗi lớp của mô hình.\n","    \"\"\"\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","    return outputs.hidden_states\n","\n","# --- 3. THỰC THI CHƯƠNG TRÌNH ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"\\n--- Bắt đầu quá trình trích xuất vector cho toàn bộ tập TEST ---\")\n","    print(f\"Đọc dữ liệu từ: {test_data_dir}\")\n","    print(f\"Lưu kết quả vào: {output_base_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục test\n","        json_files = glob.glob(os.path.join(test_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{test_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json (mỗi file cho một động từ)\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file (ví dụ: 'abolish_test_set.json' -> 'abolish')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_test_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang xử lý động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Tạo thư mục con tương ứng trong thư mục output\n","            output_verb_dir = os.path.join(output_base_dir, verb_name)\n","            os.makedirs(output_verb_dir, exist_ok=True)\n","\n","            # Đọc dữ liệu từ file JSON\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Lặp qua từng câu\n","            for i, sentence_text in enumerate(sentences):\n","                # Trích xuất vector từ mô hình fine-tuned\n","                hidden_states = get_hidden_states(model_ft, tokenizer, sentence_text)\n","\n","                # Chuẩn bị dữ liệu để lưu\n","                vectors_to_save = {}\n","                for j, layer_rep in enumerate(hidden_states):\n","                    vectors_to_save[f\"layer_{j}\"] = layer_rep.squeeze(0)\n","\n","                # Xây dựng đường dẫn lưu file mới\n","                file_name = f\"sentence_{i}.pt\"\n","                output_path = os.path.join(output_verb_dir, file_name)\n","\n","                # Lưu file\n","                torch.save(vectors_to_save, output_path)\n","\n","            print(f\"  -> Đã trích xuất và lưu {len(sentences)} file vector cho động từ '{verb_name}'.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TRÍCH XUẤT VECTOR TẬP TEST ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["991fc431045c4cc489d0c4966d63bca3","55b6a08653c14292b1238be5d679b6e4","5ce9605bc6c24c6a82525c633fc26709","58bd006313074d47abccc302c71e1dc6","4c4c4dfd92bc471b9bd7504feef0c9a5","40e9593937884673a54902f57b9f2659","7b065be7b31a41e3b5689898df674dbd","c15b3ae1ca764556bc8f46c6b78c56c7","167856a43f3d42c08e9576af2d12c531","48e3f4bcbb3f4e73a8aa28479c4efd4c","edf96196539b4c9db3fab8e98dc8bf10","52cdecca0c7c4a1ab7c7aa09193c3dc1","83e64955c6c6414889f108862ba957c7","08821349ef1642ac87f8763acadffd15","b86881fe84c3493989ed5d38cf7f4dde","4a89f694502b4b7083b1c5b5696be79f","5814943f99f746528571fc0dc4da7fa9","86d238ce222744cbbfb5f447e32857f9","bf376927477c447dbf9f63add7a24dae","ce6fd865bce04fd4b44ab36eeffa65eb","6da69a1537f14043ab92e7744511c543","f2980825b3934ece90a7e8d17cc7a206"]},"collapsed":true,"id":"3NXgB94nvkrN","executionInfo":{"status":"ok","timestamp":1759992974305,"user_tz":-420,"elapsed":210044,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"d0750e58-67bb-48b1-c697-92814a72d702"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu tải mô hình Fine-tuned ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"991fc431045c4cc489d0c4966d63bca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52cdecca0c7c4a1ab7c7aa09193c3dc1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" -> Tải thành công tokenizer và mô hình fine-tuned!\n","\n","--- Bắt đầu quá trình trích xuất vector cho toàn bộ tập TEST ---\n","Đọc dữ liệu từ: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_GramVar/Test\n","Lưu kết quả vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/test_vectors_finetuned_gramvar\n","\n","Đang xử lý động từ 1/35: begin_2\n","  -> Đã trích xuất và lưu 28 file vector cho động từ 'begin_2'.\n","\n","Đang xử lý động từ 2/35: begin_1\n","  -> Đã trích xuất và lưu 36 file vector cho động từ 'begin_1'.\n","\n","Đang xử lý động từ 3/35: translate_2\n","  -> Đã trích xuất và lưu 12 file vector cho động từ 'translate_2'.\n","\n","Đang xử lý động từ 4/35: transform_1\n","  -> Đã trích xuất và lưu 24 file vector cho động từ 'transform_1'.\n","\n","Đang xử lý động từ 5/35: delete\n","  -> Đã trích xuất và lưu 8 file vector cho động từ 'delete'.\n","\n","Đang xử lý động từ 6/35: confer\n","  -> Đã trích xuất và lưu 39 file vector cho động từ 'confer'.\n","\n","Đang xử lý động từ 7/35: splice_2\n","  -> Đã trích xuất và lưu 51 file vector cho động từ 'splice_2'.\n","\n","Đang xử lý động từ 8/35: translate_1\n","  -> Đã trích xuất và lưu 44 file vector cho động từ 'translate_1'.\n","\n","Đang xử lý động từ 9/35: inhibit\n","  -> Đã trích xuất và lưu 21 file vector cho động từ 'inhibit'.\n","\n","Đang xử lý động từ 10/35: proliferate\n","  -> Đã trích xuất và lưu 14 file vector cho động từ 'proliferate'.\n","\n","Đang xử lý động từ 11/35: result\n","  -> Đã trích xuất và lưu 36 file vector cho động từ 'result'.\n","\n","Đang xử lý động từ 12/35: lead\n","  -> Đã trích xuất và lưu 10 file vector cho động từ 'lead'.\n","\n","Đang xử lý động từ 13/35: modify\n","  -> Đã trích xuất và lưu 42 file vector cho động từ 'modify'.\n","\n","Đang xử lý động từ 14/35: decrease_2\n","  -> Đã trích xuất và lưu 18 file vector cho động từ 'decrease_2'.\n","\n","Đang xử lý động từ 15/35: splice_1\n","  -> Đã trích xuất và lưu 52 file vector cho động từ 'splice_1'.\n","\n","Đang xử lý động từ 16/35: lose\n","  -> Đã trích xuất và lưu 28 file vector cho động từ 'lose'.\n","\n","Đang xử lý động từ 17/35: transform_2\n","  -> Đã trích xuất và lưu 44 file vector cho động từ 'transform_2'.\n","\n","Đang xử lý động từ 18/35: eliminate\n","  -> Đã trích xuất và lưu 12 file vector cho động từ 'eliminate'.\n","\n","Đang xử lý động từ 19/35: encode\n","  -> Đã trích xuất và lưu 10 file vector cho động từ 'encode'.\n","\n","Đang xử lý động từ 20/35: block\n","  -> Đã trích xuất và lưu 26 file vector cho động từ 'block'.\n","\n","Đang xử lý động từ 21/35: develop\n","  -> Đã trích xuất và lưu 14 file vector cho động từ 'develop'.\n","\n","Đang xử lý động từ 22/35: decrease_1\n","  -> Đã trích xuất và lưu 36 file vector cho động từ 'decrease_1'.\n","\n","Đang xử lý động từ 23/35: recognize\n","  -> Đã trích xuất và lưu 10 file vector cho động từ 'recognize'.\n","\n","Đang xử lý động từ 24/35: abolish\n","  -> Đã trích xuất và lưu 9 file vector cho động từ 'abolish'.\n","\n","Đang xử lý động từ 25/35: translate_3\n","  -> Đã trích xuất và lưu 1 file vector cho động từ 'translate_3'.\n","\n","Đang xử lý động từ 26/35: express\n","  -> Đã trích xuất và lưu 15 file vector cho động từ 'express'.\n","\n","Đang xử lý động từ 27/35: alter\n","  -> Đã trích xuất và lưu 31 file vector cho động từ 'alter'.\n","\n","Đang xử lý động từ 28/35: catalyse\n","  -> Đã trích xuất và lưu 17 file vector cho động từ 'catalyse'.\n","\n","Đang xử lý động từ 29/35: generate\n","  -> Đã trích xuất và lưu 11 file vector cho động từ 'generate'.\n","\n","Đang xử lý động từ 30/35: initiate\n","  -> Đã trích xuất và lưu 20 file vector cho động từ 'initiate'.\n","\n","Đang xử lý động từ 31/35: skip\n","  -> Đã trích xuất và lưu 18 file vector cho động từ 'skip'.\n","\n","Đang xử lý động từ 32/35: mutate\n","  -> Đã trích xuất và lưu 43 file vector cho động từ 'mutate'.\n","\n","Đang xử lý động từ 33/35: truncate\n","  -> Đã trích xuất và lưu 47 file vector cho động từ 'truncate'.\n","\n","Đang xử lý động từ 34/35: transcribe\n","  -> Đã trích xuất và lưu 19 file vector cho động từ 'transcribe'.\n","\n","Đang xử lý động từ 35/35: disrupt\n","  -> Đã trích xuất và lưu 8 file vector cho động từ 'disrupt'.\n","\n","--- QUÁ TRÌNH TRÍCH XUẤT VECTOR TẬP TEST ĐÃ HOÀN TẤT! ---\n"]}]},{"cell_type":"markdown","source":["- **Cấu trúc thư mục mong đợi**\n","  ```\n","    .../test_vectors_finetuned_gramvar/\n","    ├── abolish/\n","    │   ├── sentence_0.pt\n","    │   ├── sentence_1.pt\n","    │   └── ...\n","    ├── bind/\n","    │   ├── sentence_0.pt\n","    │   ├── sentence_1.pt\n","    │   └── ...\n","    └── ...\n","  ```"],"metadata":{"id":"Bgej5fP1xItl"}},{"cell_type":"markdown","source":["- **Lưu ý**\n","  - Mỗi lớp (`layer_k`) có kích thước `[num_tokens, 768]`.\n","  - Bao gồm các token đặc biệt `[CLS]` và `[SEP]`.\n","  - Nếu dùng GPU, thêm `.to(\"cuda\")` cho mô hình và input để tăng tốc.\n","  - Mỗi file `.pt` có thể được nạp lại bằng `torch.load(path)` cho các bước phân tích sau."],"metadata":{"id":"lAUqObugxrnG"}},{"cell_type":"markdown","source":["## Tạo file index cho từng động từ (GramVar)"],"metadata":{"id":"3YvKwoHKkuVE"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Tạo **bộ chỉ mục (index)** liên kết giữa:\n","    - **Câu gốc** (`sentence_text`)\n","    - **Tên động từ** (`verb_name`)\n","    - **Đường dẫn đầy đủ tới file vector `.pt`**  \n","      (đã được trích xuất từ mô hình **BioBERT Fine-tuned**).\n","  - Giúp các bước sau (như chuẩn hoá, thống kê, hoặc đánh giá) **dễ dàng truy cập** vector tương ứng cho từng câu.\n"],"metadata":{"id":"NQ-hpFkzlWLq"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Các file `.json` trong thư mục:\n","    ```\n","    .../Clean_Dataset/Corpus/Split_GramVar/Test/*.json\n","    ```\n","  - Mỗi file chứa danh sách các câu có khoá `\"text\"`.\n","\n","  - Thư mục vector tương ứng (đã được tạo từ mô hình fine-tuned):\n","    ```\n","    .../test_vectors_finetuned_gramvar/<verb_name>/sentence_<i>.pt\n","    ```"],"metadata":{"id":"Eq3uh2ZolYPK"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Các file index `.json` trong:\n","    ```\n","    .../verb_indexes_finetuned_gramvar_test/\n","    ```\n","  - Cấu trúc mỗi file:\n","    ```json\n","    [\n","        {\n","            \"sentence_text\": \"Protein A binds with Protein B.\",\n","            \"verb_name\": \"bind\",\n","            \"vector_file_path\": \"/content/.../test_vectors_finetuned_gramvar/bind/sentence_0.pt\"\n","        },\n","        {\n","            \"sentence_text\": \"DNA interacts with RNA polymerase.\",\n","            \"verb_name\": \"interact\",\n","            \"vector_file_path\": \"/content/.../test_vectors_finetuned_gramvar/interact/sentence_1.pt\"\n","        }\n","    ]\n","    ```"],"metadata":{"id":"cmAAsUiplY_f"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. **Đọc toàn bộ file .json** trong thư mục TEST.\n","  2. **Trích xuất danh sách câu** từ khoá `\"text\"`.\n","  3. **Sinh đường dẫn vector tương ứng** theo mẫu:\n","     ```\n","     <vectors_base_dir>/<verb_name>/sentence_<i>.pt\n","     ```\n","  4. **Tạo danh sách index** gồm 3 trường:\n","     - `\"sentence_text\"`  \n","     - `\"verb_name\"`  \n","     - `\"vector_file_path\"`\n","  5. **Lưu danh sách vào file `<verb_name>.json`** trong thư mục index đầu ra."],"metadata":{"id":"WsKt4sbHlbGN"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu TEST GỐC (nơi lấy câu)\n","test_data_dir = os.path.join(drive_base_path, 'Clean_Dataset/Corpus/Split_GramVar/Test')\n","\n","# Đường dẫn đến thư mục chứa các file vector .pt ĐÃ ĐƯỢỢC TẠO từ mô hình fine-tuned\n","vectors_base_dir = os.path.join(drive_base_path, 'test_vectors_finetuned_gramvar')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các file index cho tập test\n","index_output_dir = os.path.join(drive_base_path, 'verb_indexes_finetuned_gramvar_test')\n","os.makedirs(index_output_dir, exist_ok=True)\n","\n","\n","# --- 2. THỰC THI CHƯƠNG TRÌNH TẠO INDEX ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"--- Bắt đầu quá trình tạo index cho các vector Test (Fine-tuned) ---\")\n","    print(f\"Đọc dữ liệu gốc từ: {test_data_dir}\")\n","    print(f\"Lưu file index vào: {index_output_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục dữ liệu test\n","        json_files = glob.glob(os.path.join(test_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{test_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json gốc\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file (ví dụ: 'abolish_test_set.json' -> 'abolish')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_test_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang tạo index cho động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Đọc dữ liệu từ file JSON gốc\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Chuẩn bị một danh sách để chứa các mục index cho động từ này\n","            verb_index_list = []\n","\n","            # Lặp qua từng câu để tạo mục index\n","            for i, sentence_text in enumerate(sentences):\n","                # Xây dựng đường dẫn đến file vector .pt tương ứng\n","                vector_file_name = f\"sentence_{i}.pt\"\n","                vector_file_path = os.path.join(vectors_base_dir, verb_name, vector_file_name)\n","\n","                # Tạo một mục index với cấu trúc bạn yêu cầu\n","                index_entry = {\n","                    \"sentence_text\": sentence_text,\n","                    \"verb_name\": verb_name,\n","                    \"vector_file_path\": vector_file_path\n","                }\n","\n","                # Thêm mục index vào danh sách\n","                verb_index_list.append(index_entry)\n","\n","            # Sau khi xử lý hết các câu, lưu danh sách index vào file .json riêng\n","            output_index_path = os.path.join(index_output_dir, f\"{verb_name}.json\")\n","            with open(output_index_path, 'w', encoding='utf-8') as f:\n","                json.dump(verb_index_list, f, indent=4, ensure_ascii=False)\n","\n","            print(f\"  -> Đã tạo và lưu file index '{verb_name}.json' với {len(verb_index_list)} mục.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TẠO INDEX CHO TẬP TEST ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"T5RsUp-ekzeh","executionInfo":{"status":"ok","timestamp":1760006720883,"user_tz":-420,"elapsed":766,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"fe7b3d09-3c09-443f-9f92-6ac3c86eca04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu quá trình tạo index cho các vector Test (Fine-tuned) ---\n","Đọc dữ liệu gốc từ: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_GramVar/Test\n","Lưu file index vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_finetuned_gramvar_test\n","\n","Đang tạo index cho động từ 1/35: begin_2\n","  -> Đã tạo và lưu file index 'begin_2.json' với 28 mục.\n","\n","Đang tạo index cho động từ 2/35: begin_1\n","  -> Đã tạo và lưu file index 'begin_1.json' với 36 mục.\n","\n","Đang tạo index cho động từ 3/35: translate_2\n","  -> Đã tạo và lưu file index 'translate_2.json' với 12 mục.\n","\n","Đang tạo index cho động từ 4/35: transform_1\n","  -> Đã tạo và lưu file index 'transform_1.json' với 24 mục.\n","\n","Đang tạo index cho động từ 5/35: delete\n","  -> Đã tạo và lưu file index 'delete.json' với 8 mục.\n","\n","Đang tạo index cho động từ 6/35: confer\n","  -> Đã tạo và lưu file index 'confer.json' với 39 mục.\n","\n","Đang tạo index cho động từ 7/35: splice_2\n","  -> Đã tạo và lưu file index 'splice_2.json' với 51 mục.\n","\n","Đang tạo index cho động từ 8/35: translate_1\n","  -> Đã tạo và lưu file index 'translate_1.json' với 44 mục.\n","\n","Đang tạo index cho động từ 9/35: inhibit\n","  -> Đã tạo và lưu file index 'inhibit.json' với 21 mục.\n","\n","Đang tạo index cho động từ 10/35: proliferate\n","  -> Đã tạo và lưu file index 'proliferate.json' với 14 mục.\n","\n","Đang tạo index cho động từ 11/35: result\n","  -> Đã tạo và lưu file index 'result.json' với 36 mục.\n","\n","Đang tạo index cho động từ 12/35: lead\n","  -> Đã tạo và lưu file index 'lead.json' với 10 mục.\n","\n","Đang tạo index cho động từ 13/35: modify\n","  -> Đã tạo và lưu file index 'modify.json' với 42 mục.\n","\n","Đang tạo index cho động từ 14/35: decrease_2\n","  -> Đã tạo và lưu file index 'decrease_2.json' với 18 mục.\n","\n","Đang tạo index cho động từ 15/35: splice_1\n","  -> Đã tạo và lưu file index 'splice_1.json' với 52 mục.\n","\n","Đang tạo index cho động từ 16/35: lose\n","  -> Đã tạo và lưu file index 'lose.json' với 28 mục.\n","\n","Đang tạo index cho động từ 17/35: transform_2\n","  -> Đã tạo và lưu file index 'transform_2.json' với 44 mục.\n","\n","Đang tạo index cho động từ 18/35: eliminate\n","  -> Đã tạo và lưu file index 'eliminate.json' với 12 mục.\n","\n","Đang tạo index cho động từ 19/35: encode\n","  -> Đã tạo và lưu file index 'encode.json' với 10 mục.\n","\n","Đang tạo index cho động từ 20/35: block\n","  -> Đã tạo và lưu file index 'block.json' với 26 mục.\n","\n","Đang tạo index cho động từ 21/35: develop\n","  -> Đã tạo và lưu file index 'develop.json' với 14 mục.\n","\n","Đang tạo index cho động từ 22/35: decrease_1\n","  -> Đã tạo và lưu file index 'decrease_1.json' với 36 mục.\n","\n","Đang tạo index cho động từ 23/35: recognize\n","  -> Đã tạo và lưu file index 'recognize.json' với 10 mục.\n","\n","Đang tạo index cho động từ 24/35: abolish\n","  -> Đã tạo và lưu file index 'abolish.json' với 9 mục.\n","\n","Đang tạo index cho động từ 25/35: translate_3\n","  -> Đã tạo và lưu file index 'translate_3.json' với 1 mục.\n","\n","Đang tạo index cho động từ 26/35: express\n","  -> Đã tạo và lưu file index 'express.json' với 15 mục.\n","\n","Đang tạo index cho động từ 27/35: alter\n","  -> Đã tạo và lưu file index 'alter.json' với 31 mục.\n","\n","Đang tạo index cho động từ 28/35: catalyse\n","  -> Đã tạo và lưu file index 'catalyse.json' với 17 mục.\n","\n","Đang tạo index cho động từ 29/35: generate\n","  -> Đã tạo và lưu file index 'generate.json' với 11 mục.\n","\n","Đang tạo index cho động từ 30/35: initiate\n","  -> Đã tạo và lưu file index 'initiate.json' với 20 mục.\n","\n","Đang tạo index cho động từ 31/35: skip\n","  -> Đã tạo và lưu file index 'skip.json' với 18 mục.\n","\n","Đang tạo index cho động từ 32/35: mutate\n","  -> Đã tạo và lưu file index 'mutate.json' với 43 mục.\n","\n","Đang tạo index cho động từ 33/35: truncate\n","  -> Đã tạo và lưu file index 'truncate.json' với 47 mục.\n","\n","Đang tạo index cho động từ 34/35: transcribe\n","  -> Đã tạo và lưu file index 'transcribe.json' với 19 mục.\n","\n","Đang tạo index cho động từ 35/35: disrupt\n","  -> Đã tạo và lưu file index 'disrupt.json' với 8 mục.\n","\n","--- QUÁ TRÌNH TẠO INDEX CHO TẬP TEST ĐÃ HOÀN TẤT! ---\n"]}]},{"cell_type":"markdown","source":["- **Cấu trúc thư mục kết quả**\n","  ```bash\n","  .../verb_indexes_finetuned_gramvar_test/\n","  ├── activate.json\n","  ├── bind.json\n","  ├── block.json\n","  └── ..."],"metadata":{"id":"0P7iT07flgwf"}},{"cell_type":"markdown","source":["## Trích xuất hidden states theo từng động từ (ParaVE) và lưu thành file `.pt`"],"metadata":{"id":"wNhGE5RD61Il"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Trích xuất **biểu diễn ẩn (hidden states)** của tất cả **13 lớp** từ mô hình BioBERT đã **fine-tuned** cho **từng câu trong tập TEST** (ParaVE).\n","  - Lưu kết quả ra file `.pt` riêng cho từng câu, phục vụ các bước đánh giá, phân tích hoặc chuẩn hoá sau này."],"metadata":{"id":"2JGffTRc7ioy"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục chứa dữ liệu test (chia theo động từ):  \n","    `.../Clean_Dataset/Corpus/Split_ParaVE/Test/*.json`\n","  - Mỗi file `.json` là danh sách các item có khoá `\"text\"`:\n","    ```json\n","    [\n","      {\"text\": \"The protein interacts with the gene.\"},\n","      {\"text\": \"They inhibit this pathway strongly.\"},\n","      ...\n","    ]\n","    ```"],"metadata":{"id":"MKXM8r-97mCi"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Cấu trúc thư mục vector:\n","    ```\n","    .../test_vectors_finetuned_parave/<verb_name>/sentence_<i>.pt\n","    ```\n","  - Mỗi file `.pt` là một **dict gồm 13 lớp**:\n","    ```python\n","    {\n","      'layer_0': Tensor[num_tokens, 768],\n","      'layer_1': Tensor[num_tokens, 768],\n","      ...\n","      'layer_12': Tensor[num_tokens, 768]\n","    }\n","    ```\n","    > Mỗi `layer_k` là biểu diễn vector của tất cả token trong câu tại lớp tương ứng.\n"],"metadata":{"id":"uwY_PxOY7osQ"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. **Tải mô hình & tokenizer**\n","     - Tokenizer: `AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")`\n","     - Model: `AutoModelForTokenClassification.from_pretrained(final_model_path)`\n","       - Mô hình `biobert-srl-best-model` đã fine-tuned trên nhiệm vụ **Semantic Role Labeling (SRL)**.\n","  \n","  2. **Đọc dữ liệu**\n","     - Duyệt qua tất cả `.json` trong thư mục TEST.\n","     - Mỗi file ứng với một động từ (ví dụ: `absorb_test_set.json` → động từ `absorb`).\n","  \n","  3. **Trích xuất vector**\n","     - Tokenize từng câu → forward model với `output_hidden_states=True`.\n","     - Lấy tuple `hidden_states` chứa 13 lớp (embedding + 12 transformer layers).\n","     - Lưu từng lớp vào `dict` `{f\"layer_{i}\": tensor.squeeze(0)}`.\n","  \n","  4. **Lưu file vector**\n","     - Ghi ra `.pt` cho từng câu dưới thư mục riêng tương ứng với động từ.\n"],"metadata":{"id":"KGxvsOE87pba"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN VÀ TẢI MÔ HÌNH ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa mô hình fine-tuned\n","final_model_path = os.path.join(drive_base_path, 'Finetuned_Models/biobert-srl-best-model')\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu TEST\n","test_data_dir = os.path.join(drive_base_path, 'Clean_Dataset/Corpus/Split_ParaVE/Test')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các vector của tập TEST\n","output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Test/Finetuned/test_vectors_finetuned_parave')\n","os.makedirs(output_base_dir, exist_ok=True)\n","\n","# Tải tokenizer và mô hình fine-tuned\n","print(\"--- Bắt đầu tải mô hình Fine-tuned ---\")\n","try:\n","    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n","    model_ft = AutoModelForTokenClassification.from_pretrained(final_model_path)\n","    print(\" -> Tải thành công tokenizer và mô hình fine-tuned!\")\n","except Exception as e:\n","    print(f\"LỖI: Không thể tải mô hình. Lỗi: {e}\")\n","    exit()\n","\n","# --- 2. HÀM XỬ LÝ CỐT LÕI ---\n","\n","def get_hidden_states(model, tokenizer, text):\n","    \"\"\"\n","    Trích xuất các trạng thái ẩn (vector biểu diễn) từ mỗi lớp của mô hình.\n","    \"\"\"\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","    return outputs.hidden_states\n","\n","# --- 3. THỰC THI CHƯƠNG TRÌNH ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"\\n--- Bắt đầu quá trình trích xuất vector cho toàn bộ tập TEST ---\")\n","    print(f\"Đọc dữ liệu từ: {test_data_dir}\")\n","    print(f\"Lưu kết quả vào: {output_base_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục test\n","        json_files = glob.glob(os.path.join(test_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{test_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json (mỗi file cho một động từ)\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file (ví dụ: 'abolish_test_set.json' -> 'abolish')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_test_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang xử lý động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Tạo thư mục con tương ứng trong thư mục output\n","            output_verb_dir = os.path.join(output_base_dir, verb_name)\n","            os.makedirs(output_verb_dir, exist_ok=True)\n","\n","            # Đọc dữ liệu từ file JSON\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Lặp qua từng câu\n","            for i, sentence_text in enumerate(sentences):\n","                # Trích xuất vector từ mô hình fine-tuned\n","                hidden_states = get_hidden_states(model_ft, tokenizer, sentence_text)\n","\n","                # Chuẩn bị dữ liệu để lưu\n","                vectors_to_save = {}\n","                for j, layer_rep in enumerate(hidden_states):\n","                    vectors_to_save[f\"layer_{j}\"] = layer_rep.squeeze(0)\n","\n","                # Xây dựng đường dẫn lưu file mới\n","                file_name = f\"sentence_{i}.pt\"\n","                output_path = os.path.join(output_verb_dir, file_name)\n","\n","                # Lưu file\n","                torch.save(vectors_to_save, output_path)\n","\n","            print(f\"  -> Đã trích xuất và lưu {len(sentences)} file vector cho động từ '{verb_name}'.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TRÍCH XUẤT VECTOR TẬP TEST ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"x2V4st3m67QD","executionInfo":{"status":"ok","timestamp":1759995853809,"user_tz":-420,"elapsed":88122,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"bd043de1-7531-4088-9f91-91b4ccc49e45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu tải mô hình Fine-tuned ---\n"," -> Tải thành công tokenizer và mô hình fine-tuned!\n","\n","--- Bắt đầu quá trình trích xuất vector cho toàn bộ tập TEST ---\n","Đọc dữ liệu từ: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_ParaVE/Test\n","Lưu kết quả vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/test_vectors_finetuned_parave\n","\n","Đang xử lý động từ 1/35: catalyse\n","  -> Đã trích xuất và lưu 7 file vector cho động từ 'catalyse'.\n","\n","Đang xử lý động từ 2/35: result\n","  -> Đã trích xuất và lưu 31 file vector cho động từ 'result'.\n","\n","Đang xử lý động từ 3/35: eliminate\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'eliminate'.\n","\n","Đang xử lý động từ 4/35: develop\n","  -> Đã trích xuất và lưu 1 file vector cho động từ 'develop'.\n","\n","Đang xử lý động từ 5/35: translate_3\n","  -> Đã trích xuất và lưu 25 file vector cho động từ 'translate_3'.\n","\n","Đang xử lý động từ 6/35: transform_1\n","  -> Đã trích xuất và lưu 3 file vector cho động từ 'transform_1'.\n","\n","Đang xử lý động từ 7/35: begin_2\n","  -> Đã trích xuất và lưu 19 file vector cho động từ 'begin_2'.\n","\n","Đang xử lý động từ 8/35: block\n","  -> Đã trích xuất và lưu 13 file vector cho động từ 'block'.\n","\n","Đang xử lý động từ 9/35: abolish\n","  -> Đã trích xuất và lưu 11 file vector cho động từ 'abolish'.\n","\n","Đang xử lý động từ 10/35: translate_2\n","  -> Đã trích xuất và lưu 16 file vector cho động từ 'translate_2'.\n","\n","Đang xử lý động từ 11/35: inhibit\n","  -> Đã trích xuất và lưu 11 file vector cho động từ 'inhibit'.\n","\n","Đang xử lý động từ 12/35: begin_1\n","  -> Đã trích xuất và lưu 6 file vector cho động từ 'begin_1'.\n","\n","Đang xử lý động từ 13/35: splice_2\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'splice_2'.\n","\n","Đang xử lý động từ 14/35: modify\n","  -> Đã trích xuất và lưu 1 file vector cho động từ 'modify'.\n","\n","Đang xử lý động từ 15/35: lose\n","  -> Đã trích xuất và lưu 4 file vector cho động từ 'lose'.\n","\n","Đang xử lý động từ 16/35: encode\n","  -> Đã trích xuất và lưu 4 file vector cho động từ 'encode'.\n","\n","Đang xử lý động từ 17/35: lead\n","  -> Đã trích xuất và lưu 9 file vector cho động từ 'lead'.\n","\n","Đang xử lý động từ 18/35: proliferate\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'proliferate'.\n","\n","Đang xử lý động từ 19/35: express\n","  -> Đã trích xuất và lưu 16 file vector cho động từ 'express'.\n","\n","Đang xử lý động từ 20/35: alter\n","  -> Đã trích xuất và lưu 9 file vector cho động từ 'alter'.\n","\n","Đang xử lý động từ 21/35: decrease_1\n","  -> Đã trích xuất và lưu 17 file vector cho động từ 'decrease_1'.\n","\n","Đang xử lý động từ 22/35: recognize\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'recognize'.\n","\n","Đang xử lý động từ 23/35: translate_1\n","  -> Đã trích xuất và lưu 6 file vector cho động từ 'translate_1'.\n","\n","Đang xử lý động từ 24/35: confer\n","  -> Đã trích xuất và lưu 27 file vector cho động từ 'confer'.\n","\n","Đang xử lý động từ 25/35: delete\n","  -> Đã trích xuất và lưu 4 file vector cho động từ 'delete'.\n","\n","Đang xử lý động từ 26/35: splice_1\n","  -> Đã trích xuất và lưu 12 file vector cho động từ 'splice_1'.\n","\n","Đang xử lý động từ 27/35: transform_2\n","  -> Đã trích xuất và lưu 8 file vector cho động từ 'transform_2'.\n","\n","Đang xử lý động từ 28/35: generate\n","  -> Đã trích xuất và lưu 10 file vector cho động từ 'generate'.\n","\n","Đang xử lý động từ 29/35: decrease_2\n","  -> Đã trích xuất và lưu 11 file vector cho động từ 'decrease_2'.\n","\n","Đang xử lý động từ 30/35: initiate\n","  -> Đã trích xuất và lưu 6 file vector cho động từ 'initiate'.\n","\n","Đang xử lý động từ 31/35: truncate\n","  -> Đã trích xuất và lưu 2 file vector cho động từ 'truncate'.\n","\n","Đang xử lý động từ 32/35: transcribe\n","  -> Đã trích xuất và lưu 8 file vector cho động từ 'transcribe'.\n","\n","Đang xử lý động từ 33/35: disrupt\n","  -> Đã trích xuất và lưu 4 file vector cho động từ 'disrupt'.\n","\n","Đang xử lý động từ 34/35: skip\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'skip'.\n","\n","Đang xử lý động từ 35/35: mutate\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'mutate'.\n","\n","--- QUÁ TRÌNH TRÍCH XUẤT VECTOR TẬP TEST ĐÃ HOÀN TẤT! ---\n"]}]},{"cell_type":"markdown","source":["- **Cấu trúc thư mục mong đợi**\n","  ```bash\n","  .../test_vectors_finetuned_parave/\n","  ├── absorb/\n","  │   ├── sentence_0.pt\n","  │   ├── sentence_1.pt\n","  │   └── ...\n","  ├── block/\n","  │   ├── sentence_0.pt\n","  │   ├── sentence_1.pt\n","  │   └── ...\n","  └── ..."],"metadata":{"id":"2uYuxJcc7t-a"}},{"cell_type":"markdown","source":["## Tạo file index cho từng động từ (ParaVE)"],"metadata":{"id":"w6BkBGOIk7sC"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Sinh ra các file **index `.json`** giúp ánh xạ:\n","    - `sentence_text` → câu gốc trong file test.\n","    - `verb_name` → tên động từ (được lấy từ tên file).\n","    - `vector_file_path` → đường dẫn đầy đủ đến file `.pt` tương ứng (vector của câu đó).\n","  - Dữ liệu index này được dùng để:\n","    - Dễ dàng truy cập các vector tương ứng trong bước **chuẩn hoá**, **đánh giá**, hoặc **so sánh** giữa mô hình.\n"],"metadata":{"id":"fX3Ytz2zlwry"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục chứa dữ liệu test:  \n","    ```\n","    .../Clean_Dataset/Corpus/Split_ParaVE/Test/*.json\n","    ```\n","  - Thư mục chứa vector của mô hình fine-tuned:  \n","    ```\n","    .../test_vectors_finetuned_parave/<verb_name>/sentence_<i>.pt\n","    ```"],"metadata":{"id":"UrzYyItPlxiU"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Thư mục index mới:  \n","    ```\n","    .../verb_indexes_finetuned_parave_test/\n","    ```\n","  - Mỗi file `.json` trong thư mục này chứa danh sách index cho từng động từ, ví dụ:\n","    ```json\n","    [\n","        {\n","            \"sentence_text\": \"Protein A activates gene B in humans.\",\n","            \"verb_name\": \"activate\",\n","            \"vector_file_path\": \"/content/.../test_vectors_finetuned_parave/activate/sentence_0.pt\"\n","        },\n","        {\n","            \"sentence_text\": \"This enzyme catalyzes the reaction efficiently.\",\n","            \"verb_name\": \"catalyze\",\n","            \"vector_file_path\": \"/content/.../test_vectors_finetuned_parave/catalyze/sentence_1.pt\"\n","        }\n","    ]\n","    ```"],"metadata":{"id":"c9zqvWGRlzL-"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. **Đọc toàn bộ file JSON test** trong thư mục `Split_ParaVE/Test/`.\n","  2. **Trích xuất câu** từ trường `\"text\"`.\n","  3. **Sinh đường dẫn vector tương ứng** theo mẫu:\n","     ```\n","     .../test_vectors_finetuned_parave/<verb_name>/sentence_<i>.pt\n","     ```\n","  4. **Tạo danh sách index** gồm:\n","     - `sentence_text`\n","     - `verb_name`\n","     - `vector_file_path`\n","  5. **Lưu danh sách index** vào file `<verb_name>.json` trong thư mục index đầu ra."],"metadata":{"id":"NKXu1W19l1ak"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu TEST GỐC (nơi lấy câu)\n","test_data_dir = os.path.join(drive_base_path, 'Clean_Dataset/Corpus/Split_ParaVE/Test')\n","\n","# Đường dẫn đến thư mục chứa các file vector .pt ĐÃ ĐƯỢỢC TẠO từ mô hình fine-tuned\n","vectors_base_dir = os.path.join(drive_base_path, 'Representation Vector/Test/Finetuned/test_vectors_finetuned_parave')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các file index cho tập test\n","index_output_dir = os.path.join(drive_base_path, 'Representation Vector/Test_Index/Finetuned/verb_indexes_finetuned_parave_test')\n","os.makedirs(index_output_dir, exist_ok=True)\n","\n","\n","# --- 2. THỰC THI CHƯƠNG TRÌNH TẠO INDEX ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"--- Bắt đầu quá trình tạo index cho các vector Test (Fine-tuned) ---\")\n","    print(f\"Đọc dữ liệu gốc từ: {test_data_dir}\")\n","    print(f\"Lưu file index vào: {index_output_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục dữ liệu test\n","        json_files = glob.glob(os.path.join(test_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{test_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json gốc\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file (ví dụ: 'abolish_test_set.json' -> 'abolish')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_test_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang tạo index cho động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Đọc dữ liệu từ file JSON gốc\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Chuẩn bị một danh sách để chứa các mục index cho động từ này\n","            verb_index_list = []\n","\n","            # Lặp qua từng câu để tạo mục index\n","            for i, sentence_text in enumerate(sentences):\n","                # Xây dựng đường dẫn đến file vector .pt tương ứng\n","                vector_file_name = f\"sentence_{i}.pt\"\n","                vector_file_path = os.path.join(vectors_base_dir, verb_name, vector_file_name)\n","\n","                # Tạo một mục index với cấu trúc bạn yêu cầu\n","                index_entry = {\n","                    \"sentence_text\": sentence_text,\n","                    \"verb_name\": verb_name,\n","                    \"vector_file_path\": vector_file_path\n","                }\n","\n","                # Thêm mục index vào danh sách\n","                verb_index_list.append(index_entry)\n","\n","            # Sau khi xử lý hết các câu, lưu danh sách index vào file .json riêng\n","            output_index_path = os.path.join(index_output_dir, f\"{verb_name}.json\")\n","            with open(output_index_path, 'w', encoding='utf-8') as f:\n","                json.dump(verb_index_list, f, indent=4, ensure_ascii=False)\n","\n","            print(f\"  -> Đã tạo và lưu file index '{verb_name}.json' với {len(verb_index_list)} mục.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TẠO INDEX CHO TẬP TEST ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"pkI8iYAPk-fR","executionInfo":{"status":"ok","timestamp":1760006794564,"user_tz":-420,"elapsed":485,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"ccc39595-ef55-431a-e83c-ff262bf3d5e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu quá trình tạo index cho các vector Test (Fine-tuned) ---\n","Đọc dữ liệu gốc từ: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_ParaVE/Test\n","Lưu file index vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_finetuned_parave_test\n","\n","Đang tạo index cho động từ 1/35: catalyse\n","  -> Đã tạo và lưu file index 'catalyse.json' với 7 mục.\n","\n","Đang tạo index cho động từ 2/35: result\n","  -> Đã tạo và lưu file index 'result.json' với 31 mục.\n","\n","Đang tạo index cho động từ 3/35: eliminate\n","  -> Đã tạo và lưu file index 'eliminate.json' với 5 mục.\n","\n","Đang tạo index cho động từ 4/35: develop\n","  -> Đã tạo và lưu file index 'develop.json' với 1 mục.\n","\n","Đang tạo index cho động từ 5/35: translate_3\n","  -> Đã tạo và lưu file index 'translate_3.json' với 25 mục.\n","\n","Đang tạo index cho động từ 6/35: transform_1\n","  -> Đã tạo và lưu file index 'transform_1.json' với 3 mục.\n","\n","Đang tạo index cho động từ 7/35: begin_2\n","  -> Đã tạo và lưu file index 'begin_2.json' với 19 mục.\n","\n","Đang tạo index cho động từ 8/35: block\n","  -> Đã tạo và lưu file index 'block.json' với 13 mục.\n","\n","Đang tạo index cho động từ 9/35: abolish\n","  -> Đã tạo và lưu file index 'abolish.json' với 11 mục.\n","\n","Đang tạo index cho động từ 10/35: translate_2\n","  -> Đã tạo và lưu file index 'translate_2.json' với 16 mục.\n","\n","Đang tạo index cho động từ 11/35: inhibit\n","  -> Đã tạo và lưu file index 'inhibit.json' với 11 mục.\n","\n","Đang tạo index cho động từ 12/35: begin_1\n","  -> Đã tạo và lưu file index 'begin_1.json' với 6 mục.\n","\n","Đang tạo index cho động từ 13/35: splice_2\n","  -> Đã tạo và lưu file index 'splice_2.json' với 5 mục.\n","\n","Đang tạo index cho động từ 14/35: modify\n","  -> Đã tạo và lưu file index 'modify.json' với 1 mục.\n","\n","Đang tạo index cho động từ 15/35: lose\n","  -> Đã tạo và lưu file index 'lose.json' với 4 mục.\n","\n","Đang tạo index cho động từ 16/35: encode\n","  -> Đã tạo và lưu file index 'encode.json' với 4 mục.\n","\n","Đang tạo index cho động từ 17/35: lead\n","  -> Đã tạo và lưu file index 'lead.json' với 9 mục.\n","\n","Đang tạo index cho động từ 18/35: proliferate\n","  -> Đã tạo và lưu file index 'proliferate.json' với 5 mục.\n","\n","Đang tạo index cho động từ 19/35: express\n","  -> Đã tạo và lưu file index 'express.json' với 16 mục.\n","\n","Đang tạo index cho động từ 20/35: alter\n","  -> Đã tạo và lưu file index 'alter.json' với 9 mục.\n","\n","Đang tạo index cho động từ 21/35: decrease_1\n","  -> Đã tạo và lưu file index 'decrease_1.json' với 17 mục.\n","\n","Đang tạo index cho động từ 22/35: recognize\n","  -> Đã tạo và lưu file index 'recognize.json' với 5 mục.\n","\n","Đang tạo index cho động từ 23/35: translate_1\n","  -> Đã tạo và lưu file index 'translate_1.json' với 6 mục.\n","\n","Đang tạo index cho động từ 24/35: confer\n","  -> Đã tạo và lưu file index 'confer.json' với 27 mục.\n","\n","Đang tạo index cho động từ 25/35: delete\n","  -> Đã tạo và lưu file index 'delete.json' với 4 mục.\n","\n","Đang tạo index cho động từ 26/35: splice_1\n","  -> Đã tạo và lưu file index 'splice_1.json' với 12 mục.\n","\n","Đang tạo index cho động từ 27/35: transform_2\n","  -> Đã tạo và lưu file index 'transform_2.json' với 8 mục.\n","\n","Đang tạo index cho động từ 28/35: generate\n","  -> Đã tạo và lưu file index 'generate.json' với 10 mục.\n","\n","Đang tạo index cho động từ 29/35: decrease_2\n","  -> Đã tạo và lưu file index 'decrease_2.json' với 11 mục.\n","\n","Đang tạo index cho động từ 30/35: initiate\n","  -> Đã tạo và lưu file index 'initiate.json' với 6 mục.\n","\n","Đang tạo index cho động từ 31/35: truncate\n","  -> Đã tạo và lưu file index 'truncate.json' với 2 mục.\n","\n","Đang tạo index cho động từ 32/35: transcribe\n","  -> Đã tạo và lưu file index 'transcribe.json' với 8 mục.\n","\n","Đang tạo index cho động từ 33/35: disrupt\n","  -> Đã tạo và lưu file index 'disrupt.json' với 4 mục.\n","\n","Đang tạo index cho động từ 34/35: skip\n","  -> Đã tạo và lưu file index 'skip.json' với 5 mục.\n","\n","Đang tạo index cho động từ 35/35: mutate\n","  -> Đã tạo và lưu file index 'mutate.json' với 5 mục.\n","\n","--- QUÁ TRÌNH TẠO INDEX CHO TẬP TEST ĐÃ HOÀN TẤT! ---\n"]}]},{"cell_type":"markdown","source":["- **Cấu trúc thư mục kết quả**\n","  ```bash\n","  .../verb_indexes_finetuned_parave_test/\n","  ├── activate.json\n","  ├── bind.json\n","  ├── block.json\n","  ├── catalyze.json\n","  └── ..."],"metadata":{"id":"ciO2KUmZl5BZ"}},{"cell_type":"markdown","source":["# Test Dataset - Mô hình Pre-trained"],"metadata":{"id":"Zm4d2P-l8RZJ"}},{"cell_type":"markdown","source":["## Trích xuất hidden states theo từng động từ (GramVar) và lưu thành file `.pt`"],"metadata":{"id":"StTxQU-k9kZ6"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Lấy **hidden states của 13 lớp** (embedding + 12 transformer layers) từ mô hình **BioBERT gốc** `dmis-lab/biobert-base-cased-v1.2` **không fine-tuned** cho **từng câu** trong tập TEST.\n","  - Lưu mỗi câu thành một file `.pt` để dùng cho thống kê / chuẩn hoá / so sánh với vector fine-tuned."],"metadata":{"id":"og00nrcJ9wLO"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục JSON của tập TEST (chia theo động từ):  \n","    `.../Clean_Dataset/Corpus/Split_GramVar/Test/*.json`\n","  - Mỗi file `.json` là một list các object có khóa `\"text\"`."],"metadata":{"id":"MDU33yTE9zDY"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Cấu trúc thư mục vector:\n","    ```\n","    .../test_vectors_pretrained_gramvar/<verb_name>/sentence_<i>.pt\n","    ```\n","  - Mỗi file `.pt` là **dict 13 khóa**:\n","    ```python\n","    {\n","      'layer_0':  Tensor[num_tokens, 768],\n","      'layer_1':  Tensor[num_tokens, 768],\n","      ...\n","      'layer_12': Tensor[num_tokens, 768]\n","    }\n","    ```\n","  - `num_tokens` phụ thuộc tokenizer (bao gồm `[CLS]`, `[SEP]`)."],"metadata":{"id":"jJAGw51e92tY"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. **Tải mô hình & tokenizer PRE-TRAINED**\n","     - `tok = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")`\n","     - `mdl = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")`  \n","       *(dùng `AutoModel`, **không** có head phân loại).*\n","  2. **Duyệt dữ liệu TEST**\n","     - Mỗi file `*_test_set.json` tương ứng một động từ → tạo thư mục `<verb_name>/`.\n","  3. **Trích xuất hidden states**\n","     - Tokenize từng câu → `mdl(**inputs, output_hidden_states=True)`.\n","     - Lấy `outputs.hidden_states` (tuple 13 tensors) → lưu thành dict `{f\"layer_{j}\": hs.squeeze(0)}`.\n","  4. **Ghi ra file**\n","     - Lưu theo tên `sentence_<i>.pt` trong thư mục động từ tương ứng."],"metadata":{"id":"4Ha4x8SH93ku"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN VÀ TẢI MÔ HÌNH ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu TEST\n","test_data_dir = os.path.join(drive_base_path, 'Clean_Dataset/Corpus/Split_GramVar/Test')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các vector PRE-TRAINED của tập TEST\n","output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Test/Pretrained/test_vectors_pretrained_gramvar')\n","os.makedirs(output_base_dir, exist_ok=True)\n","\n","# Tải tokenizer và mô hình pre-trained\n","print(\"--- Bắt đầu tải mô hình Pre-trained (BioBERT gốc) ---\")\n","try:\n","    PRETRAINED_MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.2\"\n","    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n","    # Tải mô hình gốc, không có lớp phân loại ở trên\n","    model_pt = AutoModel.from_pretrained(PRETRAINED_MODEL_NAME)\n","    print(\" -> Tải thành công tokenizer và mô hình pre-trained!\")\n","except Exception as e:\n","    print(f\"LỖI: Không thể tải mô hình. Lỗi: {e}\")\n","    exit()\n","\n","# --- 2. HÀM XỬ LÝ CỐT LÕI ---\n","\n","def get_hidden_states(model, tokenizer, text):\n","    \"\"\"\n","    Trích xuất các trạng thái ẩn (vector biểu diễn) từ mỗi lớp của mô hình.\n","    \"\"\"\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","    return outputs.hidden_states\n","\n","# --- 3. THỰC THI CHƯƠNG TRÌNH ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"\\n--- Bắt đầu quá trình trích xuất vector PRE-TRAINED cho toàn bộ tập TEST ---\")\n","    print(f\"Đọc dữ liệu từ: {test_data_dir}\")\n","    print(f\"Lưu kết quả vào: {output_base_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục test\n","        json_files = glob.glob(os.path.join(test_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{test_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json (mỗi file cho một động từ)\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file (ví dụ: 'abolish_test_set.json' -> 'abolish')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_test_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang xử lý động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Tạo thư mục con tương ứng trong thư mục output\n","            output_verb_dir = os.path.join(output_base_dir, verb_name)\n","            os.makedirs(output_verb_dir, exist_ok=True)\n","\n","            # Đọc dữ liệu từ file JSON\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Lặp qua từng câu\n","            for i, sentence_text in enumerate(sentences):\n","                # Trích xuất vector từ mô hình PRE-TRAINED\n","                hidden_states = get_hidden_states(model_pt, tokenizer, sentence_text)\n","\n","                # Chuẩn bị dữ liệu để lưu\n","                vectors_to_save = {}\n","                for j, layer_rep in enumerate(hidden_states):\n","                    vectors_to_save[f\"layer_{j}\"] = layer_rep.squeeze(0)\n","\n","                # Xây dựng đường dẫn lưu file mới\n","                file_name = f\"sentence_{i}.pt\"\n","                output_path = os.path.join(output_verb_dir, file_name)\n","\n","                # Lưu file\n","                torch.save(vectors_to_save, output_path)\n","\n","            print(f\"  -> Đã trích xuất và lưu {len(sentences)} file vector cho động từ '{verb_name}'.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TRÍCH XUẤT VECTOR TẬP TEST (PRE-TRAINED) ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["343a9bad648d42cc938164c514d59109","15bbcb45c5ad404291d8fc196b508560","2c907cc39abd46b8a18db1c197bac116","0dc10232179d437fabc847c5a5c3a8fc","275680a264b9488ebd300ec0b7b18f35","15ced74777e14ed7835b2632d034c583","49ed9b2d007a46fc88e461e44cde8103","388da669ebad4965a6510eeba48ba014","8d047e6af81d41bba0e20536e9d726f5","64061be4e68a46aca3744116f5627150","37267da34f1f4510a1c58fea0e336f1e","2a5c3176eefd4bd38cecd7e794399f24","11a1f2bf37c248b983e035735784bc29","35f94e9f1f764e8995b3369c233fa2d3","438a29bdb3c849be86e6b0eae22f58cc","117c3888d3454999b9a31e0be3e85558","b76cf3f68f464d43a8a07579dac08cb9","d03c8e75dd0f445a8e30c4fea6066005","3bf4540e9701420b85fafedc68acb3f4","83005a4a424f4a7db92d41b1b3ce4a49","67f7b7434912494f9f546cf063d29eaf","cd7341d3b7a8444eb0d3b2965f6dc1a8"]},"collapsed":true,"id":"_WA6409Y8VP5","executionInfo":{"status":"ok","timestamp":1759996512850,"user_tz":-420,"elapsed":221372,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"e0666d32-4e81-4dc6-b821-ec5664ed089d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu tải mô hình Pre-trained (BioBERT gốc) ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"343a9bad648d42cc938164c514d59109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a5c3176eefd4bd38cecd7e794399f24"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" -> Tải thành công tokenizer và mô hình pre-trained!\n","\n","--- Bắt đầu quá trình trích xuất vector PRE-TRAINED cho toàn bộ tập TEST ---\n","Đọc dữ liệu từ: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_GramVar/Test\n","Lưu kết quả vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/test_vectors_pretrained_gramvar\n","\n","Đang xử lý động từ 1/35: begin_2\n","  -> Đã trích xuất và lưu 28 file vector cho động từ 'begin_2'.\n","\n","Đang xử lý động từ 2/35: begin_1\n","  -> Đã trích xuất và lưu 36 file vector cho động từ 'begin_1'.\n","\n","Đang xử lý động từ 3/35: translate_2\n","  -> Đã trích xuất và lưu 12 file vector cho động từ 'translate_2'.\n","\n","Đang xử lý động từ 4/35: transform_1\n","  -> Đã trích xuất và lưu 24 file vector cho động từ 'transform_1'.\n","\n","Đang xử lý động từ 5/35: delete\n","  -> Đã trích xuất và lưu 8 file vector cho động từ 'delete'.\n","\n","Đang xử lý động từ 6/35: confer\n","  -> Đã trích xuất và lưu 39 file vector cho động từ 'confer'.\n","\n","Đang xử lý động từ 7/35: splice_2\n","  -> Đã trích xuất và lưu 51 file vector cho động từ 'splice_2'.\n","\n","Đang xử lý động từ 8/35: translate_1\n","  -> Đã trích xuất và lưu 44 file vector cho động từ 'translate_1'.\n","\n","Đang xử lý động từ 9/35: inhibit\n","  -> Đã trích xuất và lưu 21 file vector cho động từ 'inhibit'.\n","\n","Đang xử lý động từ 10/35: proliferate\n","  -> Đã trích xuất và lưu 14 file vector cho động từ 'proliferate'.\n","\n","Đang xử lý động từ 11/35: result\n","  -> Đã trích xuất và lưu 36 file vector cho động từ 'result'.\n","\n","Đang xử lý động từ 12/35: lead\n","  -> Đã trích xuất và lưu 10 file vector cho động từ 'lead'.\n","\n","Đang xử lý động từ 13/35: modify\n","  -> Đã trích xuất và lưu 42 file vector cho động từ 'modify'.\n","\n","Đang xử lý động từ 14/35: decrease_2\n","  -> Đã trích xuất và lưu 18 file vector cho động từ 'decrease_2'.\n","\n","Đang xử lý động từ 15/35: splice_1\n","  -> Đã trích xuất và lưu 52 file vector cho động từ 'splice_1'.\n","\n","Đang xử lý động từ 16/35: lose\n","  -> Đã trích xuất và lưu 28 file vector cho động từ 'lose'.\n","\n","Đang xử lý động từ 17/35: transform_2\n","  -> Đã trích xuất và lưu 44 file vector cho động từ 'transform_2'.\n","\n","Đang xử lý động từ 18/35: eliminate\n","  -> Đã trích xuất và lưu 12 file vector cho động từ 'eliminate'.\n","\n","Đang xử lý động từ 19/35: encode\n","  -> Đã trích xuất và lưu 10 file vector cho động từ 'encode'.\n","\n","Đang xử lý động từ 20/35: block\n","  -> Đã trích xuất và lưu 26 file vector cho động từ 'block'.\n","\n","Đang xử lý động từ 21/35: develop\n","  -> Đã trích xuất và lưu 14 file vector cho động từ 'develop'.\n","\n","Đang xử lý động từ 22/35: decrease_1\n","  -> Đã trích xuất và lưu 36 file vector cho động từ 'decrease_1'.\n","\n","Đang xử lý động từ 23/35: recognize\n","  -> Đã trích xuất và lưu 10 file vector cho động từ 'recognize'.\n","\n","Đang xử lý động từ 24/35: abolish\n","  -> Đã trích xuất và lưu 9 file vector cho động từ 'abolish'.\n","\n","Đang xử lý động từ 25/35: translate_3\n","  -> Đã trích xuất và lưu 1 file vector cho động từ 'translate_3'.\n","\n","Đang xử lý động từ 26/35: express\n","  -> Đã trích xuất và lưu 15 file vector cho động từ 'express'.\n","\n","Đang xử lý động từ 27/35: alter\n","  -> Đã trích xuất và lưu 31 file vector cho động từ 'alter'.\n","\n","Đang xử lý động từ 28/35: catalyse\n","  -> Đã trích xuất và lưu 17 file vector cho động từ 'catalyse'.\n","\n","Đang xử lý động từ 29/35: generate\n","  -> Đã trích xuất và lưu 11 file vector cho động từ 'generate'.\n","\n","Đang xử lý động từ 30/35: initiate\n","  -> Đã trích xuất và lưu 20 file vector cho động từ 'initiate'.\n","\n","Đang xử lý động từ 31/35: skip\n","  -> Đã trích xuất và lưu 18 file vector cho động từ 'skip'.\n","\n","Đang xử lý động từ 32/35: mutate\n","  -> Đã trích xuất và lưu 43 file vector cho động từ 'mutate'.\n","\n","Đang xử lý động từ 33/35: truncate\n","  -> Đã trích xuất và lưu 47 file vector cho động từ 'truncate'.\n","\n","Đang xử lý động từ 34/35: transcribe\n","  -> Đã trích xuất và lưu 19 file vector cho động từ 'transcribe'.\n","\n","Đang xử lý động từ 35/35: disrupt\n","  -> Đã trích xuất và lưu 8 file vector cho động từ 'disrupt'.\n","\n","--- QUÁ TRÌNH TRÍCH XUẤT VECTOR TẬP TEST (PRE-TRAINED) ĐÃ HOÀN TẤT! ---\n"]}]},{"cell_type":"markdown","source":["- **Cấu trúc thư mục mong đợi**\n","  ```bash\n","  .../test_vectors_pretrained_gramvar/\n","  ├── abolish/\n","  │   ├── sentence_0.pt\n","  │   ├── sentence_1.pt\n","  │   └── ...\n","  ├── bind/\n","  │   ├── sentence_0.pt\n","  │   ├── sentence_1.pt\n","  │   └── ...\n","  └── ..."],"metadata":{"id":"iF4gf7cn974Z"}},{"cell_type":"markdown","source":["## Tạo file index cho từng động từ (GramVar)"],"metadata":{"id":"RM6TUv9YmIyo"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Sinh các file **index `.json`** để ánh xạ:\n","    - `sentence_text` → câu gốc trong file test.\n","    - `verb_name` → tên động từ (rút ra từ tên file).\n","    - `vector_file_path` → đường dẫn đầy đủ đến file vector `.pt` đã trích từ **BioBERT pre-trained**.\n","  - Bộ index này dùng cho các bước **chuẩn hoá**, **đánh giá**, hoặc **truy vấn nhanh** vector theo câu."],"metadata":{"id":"I5Lg5KHdmtOc"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục JSON của tập test (GramVar):  \n","    ```\n","    .../Clean_Dataset/Corpus/Split_GramVar/Test/*.json\n","    ```\n","  - Thư mục vector đã trích từ mô hình **pre-trained**:  \n","    ```\n","    .../test_vectors_pretrained_gramvar/<verb_name>/sentence_<i>.pt\n","    ```"],"metadata":{"id":"yIbIQq8lmt1p"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Thư mục index mới:  \n","    ```\n","    .../verb_indexes_pretrained_gramvar_test/\n","    ```\n","  - Mỗi file `<verb_name>.json` là **một danh sách** các entry, ví dụ:\n","    ```json\n","    [\n","      {\n","        \"sentence_text\": \"Protein A binds to receptor B.\",\n","        \"verb_name\": \"bind\",\n","        \"vector_file_path\": \"/content/.../test_vectors_pretrained_gramvar/bind/sentence_0.pt\"\n","      },\n","      {\n","        \"sentence_text\": \"This factor inhibits gene C expression.\",\n","        \"verb_name\": \"inhibit\",\n","        \"vector_file_path\": \"/content/.../test_vectors_pretrained_gramvar/inhibit/sentence_1.pt\"\n","      }\n","    ]\n","    ```"],"metadata":{"id":"1_UBur19mveX"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. **Quét** toàn bộ `*.json` trong `Split_GramVar/Test/`.\n","  2. **Suy ra** `verb_name` từ tên file (`*_test_set.json` → `<verb_name>`).\n","  3. **Rút câu** từ trường `\"text\"` của từng item.\n","  4. **Ghép đường dẫn** đến vector `.pt` theo mẫu:  \n","     `test_vectors_pretrained_gramvar/<verb_name>/sentence_<i>.pt`\n","  5. **Ghi** danh sách entry (list) vào file `.../verb_indexes_pretrained_gramvar_test/<verb_name>.json`.\n","\n","---"],"metadata":{"id":"xYPsxbcdmxQo"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu TEST GỐC (nơi lấy câu)\n","test_data_dir = os.path.join(drive_base_path, 'Clean_Dataset/Corpus/Split_GramVar/Test')\n","\n","# Đường dẫn đến thư mục chứa các file vector .pt ĐÃ ĐƯỢC TẠO từ mô hình pre-trained\n","vectors_base_dir = os.path.join(drive_base_path, 'Representation Vector/Test/Pretrained/test_vectors_pretrained_gramvar')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các file index cho tập test\n","index_output_dir = os.path.join(drive_base_path, 'Representation Vector/Test_Index/Pretrained/verb_indexes_pretrained_gramvar_test')\n","os.makedirs(index_output_dir, exist_ok=True)\n","\n","\n","# --- 2. THỰC THI CHƯƠNG TRÌNH TẠO INDEX ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"--- Bắt đầu quá trình tạo index cho các vector Test (Pre-trained) ---\")\n","    print(f\"Đọc dữ liệu gốc từ: {test_data_dir}\")\n","    print(f\"Lưu file index vào: {index_output_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục dữ liệu test\n","        json_files = glob.glob(os.path.join(test_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{test_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json gốc\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file (ví dụ: 'abolish_test_set.json' -> 'abolish')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_test_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang tạo index cho động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Đọc dữ liệu từ file JSON gốc\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Chuẩn bị một danh sách để chứa các mục index cho động từ này\n","            verb_index_list = []\n","\n","            # Lặp qua từng câu để tạo mục index\n","            for i, sentence_text in enumerate(sentences):\n","                # Xây dựng đường dẫn đến file vector .pt tương ứng\n","                vector_file_name = f\"sentence_{i}.pt\"\n","                vector_file_path = os.path.join(vectors_base_dir, verb_name, vector_file_name)\n","\n","                # Tạo một mục index với cấu trúc bạn yêu cầu\n","                index_entry = {\n","                    \"sentence_text\": sentence_text,\n","                    \"verb_name\": verb_name,\n","                    \"vector_file_path\": vector_file_path\n","                }\n","\n","                # Thêm mục index vào danh sách\n","                verb_index_list.append(index_entry)\n","\n","            # Sau khi xử lý hết các câu, lưu danh sách index vào file .json riêng\n","            output_index_path = os.path.join(index_output_dir, f\"{verb_name}.json\")\n","            with open(output_index_path, 'w', encoding='utf-8') as f:\n","                json.dump(verb_index_list, f, indent=4, ensure_ascii=False)\n","\n","            print(f\"  -> Đã tạo và lưu file index '{verb_name}.json' với {len(verb_index_list)} mục.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TẠO INDEX CHO TẬP TEST (PRE-TRAINED) ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"awsEhpPEmRIs","executionInfo":{"status":"ok","timestamp":1760007113085,"user_tz":-420,"elapsed":474,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"76994e38-83e0-4825-bc81-094d22300a51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu quá trình tạo index cho các vector Test (Pre-trained) ---\n","Đọc dữ liệu gốc từ: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_GramVar/Test\n","Lưu file index vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_pretrained_gramvar_test\n","\n","Đang tạo index cho động từ 1/35: begin_2\n","  -> Đã tạo và lưu file index 'begin_2.json' với 28 mục.\n","\n","Đang tạo index cho động từ 2/35: begin_1\n","  -> Đã tạo và lưu file index 'begin_1.json' với 36 mục.\n","\n","Đang tạo index cho động từ 3/35: translate_2\n","  -> Đã tạo và lưu file index 'translate_2.json' với 12 mục.\n","\n","Đang tạo index cho động từ 4/35: transform_1\n","  -> Đã tạo và lưu file index 'transform_1.json' với 24 mục.\n","\n","Đang tạo index cho động từ 5/35: delete\n","  -> Đã tạo và lưu file index 'delete.json' với 8 mục.\n","\n","Đang tạo index cho động từ 6/35: confer\n","  -> Đã tạo và lưu file index 'confer.json' với 39 mục.\n","\n","Đang tạo index cho động từ 7/35: splice_2\n","  -> Đã tạo và lưu file index 'splice_2.json' với 51 mục.\n","\n","Đang tạo index cho động từ 8/35: translate_1\n","  -> Đã tạo và lưu file index 'translate_1.json' với 44 mục.\n","\n","Đang tạo index cho động từ 9/35: inhibit\n","  -> Đã tạo và lưu file index 'inhibit.json' với 21 mục.\n","\n","Đang tạo index cho động từ 10/35: proliferate\n","  -> Đã tạo và lưu file index 'proliferate.json' với 14 mục.\n","\n","Đang tạo index cho động từ 11/35: result\n","  -> Đã tạo và lưu file index 'result.json' với 36 mục.\n","\n","Đang tạo index cho động từ 12/35: lead\n","  -> Đã tạo và lưu file index 'lead.json' với 10 mục.\n","\n","Đang tạo index cho động từ 13/35: modify\n","  -> Đã tạo và lưu file index 'modify.json' với 42 mục.\n","\n","Đang tạo index cho động từ 14/35: decrease_2\n","  -> Đã tạo và lưu file index 'decrease_2.json' với 18 mục.\n","\n","Đang tạo index cho động từ 15/35: splice_1\n","  -> Đã tạo và lưu file index 'splice_1.json' với 52 mục.\n","\n","Đang tạo index cho động từ 16/35: lose\n","  -> Đã tạo và lưu file index 'lose.json' với 28 mục.\n","\n","Đang tạo index cho động từ 17/35: transform_2\n","  -> Đã tạo và lưu file index 'transform_2.json' với 44 mục.\n","\n","Đang tạo index cho động từ 18/35: eliminate\n","  -> Đã tạo và lưu file index 'eliminate.json' với 12 mục.\n","\n","Đang tạo index cho động từ 19/35: encode\n","  -> Đã tạo và lưu file index 'encode.json' với 10 mục.\n","\n","Đang tạo index cho động từ 20/35: block\n","  -> Đã tạo và lưu file index 'block.json' với 26 mục.\n","\n","Đang tạo index cho động từ 21/35: develop\n","  -> Đã tạo và lưu file index 'develop.json' với 14 mục.\n","\n","Đang tạo index cho động từ 22/35: decrease_1\n","  -> Đã tạo và lưu file index 'decrease_1.json' với 36 mục.\n","\n","Đang tạo index cho động từ 23/35: recognize\n","  -> Đã tạo và lưu file index 'recognize.json' với 10 mục.\n","\n","Đang tạo index cho động từ 24/35: abolish\n","  -> Đã tạo và lưu file index 'abolish.json' với 9 mục.\n","\n","Đang tạo index cho động từ 25/35: translate_3\n","  -> Đã tạo và lưu file index 'translate_3.json' với 1 mục.\n","\n","Đang tạo index cho động từ 26/35: express\n","  -> Đã tạo và lưu file index 'express.json' với 15 mục.\n","\n","Đang tạo index cho động từ 27/35: alter\n","  -> Đã tạo và lưu file index 'alter.json' với 31 mục.\n","\n","Đang tạo index cho động từ 28/35: catalyse\n","  -> Đã tạo và lưu file index 'catalyse.json' với 17 mục.\n","\n","Đang tạo index cho động từ 29/35: generate\n","  -> Đã tạo và lưu file index 'generate.json' với 11 mục.\n","\n","Đang tạo index cho động từ 30/35: initiate\n","  -> Đã tạo và lưu file index 'initiate.json' với 20 mục.\n","\n","Đang tạo index cho động từ 31/35: skip\n","  -> Đã tạo và lưu file index 'skip.json' với 18 mục.\n","\n","Đang tạo index cho động từ 32/35: mutate\n","  -> Đã tạo và lưu file index 'mutate.json' với 43 mục.\n","\n","Đang tạo index cho động từ 33/35: truncate\n","  -> Đã tạo và lưu file index 'truncate.json' với 47 mục.\n","\n","Đang tạo index cho động từ 34/35: transcribe\n","  -> Đã tạo và lưu file index 'transcribe.json' với 19 mục.\n","\n","Đang tạo index cho động từ 35/35: disrupt\n","  -> Đã tạo và lưu file index 'disrupt.json' với 8 mục.\n","\n","--- QUÁ TRÌNH TẠO INDEX CHO TẬP TEST (PRE-TRAINED) ĐÃ HOÀN TẤT! ---\n"]}]},{"cell_type":"markdown","source":["- **Cấu trúc thư mục kết quả (minh hoạ)**\n","  ```bash\n","  .../verb_indexes_pretrained_gramvar_test/\n","  ├── bind.json\n","  ├── inhibit.json\n","  ├── activate.json\n","  └── ..."],"metadata":{"id":"kq23xMI1mzv7"}},{"cell_type":"markdown","source":["## Trích xuất hidden states theo từng động từ (ParaVE) và lưu thành file `.pt`"],"metadata":{"id":"Ilrp2GeF-B5b"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Lấy **hidden states của 13 lớp** (embedding + 12 transformer layers) từ mô hình **BioBERT gốc** `dmis-lab/biobert-base-cased-v1.2` (không fine-tuned) cho **mỗi câu** trong tập TEST của **ParaVE**.\n","  - Lưu mỗi câu thành một file `.pt` để phục vụ thống kê/chuẩn hoá/so sánh với vector fine-tuned.\n"],"metadata":{"id":"jiteIfLM_Awi"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục JSON của tập TEST (chia theo động từ):  \n","    `.../Clean_Dataset/Corpus/Split_ParaVE/Test/*.json`\n","  - Mỗi `.json` là list các object có khoá `text`.\n"],"metadata":{"id":"NlQunRq-_C2A"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Cấu trúc thư mục:\n","    ```\n","    .../test_vectors_pretrained_parave/<verb_name>/sentence_<i>.pt\n","    ```\n","  - Mỗi `.pt` là **dict 13 khoá**:\n","    ```\n","    {'layer_0': Tensor[num_tokens, 768], ..., 'layer_12': Tensor[num_tokens, 768]}\n","    ```\n","  - `num_tokens` phụ thuộc tokenizer (có cả `[CLS]`, `[SEP]`)."],"metadata":{"id":"L4FOO644_EwD"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. **Tải mô hình & tokenizer PRE-TRAINED**\n","     - `AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")`\n","     - `AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")` *(không có head phân loại)*.\n","  2. **Duyệt dữ liệu TEST** theo từng file động từ `*_test_set.json`.\n","  3. **Với mỗi câu**:\n","     - Tokenize → `model(**inputs, output_hidden_states=True)`.\n","     - Lấy `outputs.hidden_states` (tuple 13 tensors).\n","     - Tạo dict `{f\"layer_{j}\": hs.squeeze(0)}` và **lưu** `sentence_<i>.pt`.\n","  4. Lặp đến hết toàn bộ file/câu."],"metadata":{"id":"aPbXhJTT_GxA"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN VÀ TẢI MÔ HÌNH ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu TEST\n","test_data_dir = os.path.join(drive_base_path, 'Clean_Dataset/Corpus/Split_ParaVE/Test')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các vector PRE-TRAINED của tập TEST\n","output_base_dir = os.path.join(drive_base_path, 'Representation Vector/Test/Pretrained/test_vectors_pretrained_parave')\n","os.makedirs(output_base_dir, exist_ok=True)\n","\n","# Tải tokenizer và mô hình pre-trained\n","print(\"--- Bắt đầu tải mô hình Pre-trained (BioBERT gốc) ---\")\n","try:\n","    PRETRAINED_MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.2\"\n","    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n","    # Tải mô hình gốc, không có lớp phân loại ở trên\n","    model_pt = AutoModel.from_pretrained(PRETRAINED_MODEL_NAME)\n","    print(\" -> Tải thành công tokenizer và mô hình pre-trained!\")\n","except Exception as e:\n","    print(f\"LỖI: Không thể tải mô hình. Lỗi: {e}\")\n","    exit()\n","\n","# --- 2. HÀM XỬ LÝ CỐT LÕI ---\n","\n","def get_hidden_states(model, tokenizer, text):\n","    \"\"\"\n","    Trích xuất các trạng thái ẩn (vector biểu diễn) từ mỗi lớp của mô hình.\n","    \"\"\"\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","    return outputs.hidden_states\n","\n","# --- 3. THỰC THI CHƯƠNG TRÌNH ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"\\n--- Bắt đầu quá trình trích xuất vector PRE-TRAINED cho toàn bộ tập TEST ---\")\n","    print(f\"Đọc dữ liệu từ: {test_data_dir}\")\n","    print(f\"Lưu kết quả vào: {output_base_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục test\n","        json_files = glob.glob(os.path.join(test_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{test_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json (mỗi file cho một động từ)\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file (ví dụ: 'abolish_test_set.json' -> 'abolish')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_test_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang xử lý động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Tạo thư mục con tương ứng trong thư mục output\n","            output_verb_dir = os.path.join(output_base_dir, verb_name)\n","            os.makedirs(output_verb_dir, exist_ok=True)\n","\n","            # Đọc dữ liệu từ file JSON\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Lặp qua từng câu\n","            for i, sentence_text in enumerate(sentences):\n","                # Trích xuất vector từ mô hình PRE-TRAINED\n","                hidden_states = get_hidden_states(model_pt, tokenizer, sentence_text)\n","\n","                # Chuẩn bị dữ liệu để lưu\n","                vectors_to_save = {}\n","                for j, layer_rep in enumerate(hidden_states):\n","                    vectors_to_save[f\"layer_{j}\"] = layer_rep.squeeze(0)\n","\n","                # Xây dựng đường dẫn lưu file mới\n","                file_name = f\"sentence_{i}.pt\"\n","                output_path = os.path.join(output_verb_dir, file_name)\n","\n","                # Lưu file\n","                torch.save(vectors_to_save, output_path)\n","\n","            print(f\"  -> Đã trích xuất và lưu {len(sentences)} file vector cho động từ '{verb_name}'.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TRÍCH XUẤT VECTOR TẬP TEST (PRE-TRAINED) ĐÃ HOÀN TẤT! ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"0c0R9-gv-Ey9","executionInfo":{"status":"ok","timestamp":1759996667113,"user_tz":-420,"elapsed":67863,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"291c0c03-6b5b-44c0-f974-3fe2dac95310"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu tải mô hình Pre-trained (BioBERT gốc) ---\n"," -> Tải thành công tokenizer và mô hình pre-trained!\n","\n","--- Bắt đầu quá trình trích xuất vector PRE-TRAINED cho toàn bộ tập TEST ---\n","Đọc dữ liệu từ: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_ParaVE/Test\n","Lưu kết quả vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/test_vectors_pretrained_parave\n","\n","Đang xử lý động từ 1/35: catalyse\n","  -> Đã trích xuất và lưu 7 file vector cho động từ 'catalyse'.\n","\n","Đang xử lý động từ 2/35: result\n","  -> Đã trích xuất và lưu 31 file vector cho động từ 'result'.\n","\n","Đang xử lý động từ 3/35: eliminate\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'eliminate'.\n","\n","Đang xử lý động từ 4/35: develop\n","  -> Đã trích xuất và lưu 1 file vector cho động từ 'develop'.\n","\n","Đang xử lý động từ 5/35: translate_3\n","  -> Đã trích xuất và lưu 25 file vector cho động từ 'translate_3'.\n","\n","Đang xử lý động từ 6/35: transform_1\n","  -> Đã trích xuất và lưu 3 file vector cho động từ 'transform_1'.\n","\n","Đang xử lý động từ 7/35: begin_2\n","  -> Đã trích xuất và lưu 19 file vector cho động từ 'begin_2'.\n","\n","Đang xử lý động từ 8/35: block\n","  -> Đã trích xuất và lưu 13 file vector cho động từ 'block'.\n","\n","Đang xử lý động từ 9/35: abolish\n","  -> Đã trích xuất và lưu 11 file vector cho động từ 'abolish'.\n","\n","Đang xử lý động từ 10/35: translate_2\n","  -> Đã trích xuất và lưu 16 file vector cho động từ 'translate_2'.\n","\n","Đang xử lý động từ 11/35: inhibit\n","  -> Đã trích xuất và lưu 11 file vector cho động từ 'inhibit'.\n","\n","Đang xử lý động từ 12/35: begin_1\n","  -> Đã trích xuất và lưu 6 file vector cho động từ 'begin_1'.\n","\n","Đang xử lý động từ 13/35: splice_2\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'splice_2'.\n","\n","Đang xử lý động từ 14/35: modify\n","  -> Đã trích xuất và lưu 1 file vector cho động từ 'modify'.\n","\n","Đang xử lý động từ 15/35: lose\n","  -> Đã trích xuất và lưu 4 file vector cho động từ 'lose'.\n","\n","Đang xử lý động từ 16/35: encode\n","  -> Đã trích xuất và lưu 4 file vector cho động từ 'encode'.\n","\n","Đang xử lý động từ 17/35: lead\n","  -> Đã trích xuất và lưu 9 file vector cho động từ 'lead'.\n","\n","Đang xử lý động từ 18/35: proliferate\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'proliferate'.\n","\n","Đang xử lý động từ 19/35: express\n","  -> Đã trích xuất và lưu 16 file vector cho động từ 'express'.\n","\n","Đang xử lý động từ 20/35: alter\n","  -> Đã trích xuất và lưu 9 file vector cho động từ 'alter'.\n","\n","Đang xử lý động từ 21/35: decrease_1\n","  -> Đã trích xuất và lưu 17 file vector cho động từ 'decrease_1'.\n","\n","Đang xử lý động từ 22/35: recognize\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'recognize'.\n","\n","Đang xử lý động từ 23/35: translate_1\n","  -> Đã trích xuất và lưu 6 file vector cho động từ 'translate_1'.\n","\n","Đang xử lý động từ 24/35: confer\n","  -> Đã trích xuất và lưu 27 file vector cho động từ 'confer'.\n","\n","Đang xử lý động từ 25/35: delete\n","  -> Đã trích xuất và lưu 4 file vector cho động từ 'delete'.\n","\n","Đang xử lý động từ 26/35: splice_1\n","  -> Đã trích xuất và lưu 12 file vector cho động từ 'splice_1'.\n","\n","Đang xử lý động từ 27/35: transform_2\n","  -> Đã trích xuất và lưu 8 file vector cho động từ 'transform_2'.\n","\n","Đang xử lý động từ 28/35: generate\n","  -> Đã trích xuất và lưu 10 file vector cho động từ 'generate'.\n","\n","Đang xử lý động từ 29/35: decrease_2\n","  -> Đã trích xuất và lưu 11 file vector cho động từ 'decrease_2'.\n","\n","Đang xử lý động từ 30/35: initiate\n","  -> Đã trích xuất và lưu 6 file vector cho động từ 'initiate'.\n","\n","Đang xử lý động từ 31/35: truncate\n","  -> Đã trích xuất và lưu 2 file vector cho động từ 'truncate'.\n","\n","Đang xử lý động từ 32/35: transcribe\n","  -> Đã trích xuất và lưu 8 file vector cho động từ 'transcribe'.\n","\n","Đang xử lý động từ 33/35: disrupt\n","  -> Đã trích xuất và lưu 4 file vector cho động từ 'disrupt'.\n","\n","Đang xử lý động từ 34/35: skip\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'skip'.\n","\n","Đang xử lý động từ 35/35: mutate\n","  -> Đã trích xuất và lưu 5 file vector cho động từ 'mutate'.\n","\n","--- QUÁ TRÌNH TRÍCH XUẤT VECTOR TẬP TEST (PRE-TRAINED) ĐÃ HOÀN TẤT! ---\n"]}]},{"cell_type":"markdown","source":["- **Cấu trúc thư mục mong đợi**\n","  ```bash\n","  .../test_vectors_pretrained_parave/\n","  ├── begin_1/\n","  │   ├── sentence_0.pt\n","  │   ├── sentence_1.pt\n","  │   └── ...\n","  ├── block/\n","  │   ├── sentence_0.pt\n","  │   ├── sentence_1.pt\n","  │   └── ...\n","  └── ..."],"metadata":{"id":"r6qLyVrC_KJM"}},{"cell_type":"markdown","source":["## Tạo file index cho từng động từ (ParaVE)"],"metadata":{"id":"2beo0OrxmJ28"}},{"cell_type":"markdown","source":["- **Mục đích**\n","  - Sinh các file **index `.json`** để ánh xạ mỗi câu test tới đường dẫn vector `.pt` đã trích từ **BioBERT pre-trained**.\n","  - Phục vụ các bước **chuẩn hoá**, **đánh giá** và **truy vấn** vector theo câu/động từ.\n"],"metadata":{"id":"YCru5-WUnQ0Q"}},{"cell_type":"markdown","source":["- **Đầu vào**\n","  - Thư mục JSON của tập test (ParaVE):  \n","    `.../Clean_Dataset/Corpus/Split_ParaVE/Test/*.json`\n","  - Thư mục vector **pre-trained** (đã trích sẵn):  \n","    `.../test_vectors_pretrained_parave/<verb_name>/sentence_<i>.pt`\n"],"metadata":{"id":"YmLAvMZDnUQ2"}},{"cell_type":"markdown","source":["- **Đầu ra**\n","  - Thư mục index mới:  \n","    `.../verb_indexes_pretrained_parave_test/`\n","  - Mỗi file `<verb_name>.json` là **một danh sách** các entry:\n","    ```json\n","    [\n","      {\n","        \"sentence_text\": \"Protein A interacts with protein B.\",\n","        \"verb_name\": \"interact\",\n","        \"vector_file_path\": \"/content/.../test_vectors_pretrained_parave/interact/sentence_0.pt\"\n","      },\n","      {\n","        \"sentence_text\": \"Complex C binds to DNA.\",\n","        \"verb_name\": \"bind\",\n","        \"vector_file_path\": \"/content/.../test_vectors_pretrained_parave/bind/sentence_1.pt\"\n","      }\n","    ]\n","    ```"],"metadata":{"id":"fGZx2u5RnWm_"}},{"cell_type":"markdown","source":["- **Quy trình**\n","  1. **Quét** tất cả `*.json` trong `Split_ParaVE/Test/`.\n","  2. **Suy ra** `verb_name` từ tên file (`*_test_set.json` ➜ `<verb_name>`).\n","  3. **Rút câu** từ trường `\"text\"` (bỏ dòng rỗng).\n","  4. **Ghép đường dẫn** đến vector `.pt` theo mẫu:  \n","     `test_vectors_pretrained_parave/<verb_name>/sentence_<i>.pt`\n","  5. **Ghi** danh sách entry (list) vào `verb_indexes_pretrained_parave_test/<verb_name>.json`."],"metadata":{"id":"qkdS79dgnYiw"}},{"cell_type":"code","source":["# --- 1. THIẾT LẬP CÁC ĐƯỜNG DẪN ---\n","\n","# Đường dẫn thư mục gốc trên Google Drive\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file dữ liệu TEST GỐC (nơi lấy câu)\n","test_data_dir = os.path.join(drive_base_path, 'Clean_Dataset/Corpus/Split_ParaVE/Test')\n","\n","# Đường dẫn đến thư mục chứa các file vector .pt ĐÃ ĐƯỢC TẠO từ mô hình pre-trained\n","vectors_base_dir = os.path.join(drive_base_path, 'Representation Vector/Test/Pretrained/test_vectors_pretrained_parave')\n","\n","# Đường dẫn đến thư mục MỚI để lưu các file index cho tập test\n","index_output_dir = os.path.join(drive_base_path, 'Representation Vector/Test_Index/Pretrained/verb_indexes_pretrained_parave_test')\n","os.makedirs(index_output_dir, exist_ok=True)\n","\n","\n","# --- 2. THỰC THI CHƯƠNG TRÌNH TẠO INDEX ---\n","\n","if __name__ == \"__main__\":\n","    print(f\"--- Bắt đầu quá trình tạo index cho các vector Test (Pre-trained) ---\")\n","    print(f\"Đọc dữ liệu gốc từ: {test_data_dir}\")\n","    print(f\"Lưu file index vào: {index_output_dir}\")\n","\n","    try:\n","        # Tìm tất cả các file .json trong thư mục dữ liệu test\n","        json_files = glob.glob(os.path.join(test_data_dir, '*.json'))\n","        total_files = len(json_files)\n","\n","        if total_files == 0:\n","            print(f\"LỖI: Không tìm thấy file .json nào trong '{test_data_dir}'.\")\n","\n","        # Lặp qua tất cả các file json gốc\n","        for file_idx, file_path in enumerate(json_files):\n","            # Lấy tên động từ từ tên file (ví dụ: 'abolish_test_set.json' -> 'abolish')\n","            base_name = os.path.basename(file_path)\n","            verb_name = \"_\".join(base_name.replace('_test_set.json', '').split('_'))\n","\n","            print(f\"\\nĐang tạo index cho động từ {file_idx + 1}/{total_files}: {verb_name}\")\n","\n","            # Đọc dữ liệu từ file JSON gốc\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Trích xuất các câu từ trường 'text'\n","            sentences = [item['text'] for item in data if 'text' in item and item['text'].strip()]\n","\n","            # Chuẩn bị một danh sách để chứa các mục index cho động từ này\n","            verb_index_list = []\n","\n","            # Lặp qua từng câu để tạo mục index\n","            for i, sentence_text in enumerate(sentences):\n","                # Xây dựng đường dẫn đến file vector .pt tương ứng\n","                vector_file_name = f\"sentence_{i}.pt\"\n","                vector_file_path = os.path.join(vectors_base_dir, verb_name, vector_file_name)\n","\n","                # Tạo một mục index với cấu trúc bạn yêu cầu\n","                index_entry = {\n","                    \"sentence_text\": sentence_text,\n","                    \"verb_name\": verb_name,\n","                    \"vector_file_path\": vector_file_path\n","                }\n","\n","                # Thêm mục index vào danh sách\n","                verb_index_list.append(index_entry)\n","\n","            # Sau khi xử lý hết các câu, lưu danh sách index vào file .json riêng\n","            output_index_path = os.path.join(index_output_dir, f\"{verb_name}.json\")\n","            with open(output_index_path, 'w', encoding='utf-8') as f:\n","                json.dump(verb_index_list, f, indent=4, ensure_ascii=False)\n","\n","            print(f\"  -> Đã tạo và lưu file index '{verb_name}.json' với {len(verb_index_list)} mục.\")\n","\n","    except Exception as e:\n","        print(f\"\\nĐã xảy ra lỗi không mong muốn: {e}\")\n","\n","    print(\"\\n--- QUÁ TRÌNH TẠO INDEX CHO TẬP TEST (PRE-TRAINED) ĐÃ HOÀN TẤT! ---\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"TwDyNLrLm6na","executionInfo":{"status":"ok","timestamp":1760007288842,"user_tz":-420,"elapsed":782,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"e1bf9c8f-4252-4e21-a787-53c630a78718"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu quá trình tạo index cho các vector Test (Pre-trained) ---\n","Đọc dữ liệu gốc từ: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/Clean_Dataset/Corpus/Split_ParaVE/Test\n","Lưu file index vào: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/verb_indexes_pretrained_parave_test\n","\n","Đang tạo index cho động từ 1/35: catalyse\n","  -> Đã tạo và lưu file index 'catalyse.json' với 7 mục.\n","\n","Đang tạo index cho động từ 2/35: result\n","  -> Đã tạo và lưu file index 'result.json' với 31 mục.\n","\n","Đang tạo index cho động từ 3/35: eliminate\n","  -> Đã tạo và lưu file index 'eliminate.json' với 5 mục.\n","\n","Đang tạo index cho động từ 4/35: develop\n","  -> Đã tạo và lưu file index 'develop.json' với 1 mục.\n","\n","Đang tạo index cho động từ 5/35: translate_3\n","  -> Đã tạo và lưu file index 'translate_3.json' với 25 mục.\n","\n","Đang tạo index cho động từ 6/35: transform_1\n","  -> Đã tạo và lưu file index 'transform_1.json' với 3 mục.\n","\n","Đang tạo index cho động từ 7/35: begin_2\n","  -> Đã tạo và lưu file index 'begin_2.json' với 19 mục.\n","\n","Đang tạo index cho động từ 8/35: block\n","  -> Đã tạo và lưu file index 'block.json' với 13 mục.\n","\n","Đang tạo index cho động từ 9/35: abolish\n","  -> Đã tạo và lưu file index 'abolish.json' với 11 mục.\n","\n","Đang tạo index cho động từ 10/35: translate_2\n","  -> Đã tạo và lưu file index 'translate_2.json' với 16 mục.\n","\n","Đang tạo index cho động từ 11/35: inhibit\n","  -> Đã tạo và lưu file index 'inhibit.json' với 11 mục.\n","\n","Đang tạo index cho động từ 12/35: begin_1\n","  -> Đã tạo và lưu file index 'begin_1.json' với 6 mục.\n","\n","Đang tạo index cho động từ 13/35: splice_2\n","  -> Đã tạo và lưu file index 'splice_2.json' với 5 mục.\n","\n","Đang tạo index cho động từ 14/35: modify\n","  -> Đã tạo và lưu file index 'modify.json' với 1 mục.\n","\n","Đang tạo index cho động từ 15/35: lose\n","  -> Đã tạo và lưu file index 'lose.json' với 4 mục.\n","\n","Đang tạo index cho động từ 16/35: encode\n","  -> Đã tạo và lưu file index 'encode.json' với 4 mục.\n","\n","Đang tạo index cho động từ 17/35: lead\n","  -> Đã tạo và lưu file index 'lead.json' với 9 mục.\n","\n","Đang tạo index cho động từ 18/35: proliferate\n","  -> Đã tạo và lưu file index 'proliferate.json' với 5 mục.\n","\n","Đang tạo index cho động từ 19/35: express\n","  -> Đã tạo và lưu file index 'express.json' với 16 mục.\n","\n","Đang tạo index cho động từ 20/35: alter\n","  -> Đã tạo và lưu file index 'alter.json' với 9 mục.\n","\n","Đang tạo index cho động từ 21/35: decrease_1\n","  -> Đã tạo và lưu file index 'decrease_1.json' với 17 mục.\n","\n","Đang tạo index cho động từ 22/35: recognize\n","  -> Đã tạo và lưu file index 'recognize.json' với 5 mục.\n","\n","Đang tạo index cho động từ 23/35: translate_1\n","  -> Đã tạo và lưu file index 'translate_1.json' với 6 mục.\n","\n","Đang tạo index cho động từ 24/35: confer\n","  -> Đã tạo và lưu file index 'confer.json' với 27 mục.\n","\n","Đang tạo index cho động từ 25/35: delete\n","  -> Đã tạo và lưu file index 'delete.json' với 4 mục.\n","\n","Đang tạo index cho động từ 26/35: splice_1\n","  -> Đã tạo và lưu file index 'splice_1.json' với 12 mục.\n","\n","Đang tạo index cho động từ 27/35: transform_2\n","  -> Đã tạo và lưu file index 'transform_2.json' với 8 mục.\n","\n","Đang tạo index cho động từ 28/35: generate\n","  -> Đã tạo và lưu file index 'generate.json' với 10 mục.\n","\n","Đang tạo index cho động từ 29/35: decrease_2\n","  -> Đã tạo và lưu file index 'decrease_2.json' với 11 mục.\n","\n","Đang tạo index cho động từ 30/35: initiate\n","  -> Đã tạo và lưu file index 'initiate.json' với 6 mục.\n","\n","Đang tạo index cho động từ 31/35: truncate\n","  -> Đã tạo và lưu file index 'truncate.json' với 2 mục.\n","\n","Đang tạo index cho động từ 32/35: transcribe\n","  -> Đã tạo và lưu file index 'transcribe.json' với 8 mục.\n","\n","Đang tạo index cho động từ 33/35: disrupt\n","  -> Đã tạo và lưu file index 'disrupt.json' với 4 mục.\n","\n","Đang tạo index cho động từ 34/35: skip\n","  -> Đã tạo và lưu file index 'skip.json' với 5 mục.\n","\n","Đang tạo index cho động từ 35/35: mutate\n","  -> Đã tạo và lưu file index 'mutate.json' với 5 mục.\n","\n","--- QUÁ TRÌNH TẠO INDEX CHO TẬP TEST (PRE-TRAINED) ĐÃ HOÀN TẤT! ---\n"]}]},{"cell_type":"markdown","source":["- **Cấu trúc thư mục (minh hoạ)**\n","  ```bash\n","  .../verb_indexes_pretrained_parave_test/\n","  ├── interact.json\n","  ├── bind.json\n","  ├── regulate.json\n","  └── ..."],"metadata":{"id":"HRwM9DaZnab_"}},{"cell_type":"markdown","source":["# Tải vector theo động từ & phân tích chi tiết một câu"],"metadata":{"id":"j358DPd9emFF"}},{"cell_type":"markdown","source":["- **Mục đích**:  \n","  - Đọc file **index** theo từng động từ để tìm đúng đường dẫn tới file vector `.pt` của một câu cụ thể.  \n","  - Tải lại vector biểu diễn (hidden states) đã lưu cho câu đó và **khám phá cấu trúc từng lớp** của mô hình.  "],"metadata":{"id":"Wgkmsnf2e7Su"}},{"cell_type":"markdown","source":["- **Quy trình**"],"metadata":{"id":"Ds35OGYMfBXE"}},{"cell_type":"markdown","source":["1. **Thiết lập đường dẫn**  \n","     - `drive_base_path`: thư mục gốc làm việc trên Google Drive.  \n","     - `index_dir`: nơi chứa các file index JSON (mỗi file tương ứng một động từ, ví dụ `block.json`).  \n","     - (Tùy chọn) `vectors_dir`: thư mục chứa các vector `.pt` (thường không cần vì đường dẫn tuyệt đối đã có trong file index).  "],"metadata":{"id":"DDG-w0u_fD_i"}},{"cell_type":"code","source":["# Đường dẫn thư mục gốc trên Google Drive để làm việc\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep'\n","\n","# Đường dẫn đến thư mục chứa các file index .json (mỗi file cho một động từ)\n","index_dir = os.path.join(drive_base_path, 'Representation Vector/Train_Index/Finetuned/verb_indexes_finetuned_parave_train')\n","\n","# (Không bắt buộc, vì đường dẫn đầy đủ đã có trong file index)\n","# Đường dẫn đến thư mục cha chứa các vector đã được xử lý\n","vectors_dir = os.path.join(drive_base_path, 'Representation Vector/Train/Finetuned/train_vectors_finetuned_parave')"],"metadata":{"id":"MtfbQHfvfERR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. **Hàm `load_vector_data(verb, sentence_index)`**  \n","     - **Input**:  \n","       - `verb`: tên động từ (ví dụ: `\"block\"` hoặc `\"begin_1\"`).  \n","       - `sentence_index`: chỉ số câu muốn lấy (bắt đầu từ 0).  \n","     - **Xử lý**:  \n","       - Đọc file index `{verb}.json` trong `index_dir`.  \n","       - Lấy ra `sentence_text` (câu gốc) và `vector_file_path` (đường dẫn file `.pt`).  \n","       - Dùng `torch.load()` để tải `vector_data` (dict: `layer_0`, `layer_1`, …).  \n","     - **Output**: `tuple (vector_data, sentence_text)` hoặc `(None, None)` nếu lỗi/không tìm thấy.  \n","     - **An toàn**: Có kiểm tra phạm vi `sentence_index` và bắt lỗi đọc file/json.  "],"metadata":{"id":"paxwH_qWfS2m"}},{"cell_type":"code","source":["def load_vector_data(verb, sentence_index):\n","    \"\"\"\n","    Tải dữ liệu vector cho một câu cụ thể dựa vào tên động từ và chỉ số câu.\n","\n","    Args:\n","        verb (str): Tên của động từ (ví dụ: 'begin_1').\n","        sentence_index (int): Chỉ số của câu trong danh sách của động từ đó (bắt đầu từ 0).\n","\n","    Returns:\n","        tuple: Một tuple chứa (dữ liệu vector đã tải, câu gốc tương ứng)\n","               hoặc (None, None) nếu không tìm thấy file.\n","    \"\"\"\n","    try:\n","        # Xây dựng đường dẫn đến file index .json của động từ\n","        index_file_path = os.path.join(index_dir, f\"{verb}.json\")\n","\n","        # Đọc file index\n","        with open(index_file_path, 'r', encoding='utf-8') as f:\n","            index_data = json.load(f) # Đây là một list các dictionary\n","\n","        # Kiểm tra xem chỉ số câu có hợp lệ không\n","        if sentence_index >= len(index_data):\n","            print(f\"LỖI: Chỉ số câu {sentence_index} vượt quá số lượng câu ({len(index_data)}) của động từ '{verb}'.\")\n","            return None, None\n","\n","        # Lấy thông tin của câu cần tìm\n","        sentence_info = index_data[sentence_index]\n","        sentence_text = sentence_info['sentence_text']\n","        vector_file_path = sentence_info['vector_file_path']\n","\n","        # Tải dữ liệu vector từ file .pt\n","        print(f\"Đang tải file vector từ: {vector_file_path}\")\n","        vector_data = torch.load(vector_file_path)\n","\n","        return vector_data, sentence_text\n","\n","    except FileNotFoundError:\n","        print(f\"LỖI: Không tìm thấy file index '{verb}.json' hoặc file vector tương ứng cho câu {sentence_index}.\")\n","        return None, None\n","    except Exception as e:\n","        print(f\"Đã xảy ra lỗi không mong muốn khi đọc câu {sentence_index} của động từ {verb}: {e}\")\n","        return None, None"],"metadata":{"id":"LHmZtPotfUHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" 3. **Ví dụ sử dụng (main)**  \n","     - Chọn mục tiêu:  \n","       - `target_verb = \"block\"`  \n","       - `target_sentence_index = 1`  \n","     - Gọi `load_vector_data`. Nếu thành công:  \n","       - In **câu gốc**.  \n","       - Duyệt qua từng lớp (`layer_0`, `layer_1`, …) và in:  \n","         - **Kích thước tensor** của lớp (thường là `[num_tokens, 768]` với BioBERT base).  \n","         - **10 giá trị đầu** của vector token đầu tiên `[CLS]` ở mỗi lớp (minh họa).  "],"metadata":{"id":"lt7ZAdRFfV5K"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # THAY ĐỔI CÁC THAM SỐ NÀY ĐỂ CHỌN CÂU BẠN MUỐN PHÂN TÍCH\n","    target_verb = \"block\"\n","    target_sentence_index = 1 # Phân tích câu đầu tiên (chỉ số 0)\n","\n","    # Gọi hàm để tải dữ liệu\n","    loaded_data, original_sentence = load_vector_data(target_verb, target_sentence_index)\n","\n","    # Kiểm tra và hiển thị kết quả chi tiết\n","    if loaded_data:\n","        print(\"\\n--- Phân tích chi tiết các lớp cho câu đã chọn ---\")\n","        print(f\"Câu gốc: '{original_sentence}'\")\n","\n","        # 'loaded_data' là một dictionary. Lặp qua từng lớp để in thông tin\n","        for layer_name, layer_tensor in loaded_data.items():\n","            # In tên lớp một cách trang trọng (ví dụ: \"Layer 0\", \"Layer 1\")\n","            print(f\"\\n  --- {layer_name.replace('_', ' ').title()} ---\")\n","\n","            # In kích thước của toàn bộ tensor ở lớp này\n","            # Kích thước sẽ là [số_lượng_token, 768]\n","            print(f\"  Kích thước tensor: {layer_tensor.shape}\")\n","\n","            # Lấy vector của token đầu tiên ('[CLS]') ở lớp hiện tại để xem ví dụ\n","            if layer_tensor.shape[0] > 0:\n","                cls_token_vector = layer_tensor[0]\n","                print(f\"  Vector của token '[CLS]' (10 giá trị đầu):\")\n","                print(f\"  {cls_token_vector[:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"JwcKd31AfXm0","executionInfo":{"status":"ok","timestamp":1759996009715,"user_tz":-420,"elapsed":1873,"user":{"displayName":"Linh Bùi Thu","userId":"16647398495852939072"}},"outputId":"b28b03a4-9a4e-4e7c-ab9a-88d6cac53821"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Đang tải file vector từ: /content/drive/MyDrive/Colab Notebooks/Khoa_Luan_Tot_Nghiep/train_vectors_finetuned_parave/block/sentence_1.pt\n","\n","--- Phân tích chi tiết các lớp cho câu đã chọn ---\n","Câu gốc: 'The antibody increasing GR by 14-3-3eta for the phosphopeptide Y553 was specific, as inhibition of translation would have blocked labeling.'\n","\n","  --- Layer 0 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.4710,  0.0489, -0.1954,  0.3613, -0.0913, -0.3245,  0.9731, -0.9639,\n","        -0.4767, -0.2092])\n","\n","  --- Layer 1 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.3631,  0.0481, -0.1143,  0.1713, -0.1925, -0.1522,  0.4374, -0.1837,\n","        -0.2713, -0.0470])\n","\n","  --- Layer 2 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.3720,  0.0295, -0.0118,  0.0461, -0.2433, -0.2570,  0.5109, -0.4854,\n","        -0.2953, -0.2195])\n","\n","  --- Layer 3 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.5076,  0.3313, -0.2751, -0.1892, -0.6939, -0.3570,  0.3408, -0.5328,\n","        -0.1775, -0.6751])\n","\n","  --- Layer 4 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.8626,  0.4054, -0.1584, -0.3313, -1.1119, -0.5694,  0.6123, -0.7268,\n","        -0.1592, -0.7867])\n","\n","  --- Layer 5 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.9154,  0.1285, -0.2889, -0.3108, -1.3526, -0.3761,  0.6613, -0.7404,\n","         0.1383, -0.9534])\n","\n","  --- Layer 6 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.6989, -0.3047, -0.4474, -0.4007, -1.3506,  0.0577,  1.1390, -0.4223,\n","         0.2873, -0.8509])\n","\n","  --- Layer 7 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.6781, -0.6793, -0.6191, -0.8433, -1.4386, -0.3115,  1.5550,  0.1125,\n","         0.2302, -0.6808])\n","\n","  --- Layer 8 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.5818, -1.0740, -0.5755, -0.6950, -1.4734, -0.0543,  1.1154,  0.4642,\n","        -0.2662, -0.7966])\n","\n","  --- Layer 9 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.5492, -0.7853, -0.4427, -0.8670, -1.5156, -0.4018,  1.4189,  0.1903,\n","         0.2207, -0.8809])\n","\n","  --- Layer 10 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([ 0.0228, -0.9884, -1.2942, -0.4056, -0.1398, -0.4494,  0.9319,  0.2118,\n","        -0.3041, -0.1783])\n","\n","  --- Layer 11 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([-0.5550,  0.0520, -0.2852,  0.1584,  0.3136, -0.9633, -0.0770,  0.1166,\n","        -0.7829, -0.6585])\n","\n","  --- Layer 12 ---\n","  Kích thước tensor: torch.Size([40, 768])\n","  Vector của token '[CLS]' (10 giá trị đầu):\n","  tensor([-0.4302,  0.0163,  0.9031, -0.6029,  0.4208, -0.8618,  0.4266, -0.7929,\n","        -0.5659, -0.4824])\n"]}]},{"cell_type":"markdown","source":["- **Kết quả**:  \n","  - Xem được nội dung câu gốc và cấu trúc vector ở **từng lớp** của mô hình cho câu đã chọn.  \n","  - Xác minh nhanh việc lưu/tải vector `.pt` hoạt động đúng; thuận tiện cho các phân tích tiếp theo (so sánh lớp, tính similarity, trực quan hóa…).  "],"metadata":{"id":"lyBmZuahfZJV"}},{"cell_type":"markdown","source":["- **Lưu ý**:  \n","  - Với BioBERT base, kích thước ẩn là **768**; số lớp thường là **13** tensor nếu bao gồm cả embedding (`layer_0`) + 12 lớp Transformer (`layer_1` … `layer_12`).  \n","  - Hãy đảm bảo file `{verb}.json` tồn tại trong `index_dir` và đường dẫn `vector_file_path` bên trong là hợp lệ.  "],"metadata":{"id":"poK-egAvffRx"}}]}